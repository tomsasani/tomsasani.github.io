[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "An introduction to regression (Part I): basic linear models and interpretation. Presented for the Salt Lake Learners of Biostatistics interest group (University of Utah) in 2019. Video. Slides.\nAn introduction to regression (Part II): multiple variables, interaction, and diagnostics. Presented for the Salt Lake Learners of Biostatistics interest group (University of Utah) in 2019. Slides.\nAn introduction to regression (Part III): approximating statistical tests as linear models. Presented for the Salt Lake Learners of Biostatistics interest group (University of Utah) in 2020. Slides."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Mapping mutator alleles in mice. Lawrence University, Recent Advances in Biology Lecture Series (2021). Virtual.\nMapping mutator alleles in mice. Pritchard Lab Mini-conference (2021). Virtual.\nA wild-derived antimutator drives germline mutation spectrum differences in a genetically diverse murine family. Columbia University, Przeworski Lab Meeting (2021). Virtual.\nLong read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays. Society for Molecular Biology and Evolution annual meeting (2019). Manchester, UK.\nTracking adaptive structural variation during host-pathogen conflict. London Calling, Oxford Nanopore Technologies annual meeting (2017). London, UK. Viewable here."
  },
  {
    "objectID": "talks.html#contributed-presentations",
    "href": "talks.html#contributed-presentations",
    "title": "Talks",
    "section": "Contributed Presentations",
    "text": "Contributed Presentations\nDirectly measuring the dynamics of the human mutation rate by sequencing large, multi-generational pedigrees. American Society of Human Genetics annual meeting (2018). Plenary presentation. San Diego, CA.\nHuman immune defense mechanisms drive rapid genome evolution in vaccinia virus. American Society of Human Genetics annual meeting (2017). Platform presentation. Orlando, FL."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tom Sasani, PhD",
    "section": "",
    "text": "Iâ€™m a computational biologist using genetic data to better understand health and evolution."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Tom Sasani, PhD",
    "section": "Experience",
    "text": "Experience\n\nSenior Data Scientist @ Recursion Pharmaceuticals\nðŸ”¬ 2021 - present: analyzing data from massive cellular imaging experiments to find new disease treatments\n\n\nNIH T32 Postdoctoral Fellow with Dr.Â Kelley Harris\nðŸ­ 2020 - 2021: discovering genetic modifiers of the germline mutation rate in mice\n\n\nGraduate Student with Dr.Â Aaron Quinlan\nðŸ§¬ 2015 - 2020: characterizing patterns of germline mutation in large human families"
  },
  {
    "objectID": "gatk.html",
    "href": "gatk.html",
    "title": "A simple guide to running GATK",
    "section": "",
    "text": "There are probably hundreds of â€œhow to use GATKâ€ guides published online, not including the detailed documentation on the GATK website. But in my (admittedly limited) experience, running GATK can be a bit finicky, and the workflow for producing a genotyped VCF using GATK â€œbest practicesâ€ can take a little bit of troubleshooting. Below Iâ€™ve outlined my own simple workflow for calling SNVs in human data using GATK, including the various preprocessing steps and file/software downloads. This is mostly for my own reference in the future, when the ramblings in my lab notebook eventually lose all meaning to me."
  },
  {
    "objectID": "gatk.html#installing-gatk",
    "href": "gatk.html#installing-gatk",
    "title": "A simple guide to running GATK",
    "section": "Installing GATK",
    "text": "Installing GATK\nGo to the releases on the GATK GitHub, and wget the most recent version (in my case, v.4.1.8.1). Then, just unzip the package.\nwget https://github.com/broadinstitute/gatk/releases/download/4.1.8.1/gatk-4.1.8.1.zip\n\nunzip gatk-4.1.8.1.zip\nThe gatk binary should be sitting in that directory. Just run as follows:\n/path/to/gatk/binary/gatk"
  },
  {
    "objectID": "gatk.html#preparing-bam-files-for-initial-variant-calling",
    "href": "gatk.html#preparing-bam-files-for-initial-variant-calling",
    "title": "A simple guide to running GATK",
    "section": "Preparing BAM files for initial variant calling",
    "text": "Preparing BAM files for initial variant calling\nLetâ€™s assume youâ€™re running GATK separately on individual sample BAMs. Even though each BAM contains a single sampleâ€™s reads, itâ€™s still necessary to add read groups to the BAM header and read entries.\nsamtools addreplacerg -r \"@RG\\tID:${sample_name}\\tSM:${sample_name}\" -o {output} {input}\nAlthough GATK will run just fine on a BAM with no read groups, exactly zero variants will be output to VCF. In fact, my version of GATK HaplotypeCaller stated that all reads were filtered out by MappingQualityFilter.\nOf course, if youâ€™re planning on merging multiple sample BAMs into a single BAM before running HaplotypeCaller, you should add read groups in order to know exactly which sample a given read is derived from.\nThen, as usual, itâ€™s good to sort and index the BAM, and mark duplicates.\nsamtools sort -O BAM -o {output} {input}\n\nsamtools index {input}\nFor duplicate marking, I tend to use picard (available here).\njava -jar /path/to/picard.jar MarkDuplicates INPUT={input} OUTPUT={output} METRICS_FILE={metrics} TMP_DIR={tmpdir}\nAs it marches along the BAM and marks duplicates, picard will output lots of temporary files, so itâ€™s good to specify a $tmpdir with enough space for potentially hundreds (or more) temporary files."
  },
  {
    "objectID": "gatk.html#running-gatk-haplotypecaller",
    "href": "gatk.html#running-gatk-haplotypecaller",
    "title": "A simple guide to running GATK",
    "section": "Running GATK HaplotypeCaller",
    "text": "Running GATK HaplotypeCaller\nThe first step of variant calling involves running HaplotypeCaller, which will produce an initial list of variants. By design, HaplotypeCaller is very sensitive, so itâ€™s possible that most of the variants in the output VCF will be junk. Weâ€™ll do some filtering and â€œcalibrationâ€ of variants in later steps.\ngatk HaplotypeCaller \\\n    --input ${input_bam} \\\n    --output ${output_VCF} \\\n    --reference ${reference_genome} \\\n    --java-options \"-Xmx8G\"\n    -ERC GVCF\n\nSince GATK is written in Java, we can also pass in some standard Java options (such as max memory to be used) as an argument!\n\n\nAlso, by passing in the -ERC GVCF argument, we tell HaplotypeCaller to produce GVCF (generic VCF) output, which looks a bit different from normal VCF. The benefit of outputting GVCFs is that we can then run joint genotyping on many samplesâ€™ GVCFs together quite quickly."
  },
  {
    "objectID": "gatk.html#joint-genotyping-gvcfs",
    "href": "gatk.html#joint-genotyping-gvcfs",
    "title": "A simple guide to running GATK",
    "section": "Joint genotyping GVCFs",
    "text": "Joint genotyping GVCFs\ngatk GenotypeGVCFs \\\n    --variant ${input_gvcfs} \\\n    --output {output} \\\n    --reference {input.ref} \\\n    --java-options \"-Xmx8G\"\nHere, we can run GenotypeGVCFs on one or many GVCFs together. By passing in multiple GVCFs, we can take advantage of the joint genotyping process to consider evidence from multiple samples at a given variant site. In any case, the output here will be â€œtrueâ€ VCF."
  },
  {
    "objectID": "gatk.html#variant-quality-score-recalibration-vqsr",
    "href": "gatk.html#variant-quality-score-recalibration-vqsr",
    "title": "A simple guide to running GATK",
    "section": "Variant quality score recalibration (VQSR)",
    "text": "Variant quality score recalibration (VQSR)\nNow, we arrive at the most arcane (in my opinion) step in running GATK. Essentially, VQSR takes in a few â€œtruth sets,â€ which are just VCF files containing variants considered to be either â€œvery goodâ€ or â€œvery bad,â€ so that GATK can get a feeling for the qualities that make a â€œgood variant.â€ But first, we need to get a hold of these truth sets.\n\nDownloading GATK resources for VQSR\nGATK hosts its â€œresource bundle,â€ containing various files that are important for running GATK, on Google. The four truth sets we can use are hosted here.\nTo download, we can use a tool called gsutil, developed by Google specifically for fetching files from Google Cloud.\n# download \"omni\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz\n# download \"hapmap\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz\n# download \"1000G\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz\n# download \"dbsnp\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\nWeâ€™ll also need the tabix indices for these VCFs, which can also be download from the Google Cloud repository, or done by hand. The â€œdbsnpâ€ truth set also needs to be bgzip-ed!\n\n\nRunning VQSR\nNow that weâ€™ve collected the various files we need, we can run VQSR. Importantly, VQSR does not output VCF files. Instead it outputs a .recal file and a .tranches file, which are â€œappliedâ€ to the previous VCF file in the final step of calibration.\nA couple of notes here:\n\nwe can specify exactly which annotations we want VQSR to add by specifying -an multiple times\nwe can specify the â€œtranchesâ€ we want VQSR to divide our variants into by specifying tranche multiple times\nin GATK v4.1.8.1, we must include a space between the --resource truth set info and the path to the truth set\nit looks as though GATK now has a convolutional neural net approach to VQSR in GATK v4+ for single samples (see here)\n\nMore detailed documentation on running VQSR (for human data) is here.\ngatk VariantRecalibrator \\\n    --variant {input} \\\n    --output {recal_file_output} \\\n    --tranches-file {tranch_file_output} \\\n    -an QD -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\\n    --mode SNP \\\n    --trust-all-polymorphic true \\\n    --java-options \"-Xmx8G\" \\\n    -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \\\n    --resource:omni,known=false,training=true,truth=true,prior=12 /path/to/1000G_omni2.5.hg38.vcf.gz \\\n    --resource:1000G,known=false,training=true,truth=false,prior=10 /path/to/1000G_phase1.snps.high_confidence.hg38.vcf.gz \\\n    --resource:hapmap,known=false,training=true,truth=true,prior=15 /path/to/hapmap_3.3.hg38.vcf.gz \\\n    --resource:dbsnp,known=true,training=false,truth=false,prior=7 /path/to/Homo_sapiens_assembly38.dbsnp138.vcf.gz\n\n\nApplying VQSR\nNow, we arrive at the final step of VQSR. Here, we use the .tranches and .recal files produced in the previous step, and â€œapplyâ€ them to the VCF produced in the first step of this section. The output is a recalibrated VCF, which we can then pass into any downstream filtering or analysis pipelines we want!\ngatk ApplyVQSR \\\n    --variant {input_vcf} \\\n    --recal-file {recal_file} \\\n    --tranches-file {tranches_file} \\\n    --truth-sensitivity-filter-level 99.7 \\\n    --create-output-variant-index true \\\n    -mode SNP \\\n    --output {output} \\\n    --java-options \"-Xmx8G\""
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Ashbrook DG, Sasani TA, Maksimov M, Gunturkun MH, Ma N, Villani F, Ren Y, Rothschild D, Chen H, Lu L, Colonna V, Dumont B, Harris K, Gymrek M, Pritchard JK, Palmer AA, Williams RW. Private and sub-family specific mutations of founder haplotypes in the BXD family reveal phenotypic consequences relevant to health and disease (2022). bioRxiv.\nFixsen SM, Cone KR, Goldstein SA, Sasani TA, Quinlan AR, Rothenburg S, Elde NC. Poxviruses capture host genes by LINE-1 retrotransposition (2020). bioRxiv."
  },
  {
    "objectID": "pubs.html#peer-reviewed-manuscripts",
    "href": "pubs.html#peer-reviewed-manuscripts",
    "title": "Publications",
    "section": "Peer-reviewed Manuscripts",
    "text": "Peer-reviewed Manuscripts\nSasani TA, Ashbrook DG, Beichman AC, Lu L, Palmer AA, Williams RW, Pritchard JK, Harris K. A natural mutator allele shapes mutation spectrum variation in mice (2022). Nature. Code. [.pdf]\nBelyeu JR*, Sasani TA*, Pedersen BS, Quinlan AR. Unfazed: parent-of-origin detection for large and small de novo variants (2021). Bioinformatics. Code. [.pdf]\nWallace AD, Sasani TA, Swanier J, Gates B, Greenland J, Pedersen BP, Varley KT, Quinlan AR. CaBagE: a Cas9-based Background Elimination strategy for targeted, long-read DNA sequencing (2021). PLoS One. [.pdf]\nCawthon RM, Meeks HD, Sasani TA*, Smith KR, Kerber RA, Oâ€™Brien E, Baird L, Dixon MM, Peiffer AP, Leppert MF, Quinlan AR, Jorde LB. Germline mutation rates in young adults predict longevity and reproductive lifespan (2020). Scientific Reports. [.pdf]\nSasani TA, Pedersen BS, Gao Z, Baird L, Przeworski M, Jorde LB, Quinlan AR. Large, three-generation human families reveal post-zygotic mosaicism and variability in germline mutation accumulation (2019). eLife. Code. [.pdf]\nGao Z, Moorjani P, Sasani TA, Pedersen BP, Quinlan AR, Jorde LB, Amster G, Przeworski MP. Overlooked roles of DNA damage and maternal age in generating human germline mutations (2019). PNAS. [.pdf]\nSasani TA*, Cone KR*, Quinlan AR, Elde NC. Long read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays (2018). eLife. Code. [.pdf]\nBelyeu JR, Nicholas TJ, Pedersen BS, Sasani TA, Havrilla JM, Kravitz SN, Conway ME, Lohman BK, Quinlan AR, Layer RM. SV-plaudit: a cloud-based framework for manually curating thousands of structural variants (2018). GigaScience. Code. [.pdf]\nJain M*, Koren S*, Miga K*, Quick J*, Rand AC*, Sasani TA*, Tyson JR*, Beggs AD, Dilthey AT, Fiddes IT, Malla S, Marriott H, Nieto T, Oâ€™Grady J, Olsen HE, Pedersen BS, Rhie A, Richardson H, Quinlan AR, Snutch TP, Tee L, Paten B, Phillippy AM, Simpson JT, Loman NJ, Loose M. Nanopore sequencing and assembly of a human genome with ultra-long reads (2018). Nature Biotechnology. [.pdf]\nFeusier J, Witherspoon DJ, Watkins SW, Goubert C, Sasani TA, Jorde LB. Discovery of rare, diagnostic AluYb8/9 elements in diverse human populations (2017). Mobile DNA. [.pdf]\nPiasecki BP, Sasani TA, Lessenger AT, Huth N, Farrell S. MAPK-15 is a ciliary protein required for PKD-2 localization and male mating behavior in Caenorhabditis elegans (2017). Cytoskeleton.\n\nindicates equal contribution"
  },
  {
    "objectID": "snakemake.html",
    "href": "snakemake.html",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "",
    "text": "A quick-start guide for Snakemake"
  },
  {
    "objectID": "snakemake.html#motivation",
    "href": "snakemake.html#motivation",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Motivation",
    "text": "Motivation\nThere are few (if any) scientific questions that you can answer by running a single program or script. Calling variants involves aligning reads, sorting reads, indexing reads, running a variant caller, filtering those calls, etc. Running a simulation will inevitably require changing parameters or including new combinations of those parameters.\nAnd there are few (if any) programs or scripts that will run/compile correctly on the first try, due to either user error, code bugs, dependency conflicts, improperly configured environments, or any combination therein.\nFor these reasons, itâ€™s often crucial to package the various steps of your analysis into a â€œpipeline.â€ Ideally, this pipeline would accept a file(s) as input, do some stuff with that file, and generate an output file(s). For example, your pipeline might take a FASTQ file and reference genome FASTA as input, and output an aligned, sorted, and indexed BAM.\nIn theory, a pipeline could just be a Bash script in which you enumerate each step of the process. But what if you want to run the pipeline on hundreds of samplesâ€™ FASTQ files? And what if the final step in your Bash script fails? Youâ€™ll have to re-run the entire pipeline all over again.\nThis is where Snakemake comes in. Snakemake is a flexible Python-based pipeline manager, and itâ€™s even tuned for running on the Sage Grid Engine (or pretty much any other compute environment).\n\nAs an example, letâ€™s imagine that we want to take paired-end FASTQ from 3 different mouse samples (A, B, and C) and generate a preliminary set of variant calls for each sample.\n\n\nTo start, letâ€™s imagine we only want to process one sample: A."
  },
  {
    "objectID": "snakemake.html#every-step-of-the-pipeline-gets-its-own-rule",
    "href": "snakemake.html#every-step-of-the-pipeline-gets-its-own-rule",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Every step of the pipeline gets its own â€œruleâ€",
    "text": "Every step of the pipeline gets its own â€œruleâ€\nThe first step of the pipeline will be to download an M. musculus reference genome so that we can align reads.\nrule download_reference:\n  input:\n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nThis rule, which is named download_reference, doesnâ€™t take any input, since itâ€™s just downloading a FASTA directly.\n\n\nWe specify that the expected output of this rule is a single gzipped FASTA.\n\n\nAfter shell:, we simply list the commands weâ€™d normally type at the command line to produce the specified output. These commands can be wrapped up in a docstring for easy formatting.\n\nNext, we want to align the FASTQ data from sample A to the reference.\nrule bwa_align:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"A_1.fastq.gz\",\n    fq2 = \"A_2.fastq.gz\"\n  output:\n    \"A.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nThis rule takes as input a reference genome and two FASTQ files. As you can see, itâ€™s possible to name individual input or output files (using Python variable assignment) so that we can access particular files in our shell command.\n\n\nNote that if a rule has more than one input (or output) files, they should be comma-separated."
  },
  {
    "objectID": "snakemake.html#snakemake-will-only-run-a-rule-if-it-has-to",
    "href": "snakemake.html#snakemake-will-only-run-a-rule-if-it-has-to",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Snakemake will only run a rule if it has to",
    "text": "Snakemake will only run a rule if it has to\nLetâ€™s take a look at the full pipeline weâ€™ve defined so far.\nrule all:\n  input:\n    \"A.sorted.bam\"\n\nrule download_reference:\n  input:\n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"A_1.fastq.gz\",\n    fq2 = \"A_2.fastq.gz\"\n  output:\n    \"A.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nA huge advantage of Snakemake is that it will only run a rule if its output is needed by a downstream rule.\n\n\nYou can see that Iâ€™ve added a rule (called all) to the top of the pipeline. This is because Snakemake runs in a â€œbottom-upâ€ fashion.\n\n\nThe all rule tells Snakemake what the final output of the entire pipeline should be. In this case, we want the final output to be an aligned BAM.\n\n\nIn this example, Snakemake finds the rule that outputs A.sorted.bam (which is bwa_align), and checks to see if that rule has access to all of its necessary inputs (a reference and two FASTQ files). If not, Snakemake finds the rules that produce those files and runs them. And so on. Once bwa_align has all of the inputs it needs, Snakemake runs it to produce the final output.\n\n\nAnd letâ€™s say that for whatever reason, bwa_align fails when its run. When we re-run the pipeline, Snakemake will check that its input files are present. Since all of the previous rules will have been run before invoking bwa_align, Snakemake will see that its inputs are present and wonâ€™t re-run any of the upstream steps!"
  },
  {
    "objectID": "snakemake.html#using-wildcards-to-avoid-re-writing-the-same-command-over-and-over-again",
    "href": "snakemake.html#using-wildcards-to-avoid-re-writing-the-same-command-over-and-over-again",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using wildcards to avoid re-writing the same command over and over again",
    "text": "Using wildcards to avoid re-writing the same command over and over again\nIn the previous example, we were only running the pipeline on a single sample. But to generalize the pipeline to run on any list of samples, we can make use of the expand feature, as well as Snakemake â€œwildcards.â€\nWere this a bash pipeline weâ€™d have to write out a separate set of commands for every sample. But with Snakemake itâ€™s much easier.\nHereâ€™s an example of what our pipeline would look like with wildcard placeholders instead of explicit sample names.\nsamples = [\"A\", \"B\", \"C\"]\n\nrule all:\n  input:\n    expand(\"{sample}.sorted.bam\", sample=samples)\n\nrule download_reference:\n  input: \n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input: \n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"{sample}_1.fastq.gz\",\n    fq2 = \"{sample}_2.fastq.gz\"\n  output:\n    \"{sample}.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nHere, Iâ€™ve just replaced every instance of a sample name in an output or input file with a {sample} wildcard.\n\n\nIn the all rule, Iâ€™m using the expand function to tell Snakemake that the expected output is a list of sorted BAMs, with the sample names in samples filled in.\n\nIn this case, the result of the expand would just be:\n>>> expand(\"{sample}.sorted.bam\", sample=samples)\n\n[\"A.sorted.bam\", \"B.sorted.bam\", \"C.sorted.bam\"]\nBut we can also use expand for more complicated chaining of multiple sample names and parameters."
  },
  {
    "objectID": "snakemake.html#using-expand-to-run-a-pipeline-on-many-samples-or-with-many-parameters",
    "href": "snakemake.html#using-expand-to-run-a-pipeline-on-many-samples-or-with-many-parameters",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using expand to run a pipeline on many samples or with many parameters",
    "text": "Using expand to run a pipeline on many samples or with many parameters\nLetâ€™s say that once weâ€™ve produced aligned and sorted BAMs, we want to then run a simple variant calling program on each sampleâ€™s alignments.\nTo speed up execution of the variant calling pipeline, it might help to parallelize our pipeline to run on every chromosome separately.\nTo do this, weâ€™ll again make use of expand.\nsamples = [\"A\", \"B\", \"C\"]\n\nchromosomes = list(range(1, 20))\nchromosomes = list(map(str, chromosomes))\nchromosomes.extend(['X', 'Y'])\n\nrule all:\n  input:\n    expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n\nrule download_reference:\n  input: \n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input: \n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"{sample}_1.fastq.gz\",\n    fq2 = \"{sample}_2.fastq.gz\"\n  output:\n    \"{sample}.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nrule call_variants:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    bam = \"{sample}.sorted.bam\"\n  output:\n    \"{sample}.{chrom}.vcf\"\n  shell:\n    \"\"\"\n    freebayes -f {input.ref} -r {wildcards.chrom} {input.bam} > {output}\n    \"\"\"\n\nWeâ€™ve now added a step to the pipeline which takes a reference genome and a BAM as input, and outputs a variant call file (VCF).\n\n\nNotice that in the call_variants rule, my freebayes command takes a -r argument that specifies the region we want to analyze. Whenever you want to access a wildcard inside of the shell: portion of a rule, its necessary to preface the wildcard name with wildcard.\n\nAnd in this example, expand takes the Cartesian product (i.e., itertools.product()) of the lists of parameters. So, the output of expand would be:\n>>> expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n\n[\"A.chr1.vcf\",\n \"A.chr2.vcf\",\n \"A.chr3.vcf\",\n ...\n \"C.chrY.vcf\"]\nThis way, we can enumerate every possible combination of input parameters instead of typing out 3 * 21 separate commands."
  },
  {
    "objectID": "snakemake.html#including-pure-python-in-a-rule",
    "href": "snakemake.html#including-pure-python-in-a-rule",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Including pure python in a rule",
    "text": "Including pure python in a rule\nSo far, our rules have only invoked shell commands like bwa or wget. But Snakemake also allows you to make a rule that is just a block of python code. For example, the following rule would be totally acceptable.\nrule count_snps:\n  input:\n    expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n  output:\n    \"per_sample.snp_counts.tsv\"\n  run:\n    \"\"\"\n    from cyvcf2 import VCF\n\n    vcf_file = VCF({input})\n\n    output_fh = open({output}, \"w\")\n    \n    # loop over input files\n    for vcf_fh in {input}:\n\n      sample_name = vcf_fh.split('.')[0]\n      chrom = vcf_fh.split('.')[1]\n\n      # initialize a VCF object using each\n      # file handle in the input list\n      vcf = VCF(vcf_fh)\n\n      snp_count = 0\n\n      for v in vcf:\n        if v.var_type != \"snp\": continue\n        snp_count += 1\n\n      print (','.join([sample_name, chrom, str(snp_count)]), file=output_fh)\n\n    \"\"\"\n\nNotice that when a rule includes python code, we use the run: syntax instead of the shell: syntax at the top of the code block."
  },
  {
    "objectID": "snakemake.html#running-a-snakemake-pipeline-on-the-sage-grid-engine-sge",
    "href": "snakemake.html#running-a-snakemake-pipeline-on-the-sage-grid-engine-sge",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Running a Snakemake pipeline on the Sage Grid Engine (SGE)",
    "text": "Running a Snakemake pipeline on the Sage Grid Engine (SGE)\nIn our lab, we use the SGE to submit and manage compute jobs on the cluster. Snakemake is actually compatible with SGE (and SLURM, etc.), which makes it super easy to submit jobs to SGE directly from a Snakemake pipeline.\nAs an example, we could execute our pipeline as follows:\nsnakemake -j 10 \\\n      --cluster \\\n      --rerun-incomplete \\\n       \"qsub -l centos=7 -l mfree=16G -l h_rt=12:0:0 -o /path/to/outdir -e /path/to/errdir\"\n\nThe -j flag specifies the maximum number of jobs Snakemake is allowed to submit to SGE at a time.\n\n\nThe --rerun-incomplete flag is awesome. If we run our pipeline and it fails for some reason, using --rerun-incomplete will tell Snakemake to re-run a rule if the output of that rule is incomplete (i.e., if the rule didnâ€™t finish due to a job failure in the last pipeline execution).\n\n\nAfter specifying --cluster, we just put the normal qsub command weâ€™d normally use to submit a .sh or .sge script to the cluster, specifying the memory required by each job, wall time, etc."
  },
  {
    "objectID": "snakemake.html#using-a-config-file-to-flexibly-change-grid-requirements-for-particular-rules",
    "href": "snakemake.html#using-a-config-file-to-flexibly-change-grid-requirements-for-particular-rules",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using a config file to flexibly change grid requirements for particular rules",
    "text": "Using a config file to flexibly change grid requirements for particular rules\nOne natural issue with the above command is that some rules might require different cluster specifications than others.\nFor example, Iâ€™ve written my bwa_align rule such that bwa mem will use 4 threads during alignment, and itâ€™ll probably use much more memory than downloading a reference genome.\nThankfully, Snakemake lets us create individual cluster specifications for each of our rules using a config file. See the example config file below, which is written in JSON.\n\"__default__\":\n\n{ \"memory\": \"8G\",\n  \"time\": \"1:0:0\",\n  \"threads\": \"1\",\n  \"os\": \"7\" },\n\n\"bwa_align\":\n\n{ \"memory\": \"4G\",\n  \"time\": \"8:0:0\",\n  \"threads\": \"4\",\n  \"os\": \"7\" }\nThen, when we run Snakemake, we could do the following:\nsnakemake -j 10 \\\n      --cluster-config /path/to/config.json \\\n      --cluster \\\n      --rerun-incomplete \\\n      \"qsub -l centos={cluster.os} -l mfree={cluster.memory} -l h_rt={cluster.time} -pe serial {cluster.threads}\""
  },
  {
    "objectID": "snakemake.html#visualizing-the-full-pipeline-with-a-dag",
    "href": "snakemake.html#visualizing-the-full-pipeline-with-a-dag",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Visualizing the full pipeline with a DAG",
    "text": "Visualizing the full pipeline with a DAG\nWe can also visualize the various steps of the pipeline in a directed acyclic graph (DAG).\nAfter putting the full pipeline in a file called Snakefile, we can run the following from the directory in which the Snakefile resides:\nsnakemake --dag | dot -Tsvg > dag.svg\nThis will produce an image showing us exactly what steps Snakemake will during execution. This plot ignores the VCF calling steps, since the DAG gets pretty unweildy with that many steps!"
  }
]