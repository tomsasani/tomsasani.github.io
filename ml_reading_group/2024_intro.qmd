---
title: "ML for genomics, an evolving reading list"
from: markdown+emoji
---

Expand an article's entry to see brief notes.

# General ML 

The [CS230 course](https://cs230.stanford.edu/) at Stanford has made slides and lectures available online.
A fantastic resource.

The Amidi brothers have created [cheat sheets](https://stanford.edu/~shervine/teaching/cs-221/) for CS230 and other Stanford ML courses.
Also great.

Andrej Karpathy's videos are also excellent introductions to ML concepts. 
In his own words, the first video in his main series [*The spelled-out intro to neural networks and backpropagation: building micrograd*](https://www.youtube.com/watch?v=VMj-3S1tku0) "only assumes basic knowledge of Python and a vague recollection of calculus from high school."
Highly recommended.

# Review articles

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### :star: [Deep learning: new computational modeling techniques for genomics](https://www.nature.com/articles/s41576-019-0122-6)

An excellent and detailed overview, probably where I'd start.
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [A primer on deep learning in genomics](https://www.nature.com/articles/s41588-018-0295-5)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Harnessing deep learning for population genetic inference](https://www.nature.com/articles/s41576-023-00636-3)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### :star: [Navigating the pitfalls of applying machine learning in genomics](https://www.nature.com/articles/s41576-021-00434-9)

Excellent overview of pitfalls and possible mistakes that can confound ML analyses, with a particular focus on biological inference. 
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Opportunities and obstacles for deep learning in medicine](https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0387)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Supervised machine learning for population genetics: a new paradigm](https://doi.org/10.1016/j.tig.2017.12.005)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [To transformers and beyond: large language models for the genome](https://arxiv.org/abs/2311.07621)
:::


# Applications

Papers are broadly grouped by ML architecture. 
Many of these papers involve a mix of architectures, so the groups are "fuzzy."


### Transformers


::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Effective gene expression prediction from sequence by integrating long-range interactions](https://www.nature.com/articles/s41592-021-01252-x)
Describes the "Enformer" model, which utilizes a transformer-based architecture to predict gene expression from sequence alone.

Also see a [suite](https://www.nature.com/articles/s41588-023-01524-6) [of](https://www.nature.com/articles/s41588-023-01517-5) [papers](https://www.nature.com/articles/s41588-023-01574-w) describing the limitations of "Enformer" for personal transcriptome inference.
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation](https://www.biorxiv.org/content/10.1101/2023.08.30.555582v1)
:::



### Generative adversarial networks (GANs)

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### :star: [Automatic inference of demographic parameters using generative adversarial networks](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13386)

The authors describe a novel GAN architecture that that features a population genetic simulator (in this case, the backwards-in-time `msprime` tool) as the "generator" and a convolutional neural network as the "discriminator".
The parameters of the `msprime` generator are randomly initialized, and the discriminator is trained to differentiate between simulated and real "images" of haplotypes in genomic regions of a predefined size.
Over time, the generator gets better at simulating realistic-looking data and the discriminator gets better at telling the two classes of data apart.
By the end of training, the generator can be interpreted by examining the population genetic parameters (population size, mutation rate, etc.) that optimally confused the discriminator.

A well-written and clear overview of a cool (and interpretable) method.
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Interpreting generative adversarial networks to infer natural selection from genetic data](https://www.biorxiv.org/content/10.1101/2023.03.07.531546v1)

A follow-up the the paper listed above. 
The authors fine-tune the trained discriminator from their GAN to infer regions of the genome under the effects of natural selection.
:::


### Language models (LMs)

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### :star: [DNA language models are powerful predictors of genome-wide variant effects](https://www.pnas.org/doi/10.1073/pnas.2311219120)

A deceptively simple and clever approach to inferring variant effects.
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [GPN-MSA: an alignment-based DNA language model for genome-wide variant effect prediction](https://www.biorxiv.org/content/10.1101/2023.10.10.561776v1)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [The nucleotide transformer: building and evaluating robust foundation models for human genomics](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1)

A recent attempt to build a "foundation model" for genomics.
The authors essentially adapt BERT for DNA sequence by developing an encoder-only architecture that attempts to reconstruct randomly-masked 6-mer DNA "tokens."
The learned embeddings from an input DNA sequence can then be plugged into simple regression models to make predictions about chromatin accessibility, enhancer status, etc., or the model itself can be efficiently fine-tuned for a particular downstream classification task.
:::


### Convolutional neural networks (CNNs)

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [The unreasonable effectiveness of convolutional neural networks in population genetic inference](https://academic.oup.com/mbe/article/36/2/220/5229930)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### Bassett and  Basenji

[Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks](https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115)

[Sequential regulatory activity prediction across chromosomes with convolutional neural networks](https://genome.cshlp.org/content/early/2018/03/27/gr.227819.117)
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### :star: [Localizing post-admixture adaptive variants with object detection on ancestry-painted chromosomes](https://academic.oup.com/mbe/article/40/4/msad074/7083730)
The authors train an off-the-shelf object detection model to identify genomic regions with recent (adaptive) admixture events.

Nice example of using off-the-shelf models, rather than building architectures from scratch.
:::

::: {.callout-note icon="false" collapse="true" appearance="simple"}
### [Discovery of ongoing selective sweeps within *Anopholes* mosquito populations using deep learning](https://academic.oup.com/mbe/article/38/3/1168/5918472)
A nice example of training CNNs to detect selection using pre-computed features (e.g., a large collection of population genetic summary statistics) rather than "painted haplotype" images.
:::