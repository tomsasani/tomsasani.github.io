---
title: "Applied Computational Genomics"
subtitle: "4/4/2024"
author: "Tom Sasani"
format: 
    beamer:
        fig-width: 5
        fig-height: 3
execute:
  echo: false
---

## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

\ 

\

\footnotesize

Credit to Phillipe Rigolet's 18.650 (MIT) course notes for some of the slides.


## Linear models are powerful, but limited

In a simple linear model, we model the expected value of $Y$ (our response) given some input values $X$ (our explanatory variables), along with an error term $\epsilon$.

$$\mathbb{E}(Y | X) = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_p + \epsilon$$

. . .


$\mathbb{E}(Y | X) = \mu(X)$: the expected value of $Y$ given $X$

$\beta_0$: the *intercept* term

$\beta_p$: the coefficient associated with independent variable $X_p$

$\epsilon$: residual error unexplained by coefficients


## We make a few basic assumptions when we fit linear models

1. Our errors (residuals) $\epsilon$ are **normally distributed** and **homoscedastic**


. . .


```{r}
#| fig-width: 4
#| fig-height: 2
library(HistData)
data(GaltonFamilies)
library(ggplot2)
library(cowplot)

ggplot(subset(GaltonFamilies, gender=="male"), aes(x=midparentHeight, y=childHeight)) +
    geom_point(alpha=0.5) +
    geom_smooth(method="lm") +
    theme_cowplot() +
    labs(x="Mid-parent height", y="Child height") +
    theme(text = element_text(size = 9), axis.text.y = element_text(size=9), axis.text.x = element_text(size=9))


```

::: {.notes}
For example, on Tuesday I believe Michael used Francis Galton's height measurements as a motivating example for linear regression.

This dataset is a good example of one that fits our assumptions for linear regression. 
If we fit a simple linear regression model to these data, we'd expect that the residuals (errors) -- the differences between our predictions and true values -- are normally distributed and have a constant variance as a function of our X-values.

Let's prove to ourselves that this is the case.
:::

## We make a few basic assumptions when we fit linear models

```{r}
#| fig-cap: Distribution of residuals from a linear model predicting child height as a function of mid-parent height.
m = lm(childHeight ~ midparentHeight, data=subset(GaltonFamilies, gender=="male"))

hist(resid(m), breaks=50)
```

::: {.notes}
Here we're looking at a histogram of the residuals from our linear model fit to the Galton height data.

We won't do a formal test here, but the residuals seem to be approximately normally distributed.
:::

## We make a few basic assumptions when we fit linear models

1. Our errors (residuals) $\epsilon$ are **normally distributed** and **homoscedastic**

2. We can model the expected value of $Y | X$ by applying a linear transformation to $X$

$$\mathbb{E}(Y | X) = \mu(X) = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_p$$

. . .

What if these assumptions are violated?

::: {.notes}
By *continuous* we mean that the variable can take any *real* value within some interval. 

It's pretty easy to imagine that we'll encounter biological data with residuals that are not normally distributed.
For example, a binary response or a count of something.

The second assumption is less obvious as to why it's so restrictive
Basically, in a linear model we assume that we can take our $X$ values, multiply them by coefficients, and directly get out
the expected value (mean) of $Y$ given $X$.
Here, I'm denoting the conditional expectation of Y given X as mu(X).
Later, hopefully it'll become clearer why we might need to relax that assumption when we model data that don't conform to assumption 1.
:::

## Modeling other kinds of data

In biology, we're often interested in modeling data that don't conform to the assumptions of simple linear regression.

### Count data

* Number of DNA sequencing reads aligned to a particular genomic position

### Binary data

* Presence or absence of a species in a microbiome sample

### Discrete categorical data

* Cell type identity in a large single-cell RNA-seq experiment

\

To model these data types, we can turn to **generalized linear models** (GLMs).

## The basic ingredients of a generalized linear model

1. An expected distribution for the $Y$ values^[(belonging to the exponential family)]

. . .

2. A "link" $g$ between the our linear predictor and the conditional mean $E(Y | X) = \mu(X)$ of the distribution:

$$g(\mu(X)) = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_{p} $$

or alternatively...

$$\mu(X) = g^{-1}(\beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_{p})$$

. . .

The link function makes sure that the values produced by our linear predictor are compatible with the distribution we're modeling.

::: {.notes}
We'll need two basic ingredients.

The first ingredient (maybe the most important), is that we need to make a reasonable guess about the kind of statistical distribution that produced our Y values.
So for example, if we're modeling count data, we might want to use the Poisson distribution. If binary data, maybe the Bernoulli distribution.

The second ingredient (which often follows directly from the first) is called a link function. 
Without getting too into the weeds, our link function basically makes sure that the values produced by our linear predictor (our linear combination of coefficients) are compatible with the distribution we've chosen.
Maybe the easiest way of thinking about this is that the link function is "bending the curve" we've fit to the data so that our predictions aren't total nonsense with respect to the true Y values.

As you can see in the second equation, whatever our link function is, we do the inverse of that to the values produced by our linear predictor to ensure that they aren't nonsense.
:::

## Why do we need a "link" function?

```{r}
#| fig-cap: Whether a passenger on the Titanic survived given the amount they paid for their fare.
#| fig-height: 2
#| fig-width: 4

titanic = read.csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")

ggplot(titanic, aes(x=Fare, y=Survived)) +
    geom_point(alpha=0.1) +
    theme_cowplot() 
```

::: {.notes}
Let's walk through this a little more explicitly.

Say we're modeling a binary variable. It's a little morbid, but say we're trying to predict whether someone survived the Titanic disaster as a function of how much they paid for their ticket.

If we just fit a simple linear regression to these data, we get the following.
:::

## Why do we need a "link" function?

```{r}
#| fig-height: 2
#| fig-width: 4

ggplot(titanic, aes(x=Fare, y=Survived)) +
    geom_point(alpha=0.25) +
    geom_smooth(method="lm") +
    theme_cowplot() 
```

The expected value of $Y | X$, &mdash; a.k.a $\mathbb{E}(Y | X) = \mu(X)$ &mdash; cannot be outside of the interval $(0, 1)$. But clearly, our linear predictor $\beta_0 + \beta_{1}X_1$ is predicting values outside of that interval.

We need a link function to make the two things compatible.

::: {.notes}
There's no rule against fitting a line to these data, but it's pretty obvious that simple linear regression is the wrong choice here.

The expected value of $Y | X$ (or as i've ben callin it \mu(X)) cannot possibly be less than 0 or greater than 1. Y is binary. But our linear predictor (beta_0 + beta_1X1) is clearly predicting values outside of that (0, 1) interval.
Without a link function that ensures compatibility between our predictions and the expected value of $Y | X$, our predictions are nonsensical.
:::

## Why do we need a "link" function?

```{r}
#| fig-height: 2
#| fig-width: 4

ggplot(titanic, aes(x=Fare, y=Survived)) +
    geom_point(alpha=0.25) +
    geom_smooth(method="glm", method.args = list(family = binomial(link="logit"))) +
    theme_cowplot() 
```

Using an appropriate link function, we make predictions that are compatible with the expected value of $Y | X$.

The "canonical" link $g(\mu)$ for a logistic regression is $\log\left(\frac{\mu}{1 - \mu}\right)$. 

We can apply the inverse of $g(\mu)$, denoted $g^{-1}(\mu)$, to our linear predictor $\beta_0 + \beta_{1}X_1$ to ensure that our predictions match the expected distribution.

## Side note: picking link functions

Assuming you already have ingredient #1 (an expected distribution for your $Y$ values), there is a "canonical" link function for that distribution.

| GLM type | canonical link function |
| - | - |
| Simple linear regression | $g(\mu) = \mu$ |
| Poisson regression | $g(\mu) = \log(\mu)$ |
| Logistic regression | $g(\mu) = \log\frac{\mu}{1 - \mu}$ |

Here, $\mu$ is shorthand for $\mu(X)$, which is the expected value of $Y$ given $X$.

## Basic ingredients of a GLM *in practice*

We've established that we need two things in order to fit a GLM:

1. An expected distribution for the $Y$ values

2. A "link" $g$ between the our linear predictor and the conditional mean $E(Y | X) = \mu(X)$ of the distribution

When we fit a GLM in `R`, what does this involve?


```{r}
#| eval: false
#| echo: true

# this...
m = glm(Y ~ X, family="poisson")

# is the same as this...
m = glm(Y ~ X, family=poisson(link="log"))
```

::: {.notes}

When you fit GLMs in R, arguably the most important thing is to know #1. 
There are default link functions for modeling most distributions (on the previous slide) -- these links are the ones that are the most natural for each distribution.
You can always change the link function you use, depending on the expected domain of your the conditional mean.

Biggest point here: think carefully about the distribution you expect your Y values to take on, and think carefully about the expected domain of your predictions.
:::

## GLM case study: *de novo* mutations in human genomes

New mutations occur in germline (sperm or egg) cells and can be passed on to children.

In humans, we typically identify *de novo* mutations by sequencing trios (two parents and their child).

::: {#fig-dnm}
![](../img/Artboard 9.png){width=40%}

Example of a *de novo* single-nucleotide mutation
:::

::: {.notes}

OK. now that we've talked through the ingredients of a GLM, let's walk through a biological case study.
:::

## We expect to see around 70 DNMs per child

::: {fig-decode}

![](../img/sasani_dnm_fig1.jpg){width=75% fig-align="center"}

Data from Sasani et al. (2019) *eLife*
:::

## Modeling the dependence between DNM counts and parental age

We also expect *de novo* mutation counts to depend on parental age. 

Each time a spermatogonial stem cell undergoes mitotic DNA replication, there's an opportunity for mutations to occur.

DNA damage also accumulates in germ cells over time, leading to new mutations.

\

**Question:** How many additional germline mutations do we expect to see with each year of paternal age?

Let's build a GLM to find out.



## *De novo* mutation counts increase with age

Let's look at our data first.

```{r}
#| fig-cap: 
#|   - "*De novo* mutation counts observed in children as a function of paternal age at birth. Data from Sasani et al. (2019) *eLife*."
library(ggplot2)
library(dplyr)
library(cowplot)

dnms = read.csv("https://raw.githubusercontent.com/quinlan-lab/ceph-dnm-manuscript/master/data/second_gen.dnms.summary.csv")

ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    labs(x="Paternal age", y="Number of DNMs in child") +
    theme_cowplot()

```

Data from Sasani et al. (2019) *eLife*.

## *De novo* mutation counts increase with age

It looks like there is a strong relationship between *de novo* mutation counts and paternal age.

However, a linear model probably isn't appropriate for modeling the relationship.

* *De novo* mutations are counts (can't be less than 0)

* For count data, variance of errors/residuals is typically not constant as a function of $X$ (heteroscedasticity)

This is where GLMs can come in handy.

## The Poisson distribution is useful for modeling count data

Remember the first ingredient we need to fit a GLM:

1. An expected distribution for the $Y$ values

Since we're dealing with count data, the Poisson makes sense.

. . .

The Poisson distribution tells us:

$$P(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!} \ \text{for} \ k = 0,1,2...$$

where $\lambda$ is the expected number of "events."

We expect to see an average of $\lambda = 70$ *de novo* mutations (DNMs) per child.


## Modeling count data with the Poisson

Now that we've decided to model our data using the Poisson distribution, remember the second ingredient we need for a GLM:

2. A "link" $g$ between the our linear predictor and the conditional mean $\mathbb{E}(Y | X) = \mu(X)$ of the distribution

. . .

In Poisson regression, the "canonical" link function $g(\mu(X))$ is the "log link" $\log(\mu(X))$:

$$\log(\mu(X)) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

. . .

If we exponentiate both sides of the equation,

$$\mu(X) = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$

We see that we're applying the inverse of the link function to our linear predictor.

::: {.notes}

Why the "log link" for Poisson data? 

My modeling the log of the expected value (or put another way, exponentiating our predictions), we ensure that our predicted values are never negative. E to the something can never be negative
:::


<!-- ## Why not log-transform our input data?

There's a subtle (but important) difference between log-transforming our $Y$ values and modeling the logarithm of the *expected value* of our $Y$ values.

In other words,

$$E[\log(Y) | X] \neq \log(E[Y | X])$$ -->

## We can fit a simple Poisson model using `R`

```{r}
#| echo: true

m = glm(snv_dnms ~ dad_age, 
        family=poisson(link = "log"), 
        data=dnms,
        )
```

## Interpreting our Poisson GLM in `R`

Let's look at the model summary:

\scriptsize
```{r}
summary(m)
```

## Interpreting our Poisson GLM in `R`

Just like a typical `summary()` output, we can see the coefficient estimates and p-values for each of our independent variables.

One key difference is that the coefficients are on a logarithmic scale.

Recall that:

$$\log(\mu(X)) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

and thus,

$$\mu(X) = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$


## Interpreting our Poisson GLM in `R`

In our `summary()` output, let's look at the coefficients:

\

\footnotesize
```{r}
#| echo: true
coef(summary(m))
```

\

\normalsize

For a unit increase in `dad_age`, there is a multiplicative effect of $e^{0.02581} = 1.0261$ on the response variable (expected # of DNMs).

In other words, the number of DNMs observed in a child born to a 25-year-old father is $1.0261$ times larger than the number of DNMs observed in a child born to a 24-year-old father.


## Visualizing our Poisson model fit

```{r}
#| echo: false
ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    geom_smooth(method="glm", method.args=list(family="poisson"), fullrange=TRUE, se=TRUE) +
    labs(x="Paternal age", y="Number of DNMs") +
    theme_cowplot()
```

## Explicitly modeling counts as *rates*

In Sasani et al. (2019), we only looked for mutations at genomic positions where at least 10 sequencing reads were aligned in mom, dad, and the child. 


```{r}
#| fig-width: 4
#| fig-height: 1.5
library(cowplot)

dnms$callable_bp_bill = dnms$autosomal_callable_fraction / 1e9

# print (dnms$callable_bp_bill)

ggplot(dnms, aes(x=factor(sample_id), y=callable_bp_bill)) +
    geom_col() +
    labs(x = "Sample ID", y="Callable bp (billions)") +
    #ylim(c(2, 3)) +
    coord_cartesian(ylim = c(2.45, 2.6)) +
    theme_cowplot() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = 9), axis.text.y = element_text(size=9))
```

We were able to look for DNMs at ~2.6 billion positions in most children, but closer to ~2.4 billion in others.

Does this variable sequencing depth affect our observed DNM counts?


## Explicitly modeling counts as *rates*

Our current model is:

$$\log(\mu(X)) = \beta_0 + \beta_{1}X_{1}$$

where $\mu(X)$ is the expected number of DNMs observed in a child given the age of the child's father.

We actually want to model:
$$\log(\frac{\mu(X)}{N}) = \beta_0 + \beta_{1}X_{1}$$

where $N$ is the number of genomic positions at which we had enough sequencing data in the trio to detect a mutation.

In other words, we want to model the expected number of DNMs in a child *per "callable" base pair* as a function of age.


## Explicitly modeling counts as rates

Thanks to logarithm rules (remember those?),

$$\log(\frac{\mu(X)}{N}) = \beta_0 + \beta_{1}X_{1}$$

$$\log(\mu(X)) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

In the context of Poisson GLMs, the $\log(N)$ term is sometimes referred to as "exposure" or "offset."

## Modeling with offsets in `R`

In `R`, we can easily update our linear model to account for an "offset."

Recall that our new model is

$$\log(\mu(X)) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

where $N$ is the number of callable base pairs per trio.

In `R`, this looks like:

\small
```{r}
#| echo: true
#| eval: false
m = glm(
    all_dnms ~ dad_age + offset(log(callable_bp)), 
    family="poisson", 
    data=dnms,
)
```

## Take-home big picture

When fitting GLMs, you need to answer:

### What distribution are my $Y$ values expected to follow?

#### what `family` do I choose in the `glm` function?

### How do I make the expected values of $Y$ compatible with my linear predictor?

* what `link` do I choose?

\

Be careful to interpret coefficients accordingly!



<!-- ## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

2. Fitting linear models

(h/t to the [CS229 course notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) from Andrew Ng)

## Let's return to ordinary least squares

Let's look at the relationship between beak length and flipper length among the Palmer Penguins.

```{r}
#| echo: false
#| warning: false
# install.packages("palmerpenguins", repos='http://cran.us.r-project.org')
library(palmerpenguins)

ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g, col=species)) +
    geom_point() +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

## We can easily fit a linear model to these data

```{r}
ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g)) +
    geom_point() +
    geom_smooth(method="lm") +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

Our goal is to find a line that minimizes the residual sum of squares (RSS).

But how exactly can we find the coefficients that produce that line?

## Some setup and definitions

We want to approximate $y$ as a linear function of our predictors $x$.

$$h(x) = \theta_0 + \theta_{1}x_1$$

The $\theta$ values are our coefficients, or in machine learning lingo, our "parameters."

Given input values $x$, our function $h(x)$ will hopefully produce values that are very close to $y$. 

## Defining our "cost function"

In machine learning, we train models by finding model parameters that minimize a cost/loss function. 

Since we want our function $h(x)$ to produce values very close to the "target" values $y$, we can measure that "closeness" using a cost function $J$:

$$J(\theta) = \frac{1}{2}\sum_{i = 1}^{m}(h_{\theta}(x^i) - y^i)^2$$

## Gradient descent

To find the values of $\theta$ that minimize our cost function $J(\theta)$, we can start by picking a *totally random* initial value for $\theta$.

Then, we can iteratively update $\theta$ values by asking: if we increase $\theta$, does $J(\theta)$ go up or down? 

If increasing $\theta$ makes the cost function decrease, we should continue bumping up our values of $\theta$, and vice versa.

More formally, we perform the following:

$$\theta_j := \theta_j - \alpha\frac{\delta}{\delta\theta_j}J(\theta)$$

where $\alpha$ is a term called the "learning rate."

## Gradient descent (less formally)

In plain-ish English:

For the $j$th parameter, take the partial derivative of the cost function $J(\theta)$ with respect to $\theta_j$. That partial derivative will be negative if the cost function *decreases* when $\theta_j$ *increases*, and positive if the cost function *increases* when $\theta_j$ *increases*.

Then, update $\theta_j$ by subtracting the partial derivative multiplied by the learning rate. If the learning rate is large, our "updates" to $\theta_j$ will be large, and if the learning rate is small, our updates will be small.

**Note:** This will always update $\theta_j$ in the direction of decreasing the cost function!

## Gradient descent (without partial derivatives)

$$\theta_j := \theta_j - \alpha\sum_{i = 1}^{m}(y^i - h_\theta(x^i))x_j^i$$ -->
