---
title: "Applied Computational Genomics"
subtitle: "4/4/2024"
author: "An introduction to GLMs"
format: 
    revealjs:
        theme: [default, theme.scss]
        html-math-method: mathjax
        scrollable: true
        code-block-height: 150px
        code-fold: true
        code-summary: "Show the code"
execute:
  echo: true
---

## Linear models are powerful, but limited

\

In a simple linear model, we model a response $Y$ as a function of some inputs $X$ (our independent variables), along with an error term $\epsilon$.

$$Y = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_p + \epsilon$$

. . .


$Y$ : our response variable

$\beta_0$ : the intercept term

$\beta_p$ : the coefficient associated with independent variable $X_p$

$\epsilon$ : error (residual) term

::: {.notes}
On Tuesday, Michael introduced you to simple linear regression.

In a simple...
:::


## We make a few assumptions when we fit linear models

::: {.callout-note icon="false" appearance="simple"}
# *Linearity*: can the relationship between $X$ and $Y$ be approximated by a straight line?
:::

::: {.callout-warning icon="false" appearance="simple"}
# *Independence*: are the errors (residuals) independent of each other?
:::

::: {.callout-important icon="false" appearance="simple"}
# *Homoscedasticity*: is the variance of the errors (residuals) constant as a function of $X$?
:::

::: {.callout-tip icon="false" appearance="simple"}
# *Normality*: are the errors (residuals) normally distributed?
:::

::: {.notes}
While simple linear models can be really powerful, we make a number of basic (and sometimes limiting) assumptions when we fit them.
:::

## Checking the *independence* assumption

```{r}
library(tidyquant) 
library(ggplot2)
library(cowplot)

googl <- tq_get(x = "GOOG", from = '2020-01-01')

ggplot(googl, aes(x=date, y=open)) +
    geom_line() +
    labs(x="Date", y="GOOGL opening stock price ($)") +
    theme_cowplot()
```

::: {.notes}
One of our first assumptions when we fit linear models is that our errors (residuals) are independent.

Oftentimes, this assumption is violated when our data have a temporal component.

We won't spend much time on this assumption today, but here's an example of data that violate the assumption.

Since there's a temporal component to these data, it's pretty clear that 
:::

## Checking the *independence* assumption

```{r}
m = lm(open ~ as.numeric(date), data=googl)
googl$pred = predict(m)

ggplot(googl, aes(x=date, y=open)) +
    geom_line() +
    geom_smooth(method="lm", se=FALSE) +
    geom_segment(aes(xend=date, yend=pred), col="firebrick") +
    labs(x="Date", y="GOOGL opening stock price ($)") +
    theme_cowplot()
```

::: {.notes}
Since there's a temporal component to these data, it's pretty clear that if we fit a linear model to the data, our residuals will actually be correlated.

A large residual (here) will likely be followed by another large residual.
:::


## Checking the *homoscedasticity* assumption

```{r}
library(ggplot2)
library(cowplot)

set.seed(12345)

# number of observations
n <- 100 
# generate x values from 1 to n
x <- 1:n
# generate an error term, normally distributed with mean 0 and SD 4 
sd <- runif(n, min = 0, max = 4)
error <- rnorm(n, 0, sd*x) 
# generate y values
y <- (x * 10) + error 

# put x and y into dataframe
df = as.data.frame(cbind(x, y))

# plot
ggplot(df, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) + 
    theme_cowplot() + 
    labs(x = "X", y = "Y")
```

::: {.notes}
One assumption that is frequently violated in biological data is the *homoscedasticity* assumption, so let's spend a minute on that.

Here's some toy X and Y data I've plotted. There appears to be a clear linear relationship between the two (assumption #1, check), but there's something kind of odd about the Y values.

The "spread" increases as the Y values get bigger.
:::

## Checking the *homoscedasticity* assumption

```{r}
# make predictions using simple linear model
m = lm(y ~ x, data=df)
df$pred = predict(m)

# plot residuals
ggplot(df, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) + 
    geom_segment(aes(xend = x, yend = pred), color="firebrick") +
    theme_cowplot() + 
    labs(x = "X", y = "Y")

```

::: {.notes}
If we explicitly plot the residuals in red, we can see that they get larger as Y increases.

This phenomenon (variance of our residuals is not constant) is called *heteroscedasticity*, and it generally means that a simple linear model is not appropriate.

Heteroscedasticity is a common feature of count data, which is super common in biology.
:::


## Checking the *normality* assumption

```{r}
library(HistData)
data(GaltonFamilies)

# subset to male children
galton_filt = subset(GaltonFamilies, gender=="male")

ggplot(galton_filt, aes(x=midparentHeight, y=childHeight)) +
    geom_point(alpha=0.5, size=5) +
    geom_smooth(method="lm", se=FALSE) +
    theme_cowplot() +
    labs(x="Mid-parent height", y="Child height")
```

::: {.notes}
Let's look at another one of those assumptions in some more detail.

The "normality" assumption states that the residuals in our fitted model should be normally distributed.

To test this assumption, let's look at the Galton height data that Michael discussed on Tuesday.

Here we have...
:::

## Checking the *normality* assumption

```{r}
m = lm(childHeight ~ midparentHeight, data=galton_filt)
galton_filt$pred = predict(m)

ggplot(galton_filt, aes(x=midparentHeight, y=childHeight)) +
    geom_segment(aes(xend = midparentHeight, yend = pred), color="firebrick") +
    geom_point(alpha=0.5, size=5) +
    geom_smooth(method="lm", se=FALSE) +
    theme_cowplot() +
    labs(x="Mid-parent height", y="Child height")
```

## Checking the *normality* assumption

```{r}
hist(resid(m), breaks=50, main=NULL)
```


## We make a few assumptions when we fit linear models

::: {.callout-note icon="false" appearance="simple"}
# *Linearity*: can the relationship between $X$ and $Y$ be approximated by a straight line?
:::

::: {.callout-warning icon="false" appearance="simple"}
# *Independence*: are the $Y$ observations independent of each other?
:::

::: {.callout-important icon="false" appearance="simple"}
# *Homoscedasticity*: is the variance of the errors (residuals) constant as a function of $X$?
:::

::: {.callout-tip icon="false" appearance="minimal"}
# *Normality*: are the errors (residuals) normally distributed?
:::

\

**What if these assumptions are violated?**

::: {.notes}
It's pretty easy to imagine that we'll encounter biological data with residuals that are not normally distributed, or that aren't homoscedastic.
For example, a binary response or a count of something.
:::

## Modeling other kinds of data

In biology, we're often interested in modeling data that don't conform to the assumptions of simple linear regression.

::: {.callout-note icon="false"}
# Count data
Number of DNA sequencing reads aligned to a particular genomic position
:::

::: {.callout-tip icon="false"}
# Binary data
Presence or absence of a species in a microbiome sample
:::

::: {.callout-caution icon="false"}
# Discrete categorical data
Cell type identity in a large single-cell RNA-seq experiment
:::

. . .

To model these data types, we can turn to *generalized linear models*.


## The basic ingredients of a GLM

1. An expected distribution for the $Y$ values

$$\textcolor{teal}{\mu} = \mathbb{E}(Y | X)$$

. . .

2. A linear predictor

$$\textcolor{violet}{\eta} = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_{p}$$


. . .

3. A "link" function

$$\textcolor{olive}{g}(\textcolor{teal}{\mu}) = \textcolor{violet}{\eta}$$
$$\textcolor{teal}{\mu} = \textcolor{olive}{g}^{-1}(\textcolor{violet}{\eta})$$

::: {.notes}
To extend linear models to other data distributions, we'll need three basic ingredients.

The first ingredient (maybe the most important), is that we need to make a reasonable guess about the kind of statistical distribution that produced our Y values.
So for example...
We need to give our GLM a statistical distribution so that it's able to appropriately model the mean and variance of our Y values.
Must be exponential family.

We also need a linear predictor.
Basically, the mx + b part of our equation, that takes indepdnent varaibles, multiplies them by coefficients, and produces an output.

The final ingredient (which often follows directly from the first) is called a link function. 
Without getting too into the weeds, our link function basically makes sure that the values produced by our linear predictor (our linear combination of coefficients) are compatible with the distribution we've chosen.
Maybe the easiest way of thinking about this is that the link function is "bending the curve" we've fit to the data so that our predictions aren't total nonsense with respect to the true Y values.

As you can see in the second equation, whatever our link function is, we do the inverse of that (g^-1) to the values produced by our linear predictor to ensure that they aren't nonsense.
:::

## Why do we need GLMs?

```{r}
#| fig-cap: Whether a passenger on the Titanic survived given the amount they paid for their fare.
titanic = read.csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")

ggplot(titanic, aes(x=Fare, y=Survived)) +
    geom_point(alpha=0.1, size=5) +
    theme_cowplot() 
```

::: {.notes}
Let's walk through this a little more explicitly.

Say we're modeling a binary variable. It's a little morbid, but say we're trying to predict whether someone survived the Titanic disaster as a function of how much they paid for their ticket.

If we just fit a simple linear regression to these data, we get the following.
:::

## Why do we need GLMs?

```{r}
ggplot(titanic, aes(x=Fare, y=Survived)) +
    geom_point(alpha=0.1, size=5) +
    geom_smooth(method="lm") +
    theme_cowplot()
```

::: {.notes}
There's no rule against fitting a line to these data, but it's pretty obvious that simple linear regression is the wrong choice here.

The expected value of $Y | X$ (the conditional expectation of Y given X, or as i've been callin it \mu) cannot possibly be less than 0 or greater than 1. Y is binary. But our linear predictor (beta_0 + beta_1X1) is clearly predicting values outside of that (0, 1) interval.
Without a link function that ensures compatibility between our predictions and the expected value (or mean) of our $Y | X$, our predictions are nonsensical.
:::

## Why do we need GLMs?

```{r}
ggplot(titanic, aes(x=Fare, y=Survived)) +
    geom_point(alpha=0.1, size=5) +
    geom_smooth(method="glm", method.args = list(family = binomial(link="logit"))) +
    theme_cowplot()
```

The "canonical" link $g(\mu)$ for a logistic regression is $\log\left(\frac{\mu}{1 - \mu}\right)$. 

If we define our linear predictor as $\eta$, we can do $g^{-1}(\eta) = \frac{e^{\eta}}{1 + e^{\eta}}$ to ensure that our predictions match the expected distribution.

::: {.notes}
But, if we fit a GLM by telling it 1) we're modeling binary data and 2) we don't expect our Y values to be outside the (0, 1) range, we can produce reasonable predictions.

We do this by applying something called the "logit link," defined as (this) with respect to the conditional mean of our Y distribution.
More explicitly, if we define our linear predictor (the mx + b) part of the equation as eta, we apply the inverse (g^-1) of the link to eta.
By doing that, we "bend the lien of best fit" so that our predicted values are between 0 and 1.

You can see that if we apply this thing to our predicted values (produced by mx + b), there's no way for the output to exceed 1 or go below 0.
:::

## Side note: picking link functions

Assuming you already have ingredients #1 (an expected distribution for your $Y$ values, denoted $\mu$) and #2 (a linear predictor, denoted $\eta$) there is a "canonical" link function for that distribution.

\

| GLM type | canonical link function | inverse of link function
| - | - | - |
| Simple linear regression | $g(\mu) = \mu$ | $\mu = \eta$ |
| Poisson regression | $g(\mu) = \log(\mu)$ | $\mu = e^{\eta}$ |
| Logistic regression | $g(\mu) = \log(\frac{\mu}{1 - \mu})$ | $\mu = \frac{e^{\eta}}{1 + e^{\eta}}$|

. . .

::: {.callout-note appearance="simple" icon="false"}
**Note:** You can pick any link function you like, even if it's not the canonical or natural link &mdash; it just depends on how you want to model $Y | X$!
:::

::: {.notes}
As a side note, for every distribution you might want to model, there is something called a "canonical" link function -- basically, a natural way to transform your mx + b predictions given that distribution.

As mentioned before, we apply the inverse of the link function to our predictions.

I should mention that you can technically use any link you want. It really just depends on how you want to model Y | X.
:::

## Basic ingredients of a GLM *in practice*

We've established that we need three things in order to fit a GLM:

* An expected distribution for the $Y$ values ($\mu$)

* A linear predictor ($\eta$)

* A "link" between the our linear predictor and the response variable ($g$)

When we fit a GLM in R, what does this involve?


## GLM case study: *de novo* mutations in human genomes

New mutations occur in germline (sperm or egg) cells and can be passed on to children.

In humans, we typically identify *de novo* mutations by sequencing trios (two parents and their child).

::: {#fig-dnm}
![](../img/Artboard 9.png){width=40%}
:::

::: {.notes}

OK. now that we've talked through the ingredients of a GLM, let's walk through a biological case study.

Selfishly, the example is based on data that I've previously analyzed myself, and has to do with the rate at which mutations occur in the human germline.
:::

## We expect to see around 70 DNMs per child

```{r}
#| fig-cap: De novo mutation counts in Utah families. Data from Sasani et al. (2019) *eLife*.
library(dplyr)

# read in DNM data
dnms = read.csv("https://raw.githubusercontent.com/quinlan-lab/ceph-dnm-manuscript/master/data/second_gen.dnms.txt", sep="\t")

# remove unphased DNMs
dnms = subset(dnms, phase != "na")

# group DNMs by sample id and phase
by_phase = dnms %>% count(new_sample_id, phase)
# group by sample alone to get totals
by_sample = dnms %>% count(new_sample_id)
by_sample$phase = "total"
merged = rbind(by_phase, by_sample)

ggplot(merged, aes(x=phase, y=n, col=phase)) +
    geom_boxplot(fill="white") +
    geom_jitter(size=2, width=0.15, alpha=0.5) +
    theme_cowplot() +
    labs(x="Parent of origin", y="Count")
```

### {.notes}
If we sequence a bunch of these trios, we can look at the distribution of counts observed in the children.

On average, we see that a child has about 70 new mutations that were absent from their parents. Most of these mutations come from dads.
:::

## Modeling the dependence between DNM counts and parental age

We also expect *de novo* mutation counts to depend on parental age. 

Each time a spermatogonial stem cell undergoes mitotic DNA replication, there's an opportunity for mutations to occur.

DNA damage also accumulates in germ cells over time, leading to new mutations.

. . .

\

##### How many additional germline mutations do we expect to see with each year of paternal age?

Let's build a GLM to find out.



## *De novo* mutation counts increase with age

```{r}
#| fig-cap: 
#|   - "*De novo* mutation counts observed in children as a function of paternal age at birth. Data from Sasani et al. (2019) *eLife*."
library(ggplot2)
library(dplyr)
library(cowplot)

dnms = read.csv("https://raw.githubusercontent.com/quinlan-lab/ceph-dnm-manuscript/master/data/second_gen.dnms.summary.csv")

ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point(size=5) +
    labs(x="Paternal age", y="Number of DNMs in child") +
    theme_cowplot()
```

::: {.notes}
It looks like there is a strong relationship between *de novo* mutation counts and paternal age.

However, a simple linear model probably isn't appropriate for modeling the relationship.

* *De novo* mutations are counts (not continuous, can't be less than 0)

* For count data, variance of errors/residuals is typically not constant as a function of $X$ (heteroscedasticity)

This is where GLMs can come in handy.
:::


## The Poisson distribution is useful for modeling count data

Remember the first ingredient we need to fit a GLM:

> An expected distribution for the $Y$ values ($\mu$)

Our model needs to appropriately model the mean and variance of our $Y$ values. Since we're dealing with count data, the **Poisson** makes sense.

. . .

The Poisson distribution tells us:

$$P(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!} \ \text{for} \ k = 0,1,2...$$

where $\lambda$ is the expected number of "events" (the *mean* we want to model in our GLM).

::: {.notes}
Whenever we're thinking about modeling count data, the Poisson distribution is usually a good choice.

As Aaron discussed...

So, whenever you see a \mu in our later equations (the expected value of Y given X), that's basically just the \lambda parameter from the Poisson -- the average number of events.
:::


## Modeling count data with the Poisson

Now we need the remaining ingredients for a GLM:

> A "link" $g$ between the our linear predictor $\eta$ and the conditional mean $\mu$ of the response variable.

. . .

In Poisson regression, the canonical link function $g(\mu)$ is the "log link":

$$\log(\mu) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

The log link makes sense for count data because it ensures that our predictions are always greater than 0.


::: {.notes}

Keep in mind here that we're modeling the mean of our response (counts) given our X values using a Poisson distribution.
Basically, our model's predictions are the "lambdas" that get fed into a Poisson distribution (the mean number of events we expect to see given some X value).

Why the "log link" for Poisson data? 

My modeling the log of the expected value (or put another way, exponentiating our predictions), we ensure that our predicted values are never negative. E to the something can never be negative
:::


## We can fit a simple Poisson model using R

\

```{r}
#| echo: true
#| eval: false
#| code-fold: false
m = glm(snv_dnms ~ dad_age, family=poisson(link = "log"))
```

Since the log link is the default for Poisson GLMs, this is the same as:

```{r}
#| echo: true
#| eval: false
#| code-fold: false
m = glm(snv_dnms ~ dad_age, family="poisson")
```


## Interpreting our Poisson GLM in `R`

Let's look at the model summary:

```{r}
m = glm(snv_dnms ~ dad_age, family="poisson", data=dnms)

summary(m)

```

::: {.notes}
The summary output here looks pretty much the same as what we saw on Tuesday with Michael's intro to linear modeling.

The major difference we'll focus on has to do with the coefficient estimates.
:::

## Interpreting our Poisson GLM in `R`

One key difference is that the **coefficients are on a different scale.**

Recall that our Poisson GLM is:

$$\log(\mu) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

. . .

This is the same as saying:

$$\mu = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$

(applying the inverse of our link function to the linear predictor)


::: {.notes}
Basically, we're saying that we can predict our Y values (\mu) by doing e to the (mx + b)
:::

## Interpreting our Poisson GLM in `R`

In our `summary()` output, let's look at the coefficients:

\

```{r}
#| echo: true
#| code-fold: false
coef(summary(m))
```

\

For a unit increase in `dad_age`, there is a multiplicative effect of $e^{0.02581} = 1.0261$ on the response variable (mean # of DNMs).

In other words, the number of DNMs observed in a child born to a 25-year-old father is $1.0261$ times larger than the number of DNMs observed in a child born to a 24-year-old father.


## Visualizing our Poisson model fit

```{r}
ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point(size=5) +
    geom_smooth(method="glm", method.args=list(family="poisson"), fullrange=TRUE, se=TRUE) +
    labs(x="Paternal age", y="Number of DNMs") +
    theme_cowplot()

```

## Explicitly modeling counts as *rates*

We only looked for mutations at genomic positions where at least 10 sequencing reads were aligned in mom, dad, and the child. 


```{r}
#| fig-width: 20
library(cowplot)

dnms$callable_bp_bill = dnms$autosomal_callable_fraction / 1e9

ggplot(dnms, aes(x=factor(sample_id), y=callable_bp_bill)) +
    geom_col() +
    labs(x = "Sample ID", y="Callable bp (billions)") +
    coord_cartesian(ylim = c(2.45, 2.6)) +
    theme_cowplot() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = 24), axis.text.y = element_text(size=24))

```

We were able to look for DNMs at ~2.6 billion positions in most children, but closer to ~2.4 billion in others.

Does this variable sequencing depth affect our observed DNM counts?

::: {.notes}
One final piece of the Poisson GLM I'll introduce is the concept of an "offset" or "exposure."

In our original de novo mutation study, we only looked for mutations at positions in the genome where we had at least 10 sequencing reads aligned in mom, dad, and the kid.

Due to differences in sequencing depth, this meant that in some trios, we could look for de novo mutations at many more sites than in others.

Now, we'd like to figure out whether that variable sequencing coverage may have impacted our ability to detect DNMs -- maybe the kids with lower numbers of DNMs simply had less sequencing coverage! This would be a huge problem! 
:::
 


## Explicitly modeling counts as *rates*

Our current model is:

$$\log(\mu) = \beta_0 + \beta_{1}X_{1}$$

where $\mu$ is the expected number of DNMs observed in a child given $X_1$,  the age of the child's father.

. . .

We actually want to model:
$$\log(\frac{\mu}{N}) = \beta_0 + \beta_{1}X_{1}$$

where $N$ is the number of genomic positions at which we had enough sequencing data in the trio to detect a mutation.

. . .

In other words, we want to model the expected number of DNMs in a child *per "callable" base pair* as a function of age.


## Explicitly modeling counts as rates

Thanks to logarithm rules (remember those?),

$$\log(\frac{\mu}{N}) = \beta_0 + \beta_{1}X_{1}$$

$$\log(\mu) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

In the context of Poisson GLMs, the $\log(N)$ term is sometimes referred to as "exposure" or "offset."

## Modeling with offsets in `R`

In `R`, we can easily update our linear model to account for an "offset."

Recall that our new model is

$$\log(\mu) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

where $N$ is the number of callable base pairs per trio.

. . .

In `R`, this looks like:

```{r}
#| echo: true
#| eval: false
#| code-fold: false
m = glm(
    all_dnms ~ dad_age + offset(log(callable_bp)), 
    family="poisson", 
    data=dnms,
)
```

## *De novo* mutation rates are robust to coverage

```{r}
m_with_offset = glm(
    all_dnms ~ dad_age + offset(log(autosomal_callable_fraction)), 
    family="poisson", 
    data=dnms,
    )

summary(m_with_offset)
```

The `dad_age` term is still significant, and the coefficient is very similar to its estimate in our "vanilla" Poisson model. 


## Take-home big picture

When fitting GLMs, you need to answer:

::: {.callout-note icon="false"}
## What distribution are my $Y$ values expected to follow?

This will determine the `family` you choose in the `glm` function
:::

::: {.callout-note icon="false"}
## How do I make the expected values of $Y$ compatible with my linear predictor?

This will determine the `link` you choose
:::

. . .


Be careful to interpret coefficients accordingly!

## Examples of `R` methods for various types of data

```{r}
#| echo: true
#| eval: false
#| code-fold: false

# simple linear regression, same as lm(y ~ x)
m = glm(y ~ x, family = gaussian(link = "identity"))
# logistic regression
m = glm(y ~ x, family = binomial(link = "logit"))
# poisson regression
m = glm(y ~ x, family = poisson(link = "log"))
```

<!-- ## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

2. Fitting linear models

(h/t to the [CS229 course notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) from Andrew Ng)

## Let's return to ordinary least squares

Let's look at the relationship between beak length and flipper length among the Palmer Penguins.

```{r}
#| echo: false
#| warning: false
# install.packages("palmerpenguins", repos='http://cran.us.r-project.org')
library(palmerpenguins)

ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g, col=species)) +
    geom_point() +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

## We can easily fit a linear model to these data

```{r}
ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g)) +
    geom_point() +
    geom_smooth(method="lm") +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

Our goal is to find a line that minimizes the residual sum of squares (RSS).

But how exactly can we find the coefficients that produce that line?

## Some setup and definitions

We want to approximate $y$ as a linear function of our predictors $x$.

$$h(x) = \theta_0 + \theta_{1}x_1$$

The $\theta$ values are our coefficients, or in machine learning lingo, our "parameters."

Given input values $x$, our function $h(x)$ will hopefully produce values that are very close to $y$. 

## Defining our "cost function"

In machine learning, we train models by finding model parameters that minimize a cost/loss function. 

Since we want our function $h(x)$ to produce values very close to the "target" values $y$, we can measure that "closeness" using a cost function $J$:

$$J(\theta) = \frac{1}{2}\sum_{i = 1}^{m}(h_{\theta}(x^i) - y^i)^2$$

## Gradient descent

To find the values of $\theta$ that minimize our cost function $J(\theta)$, we can start by picking a *totally random* initial value for $\theta$.

Then, we can iteratively update $\theta$ values by asking: if we increase $\theta$, does $J(\theta)$ go up or down? 

If increasing $\theta$ makes the cost function decrease, we should continue bumping up our values of $\theta$, and vice versa.

More formally, we perform the following:

$$\theta_j := \theta_j - \alpha\frac{\delta}{\delta\theta_j}J(\theta)$$

where $\alpha$ is a term called the "learning rate."

## Gradient descent (less formally)

In plain-ish English:

For the $j$th parameter, take the partial derivative of the cost function $J(\theta)$ with respect to $\theta_j$. That partial derivative will be negative if the cost function *decreases* when $\theta_j$ *increases*, and positive if the cost function *increases* when $\theta_j$ *increases*.

Then, update $\theta_j$ by subtracting the partial derivative multiplied by the learning rate. If the learning rate is large, our "updates" to $\theta_j$ will be large, and if the learning rate is small, our updates will be small.

**Note:** This will always update $\theta_j$ in the direction of decreasing the cost function!

## Gradient descent (without partial derivatives)

$$\theta_j := \theta_j - \alpha\sum_{i = 1}^{m}(y^i - h_\theta(x^i))x_j^i$$ -->
