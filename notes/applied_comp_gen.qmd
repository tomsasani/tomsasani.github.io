---
title: "Applied Computational Genomics"
subtitle: "4/4/2024"
author: "Tom Sasani"
format: 
    beamer:
        fig-width: 5
        fig-height: 3
execute:
  echo: false
---

## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

2. Fitting linear models like machine learning models

## Linear models are powerful, but limited

In a linear model, we model a response as a linear function of explanatory variables, along with an error term.

$$Y = \beta_0 + \beta_{1}X + \epsilon$$

<!-- ::: {.incremental} -->

. . .

$Y$: the *dependent variable*

$\beta_0$: the *intercept*

$\beta_1$: the *independent variable*

$\epsilon$: residual error unexplained by $\beta_0$ and $\beta_1$
<!-- ::: -->

## We make a lot of assumptions when we fit linear models

1. **Independence**: are the $Y$ observations independent of each other?

2. **Homoscedasticity**: is the variance of $Y$ constant as a function of $X$?

3. **Linearity**: is the relationship between $X$ and $Y$ linear?

4. **Normality**: for any given value of $X$, are the $Y$ values normally distributed?


## Checking the *independence* assumption

1. **Independence**: are the $Y$ observations independent of each other?

Example: AAPL stock price ($Y$) as a function of time ($X$).


## Checking the *homoscedasticity* assumption

`R` has a built-in function that lets us visually examine whether our model meets many of these assumptions.




## Checking the *linearity* assumption


## Checking the *normality* assumption


## Modeling other kinds of data

In biology, we're often interested in modeling data that don't conform to the assumptions of linear regression.

For example, *count data* are ubiquitous in genetics.

\

* Number of transcribed RNA molecules present in a cell for each protein-coding gene

* Number of *de novo* germline mutations observed in a child

## *De novo* germline mutations in human genomes

New mutations occur in germline (sperm or egg) cells and can be passed on to children.

In humans, we typically identify *de novo* mutations by sequencing trios (two parents and their child).

::: {#fig-elephants}

![](../img/Artboard 9.png){width=40%}

A single-nucleotide *de novo* mutation observed in a trio
:::

## We expect the number of germline mutations to increase with parental age

Spermatagonial stem cells constantly self-renew throughout life (~20-30 cell divisions per year).

Each time a stem cell undergoes mitotic DNA replication, there's an opportunity for mutations to occur.

\

**Question:** How many additional germline mutations do we expect to see with each year of paternal age?

## Modeling *de novo* mutation accumulation with age

Some basic assumptions:

* Children can never have negative counts of *de novo* mutations

* We expect the number of *de novo* mutations to increase with parental age

. . .

Let's model the 

## The Poisson distribution is useful for modeling count data

$$P(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!} \ \text{for} \ k = 0,1,2...$$

$\lambda$ is the *rate parameter* (e.g., how frequently do we expect the event to occur in an interval of time?)

\

In the Poisson distribution, $\lambda = E(Y) = Var(Y)$. 

In other words, the variance of $Y$ is *not constant*, but is equal to the mean of $Y$.


## Modeling count data with the Poisson

We model the *logarithm* of our response variable as a linear combination of the independent variables.

$$\log(E[Y]) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

where $E[Y]$ is the expected value of $Y$, and $E[Y] = \lambda$.


If we exponentiate both sides of the equation,

$$E[Y] = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$

where $e$ is Euler's number (2.7182...).

## Let's apply Poisson regression to *de novo* mutation data

```{r}
#| echo: false
#| fig-cap: 
#|   - "*De novo* mutation counts observed in children as a function of paternal age at birth. Data from Sasani et al. (2019) *eLife*."
library(ggplot2)
library(cowplot)

dnms = read.csv("https://raw.githubusercontent.com/quinlan-lab/ceph-dnm-manuscript/master/data/second_gen.dnms.summary.csv")

names(dnms)[names(dnms) == "autosomal_callable_fraction"] <- "callable_bp"

ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    labs(x="Paternal age", y="Total number of DNMs") +
    theme_cowplot()
```

## We can fit a simple Poisson model using `R`

\footnotesize
```{r}
#| echo: true

m = glm(snv_dnms ~ dad_age, family="poisson", data=dnms)
```

## Interpreting our Poisson GLM in `R`

Let's look at the model summary:

\scriptsize
```{r}
summary(m)
```

## Interpreting our Poisson GLM in `R`

Just like a typical `summary()` output, we can see the coefficient estimates and p-values for each of our independent variables.

One key difference is that the coefficients are on a logarithmic scale.

Recall that:

$$\log(E[Y]) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

and thus,

$$E[Y] = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$

To convert our coefficients into a "more interpretable" value, we should exponentiate them.

## Interpreting our Poisson GLM in `R`

In our `summary()` output, let's look at the coefficients:

\

\footnotesize
```{r}
#| echo: true
coef(summary(m))
```

\

\normalsize

For a unit increase in `dad_age`, there is a multiplicative effect of $e^{0.02479512} = 1.0251$ on the response variable (# of DNMs).

In other words, the number of DNMs observed in a child born to a 25-year-old father is $1.0251$ times larger than the number of DNMs observed in a child born to a 24-year-old father.

## Visualizing our Poisson model fit

\tiny
```{r}
#| echo: true
ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    geom_smooth(method="glm", method.args=list(family="poisson"), fullrange=TRUE, se=TRUE) +
    labs(x="Paternal age", y="Number of DNMs")
```

## Explicitly modeling counts as *rates*

Sometimes, count data are best understood as rates. 

In Sasani et al. (2019), we only looked for mutations at genomic positions where at least 10 sequencing reads were aligned in mom, dad, and the child. 

There are roughly $3 \times 10^9$ nucleotides in the human genome, but many of them occur in low-complexity and repetitive sequence. We might only be able to sequence ~80-90% of those nucleotides, and that "callable" fraction will vary across trios.


## Explicitly modeling counts as *rates*

Our current model is:

$$\log(E[Y]) = \beta_0 + \beta_{1}X_{1}$$

where $E[Y]$ is the expected number of DNMs observed in a child and $X$ is the age of the child's father.

We actually want to model:
$$\log(\frac{E[Y]}{N}) = \beta_0 + \beta_{1}X_{1}$$

where $N$ is the number of genomic positions at which we had enough sequencing data in the trio to detect a mutation.

In other words, we want to model the number of DNMs *per "callable" base pair* as a function of age.


## Explicitly modeling counts as rates

Thanks to logarithm rules (remember those?),

$$\log(\frac{E[Y]}{N}) = \beta_0 + \beta_{1}X_{1}$$

$$\log(E[Y]) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

In the context of linear models, the $\log(N)$ term is sometimes referred to as "exposure" or "offset."

## Modeling with offsets in `R`

In `R`, we can easily update our linear model to account for an "offset."

Recall that our new model is

$$\log(E[Y]) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

where $N$ is the number of callable base pairs per trio.

In `R`, this looks like:

\

\footnotesize
```{r}
#| echo: true
m = glm(
    all_dnms ~ dad_age + offset(log(callable_bp)), 
    family="poisson", 
    data=dnms,
)
```

## The ingredients of a GLM

1. A linear predictor 

How does the response depend on independent variables?

$$\mu_i = \beta_0 + \beta_{1}x_{1i} + ... + \beta_{p}x_{pi}$$

2. A "link" function $\eta(\mu)$

How does the mean of the response variable depend on the linear predictor?

For Poisson regression, $\eta(\mu) = \log(\mu)$.

For "standard" linear regression, $\eta(\mu) = \mu$.

## Lots of generalized linear models out there

For example, logistic regression can be formulated as:

```{r}
#| eval: false
#| echo: true
m = glm(Y ~ X, family="binomial", link="logit")
```


## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

2. Fitting linear models

(h/t to the [CS229 course notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) from Andrew Ng)

## Let's return to ordinary least squares

Let's look at the relationship between beak length and flipper length among the Palmer Penguins.

```{r}
#| echo: false
#| warning: false
# install.packages("palmerpenguins", repos='http://cran.us.r-project.org')
library(palmerpenguins)

ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g, col=species)) +
    geom_point() +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

## We can easily fit a linear model to these data

```{r}
ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g)) +
    geom_point() +
    geom_smooth(method="lm") +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

Our goal is to find a line that minimizes the residual sum of squares (RSS).

But how exactly can we find the coefficients that produce that line?

## Some setup and definitions

We want to approximate $y$ as a linear function of our predictors $x$.

$$h(x) = \theta_0 + \theta_{1}x_1$$

The $\theta$ values are our coefficients, or in machine learning lingo, our "parameters."

Given input values $x$, our function $h(x)$ will hopefully produce values that are very close to $y$. 

## Defining our "cost function"

In machine learning, we train models by finding model parameters that minimize a cost/loss function. 

Since we want our function $h(x)$ to produce values very close to the "target" values $y$, we can measure that "closeness" using a cost function $J$:

$$J(\theta) = \frac{1}{2}\sum_{i = 1}^{m}(h_{\theta}(x^i) - y^i)^2$$

## Gradient descent

To find the values of $\theta$ that minimize our cost function $J(\theta)$, we can start by picking a *totally random* initial value for $\theta$.

Then, we can iteratively update $\theta$ values by asking: if we increase $\theta$, does $J(\theta)$ go up or down? 

If increasing $\theta$ makes the cost function decrease, we should continue bumping up our values of $\theta$, and vice versa.

More formally, we perform the following:

$$\theta_j := \theta_j - \alpha\frac{\delta}{\delta\theta_j}J(\theta)$$

where $\alpha$ is a term called the "learning rate."

## Gradient descent (less formally)

In plain-ish English:

For the $j$th parameter, take the partial derivative of the cost function $J(\theta)$ with respect to $\theta_j$. That partial derivative will be negative if the cost function *decreases* when $\theta_j$ *increases*, and positive if the cost function *increases* when $\theta_j$ *increases*.

Then, update $\theta_j$ by subtracting the partial derivative multiplied by the learning rate. If the learning rate is large, our "updates" to $\theta_j$ will be large, and if the learning rate is small, our updates will be small.

**Note:** This will always update $\theta_j$ in the direction of decreasing the cost function!

## Gradient descent (without partial derivatives)

$$\theta_j := \theta_j - \alpha\sum_{i = 1}^{m}(y^i - h_\theta(x^i))x_j^i$$
