---
title: "Applied Computational Genomics"
subtitle: "4/4/2024"
author: "Tom Sasani"
format: 
    beamer:
        fig-width: 5
        fig-height: 3
execute:
  echo: false
---

## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

2. Ordinary least squares (OLS) as machine learning

## notes

lienar model assumes sepcifi form for mean

u(X) = X.Tbeta

GLM

lienar because X values get transformed by X.Tbeta -- applying linear transformation

Random componeent

before, Y belonged to gaussian, now we say that Y|X belongs to some specific distribution (exponential family)

Link function

before, u(X) = X.TB directly related. 

now, we extend this such that some function g(u(X)) is equal to X.Tbeta

why do we need a link function? 

what is u(X) if we're dealing with Poisson data? 0, inf (positive)
but what is X.Tb? could be anything on the real number line

need a way to map our 
## Linear models are powerful, but limited

In a simple linear model, we model a dependent (response) variable using an independent (explanatory) variable, along with an error term.

$$Y = \beta_0 + \beta_{1}X_1 + \epsilon$$

. . .

$Y$: the *dependent variable*

$\beta_0$: the *intercept* term

$\beta_1$: the coefficient associated with our *independent variable*

$X_1$: the *independent variable*

$\epsilon$: residual error unexplained by $\beta_0$ and $\beta_1$
<!-- ::: -->

## We make a number of assumptions when we fit linear models

1. **Linearity**: is the relationship between $X$ and $Y$ linear?

2. **Independence**: are the $Y$ observations independent of each other?

3. **Homoscedasticity**: is the variance of $Y$ constant as a function of $X$?

4. **Normality**: for any given value of $X$, are the $Y$ values normally distributed?


<!-- ## Checking the *linearity* assumption

Enzymatic reaction velocity vs. substrate concentration in cells that were treated with puromycin.

```{r}
library(datasets)
library(ggplot2)
library(cowplot)

ggplot(subset(Puromycin, state == "treated"), aes(x=conc, y=rate)) +
    geom_point() +
    theme_cowplot() +
    theme(text = element_text(size = 9), 
    axis.text.x = element_text(size=9), 
    axis.text.y = element_text(size=9)) +
    labs(x="Substrate concentration (ppm)", y="Reaction rate (counts/min/min)")
```

## Checking the *linearity* assumption

It seems like these data do *not* follow a linear trend, but we can check that more explicitly by looking at the model residuals.

In `R`, if we fit a model `m = y ~ x`, we can simply type `plot(m)` to see diagnostic plots.

\footnotesize
```{r}
m = lm(rate ~ conc, data=Puromycin)
plot(m, which=c(1, 1))
```


## Checking the *homoscedasticity* assumption

```{r}
m = lm(wt ~ mpg, data=mtcars)
plot(m, which=c(3, 3))
```

## Checking the *normality* assumption

```{r}
m = lm(wt ~ mpg, data=mtcars)
plot(m, which=c(2, 2))
```

## Checking the *independence* assumption

```{r}
library(tidyquant)
library(cowplot)

aapl <- tq_get('AAPL',
               from = "2017-01-01",
               to = "2018-03-01",
               get = "stock.prices")

ggplot(aapl, aes(x=date, y=adjusted)) +
    geom_line() +
    labs(x="Date", y="Adjusted AAPL stock price ($)") +
    theme_cowplot() +
    theme(text = element_text(size = 9), 
    axis.text.x = element_text(size=9), 
    axis.text.y = element_text(size=9))
```

Apple stock prices on consecutive days are not independent! -->

## Modeling other kinds of data

In biology, we're often interested in modeling data that don't conform to the assumptions of simple linear regression.

### Count data

* Number of DNA sequencing reads aligned to a particular genomic position

### Binary data

* Presence or absence of a species in a microbiome sample

### Discrete categorical data

* Cell type identity in a large single-cell RNA-seq experiment

\

To model these data types, we can turn to **generalized linear models** (GLMs).


## GLM case study: *de novo* mutations in human genomes

New mutations occur in germline (sperm or egg) cells and can be passed on to children.

In humans, we typically identify *de novo* mutations by sequencing trios (two parents and their child).

::: {#fig-dnm}
![](../img/Artboard 9.png){width=40%}

Example of a *de novo* single-nucleotide mutation
:::

## We expect to see around 70 DNMs per child

::: {fig-decode}

![](../img/sasani_dnm_fig1.jpg){width=70%}

Data from Sasani et al. (2019) *eLife*
:::

Since *de novo* mutations are a form of *count data*, we can model them using Poisson models.


## The Poisson distribution is useful for modeling count data

$$P(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!} \ \text{for} \ k = 0,1,2...$$

where $\lambda$ is the expected number of "events."

We expect to see an average of $\lambda = 70$ *de novo* mutations (DNMs) per child.

. . . 

The probability that a child has exactly $75$ DNMs is:

$$P(Y = 75) = \frac{(e^{-70})(70^{75})}{75!}$$

. . . 

\footnotesize
```{r}
#| echo: true
print ((exp(-70) * (70^75)) / factorial(75))
```


## Modeling the dependence between DNM counts and parental age

We expect *de novo* mutation counts to depend on parental age. 

Spermatagonial stem cells constantly self-renew throughout life (~20-30 cell divisions per year).

Each time a stem cell undergoes mitotic DNA replication, there's an opportunity for mutations to occur.

DNA damage also accumulates in germ cells over time, leading to new mutations.

\

**Question:** How many additional germline mutations do we expect to see with each year of paternal age?

<!-- ## Modeling *de novo* mutation accumulation with age

Some basic assumptions:

* Numbers of *de novo* mutations are not continuous

* Children can never have negative counts of *de novo* mutations

* We expect the number of *de novo* mutations to increase with parental age -->


## *De novo* mutation counts increase with age

```{r}
library(ggplot2)
library(dplyr)
library(cowplot)

dnms = read.csv("https://raw.githubusercontent.com/quinlan-lab/ceph-dnm-manuscript/master/data/second_gen.dnms.summary.csv")

ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    labs(x="Paternal age", y="Number of DNMs in child") +
    theme_cowplot()

```

Data from Sasani et al. (2019) *eLife*.

## *De novo* mutation counts increase with age

It looks like there is a strong relationship between *de novo* mutation counts and paternal age.

However, a simple linear model probably isn't appropriate for modeling the relationship.

* *De novo* mutation counts are not continuous

* Variance of errors/residuals is not constant as a function of $X$

This is where GLMs can come in handy.


## The basic ingredients of a generalized linear model

1. An expected distribution for the $Y$ values (belonging to the exponential family)

. . .

2. A "link" $g$ between the conditional mean $E(Y | X) = \mu$ and our linear predictor:

$$g(E(Y | X)) = g(\mu) = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_{p} = X^{\top}\beta$$

| GLM type | canonical link function |
| - | - |
| Simple linear regression | $g(\mu) = \mu$ |
| Poisson regression | $g(\mu) = \log(\mu)$ |
| Logistic regression | $g(\mu) = \log\frac{\mu}{1 - \mu}$ |

## Why do we need a "link" function?

In ordinary linear regression, we approximate the conditional mean $\mu = E(Y | X)$ by applying a linear transformation to our input variables $X$.

$$E(Y | X) = \beta_0 + \beta_{1}X_1 + ... + \beta_{p}X_{p}$$

But what if $E(Y | X)$ is restricted between e.g., $(0, 1)$ (binary data)?

. . .

We need to make sure that the values on both sides of the $=$ sign are compatible, and our predictions (from the right side of the equation) aren't nonsense. 

Our link function maps $E(Y | X)$ to the real number line so that we can use a *linear* predictor to find coefficients.
<!-- ## Modeling count data with the Poisson

Recall that the Poisson distribution is parameterized by $\lambda$ (the mean number of events):

$$P(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!} \ \text{for} \ k = 0,1,2...$$

We want to model the expected value of $Y$ given our input $X$ values:

$$E(Y | X)$$

which, in Poisson regression, is the same as saying:

$$E(\lambda | X)$$ -->

## Modeling count data with the Poisson

In Poisson regression, the "canonical" link function $g(\mu)$ is the "log link" $\log(\mu)$, so we model the *logarithm* of $\mu = E(Y | X)$:

$$\log(E(Y | X)) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

If we exponentiate both sides of the equation,

$$E(Y | X) = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$

where $e$ is Euler's number (2.7182...).

<!-- ## Why not take the log of our dependent variable instead?

Given input data $X$ and $Y$, why not just take $\log(Y)$ and use simple linear regression to model our data?

```{r}
dnms$log_snv_dnms = log(dnms$snv_dnms)

ggplot(dnms, aes(x=dad_age, y=log_snv_dnms)) +
    geom_point() +
    labs(x="Paternal age", y="Number of DNMs (log-transformed)") +
    theme_cowplot()
```

## Why not log-transform our input data?

There's a subtle (but important) difference between log-transforming our $Y$ values and modeling the logarithm of the *expected value* of our $Y$ values.

In other words,

$$E[\log(Y) | X] \neq \log(E[Y | X])$$ -->

## Let's apply Poisson regression to *de novo* mutation data

```{r}
#| echo: false
#| fig-cap: 
#|   - "*De novo* mutation counts observed in children as a function of paternal age at birth. Data from Sasani et al. (2019) *eLife*."
library(ggplot2)
library(cowplot)

dnms = read.csv("https://raw.githubusercontent.com/quinlan-lab/ceph-dnm-manuscript/master/data/second_gen.dnms.summary.csv")

names(dnms)[names(dnms) == "autosomal_callable_fraction"] <- "callable_bp"

ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    labs(x="Paternal age", y="Total number of DNMs") +
    theme_cowplot()
```

## We can fit a simple Poisson model using `R`

\footnotesize
```{r}
#| echo: true

m = glm(snv_dnms ~ dad_age, family=poisson(link = "log"), data=dnms)
```

## Interpreting our Poisson GLM in `R`

Let's look at the model summary:

\scriptsize
```{r}
summary(m)
```

## Interpreting our Poisson GLM in `R`

Just like a typical `summary()` output, we can see the coefficient estimates and p-values for each of our independent variables.

One key difference is that the coefficients are on a logarithmic scale.

Recall that:

$$\log(E(Y | X)) = \beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$$

and thus,

$$E(Y | X) = e^{\beta_0 + \beta_{1}X_{1} + ... + \beta_{p}X_{p}}$$


## Interpreting our Poisson GLM in `R`

In our `summary()` output, let's look at the coefficients:

\

\footnotesize
```{r}
#| echo: true
coef(summary(m))
```

\

\normalsize

For a unit increase in `dad_age`, there is a multiplicative effect of $e^{0.02479512} = 1.0251$ on the response variable (# of DNMs).

In other words, the number of DNMs observed in a child born to a 25-year-old father is $1.0251$ times larger than the number of DNMs observed in a child born to a 24-year-old father.


## Visualizing our Poisson model fit

```{r}
#| echo: false
ggplot(dnms, aes(x=dad_age, y=snv_dnms)) +
    geom_point() +
    geom_smooth(method="glm", method.args=list(family="poisson"), fullrange=TRUE, se=TRUE) +
    labs(x="Paternal age", y="Number of DNMs") +
    theme_cowplot()
```

## Explicitly modeling counts as *rates*

In Sasani et al. (2019), we only looked for mutations at genomic positions where at least 10 sequencing reads were aligned in mom, dad, and the child. 


```{r}
#| fig-width: 4
#| fig-height: 1.5
library(cowplot)

dnms$callable_bp_bill = dnms$callable_bp / 1e9

# print (dnms$callable_bp_bill)

ggplot(dnms, aes(x=factor(sample_id), y=callable_bp_bill)) +
    geom_col() +
    labs(x = "Sample ID", y="Callable bp (billions)") +
    #ylim(c(2, 3)) +
   coord_cartesian(ylim = c(2.45, 2.6)) +
    theme_cowplot() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = 9), axis.text.y = element_text(size=9))
```

We were able to look for DNMs at ~2.6 billion positions in most children, but closer to ~2.4 billion in others.

Does this variable sequencing depth affect our observed DNM counts?


## Explicitly modeling counts as *rates*

Our current model is:

$$\log(E(Y | X)) = \beta_0 + \beta_{1}X_{1}$$

where $E(Y | X)$ is the conditional mean number of DNMs observed in a child given the age of the child's father.

We actually want to model:
$$\log(\frac{E(Y | X)}{N}) = \beta_0 + \beta_{1}X_{1}$$

where $N$ is the number of genomic positions at which we had enough sequencing data in the trio to detect a mutation.

In other words, we want to model the expected number of DNMs in a child *per "callable" base pair* as a function of age.


## Explicitly modeling counts as rates

Thanks to logarithm rules (remember those?),

$$\log(\frac{E(Y | X)}{N}) = \beta_0 + \beta_{1}X_{1}$$

$$\log(E(Y | X)) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

In the context of linear models, the $\log(N)$ term is sometimes referred to as "exposure" or "offset."

## Modeling with offsets in `R`

In `R`, we can easily update our linear model to account for an "offset."

Recall that our new model is

$$\log(E(Y | X)) = \beta_0 + \beta_{1}X_{1} - \log(N)$$

where $N$ is the number of callable base pairs per trio.

In `R`, this looks like:

\small
```{r}
#| echo: true
m = glm(
    all_dnms ~ dad_age + offset(log(callable_bp)), 
    family="poisson", 
    data=dnms,
)
```

## Lots of generalized linear models out there

For example, logistic regression can be formulated as:

```{r}
#| eval: false
#| echo: true
m = glm(Y ~ X, family="binomial", link="logit")
```

### Take-home big picture

When fitting GLMs, you need to answer:

1. What distribution are my $Y$ values expected to follow?

* i.e., what `family` do I choose in the `glm` function?

2. How do I make the expected values of $Y$ compatible with my linear predictor?

* i.e., what `link` do I choose?

Be careful to interpret coefficients accordingly!



## Goals for today's lecture

1. An introduction to generalized linear models (GLMs)

2. Fitting linear models

(h/t to the [CS229 course notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) from Andrew Ng)

## Let's return to ordinary least squares

Let's look at the relationship between beak length and flipper length among the Palmer Penguins.

```{r}
#| echo: false
#| warning: false
# install.packages("palmerpenguins", repos='http://cran.us.r-project.org')
library(palmerpenguins)

ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g, col=species)) +
    geom_point() +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

## We can easily fit a linear model to these data

```{r}
ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g)) +
    geom_point() +
    geom_smooth(method="lm") +
    labs(x="Flipper length (mm)", y="Body mass (g)")
```

Our goal is to find a line that minimizes the residual sum of squares (RSS).

But how exactly can we find the coefficients that produce that line?

## Some setup and definitions

We want to approximate $y$ as a linear function of our predictors $x$.

$$h(x) = \theta_0 + \theta_{1}x_1$$

The $\theta$ values are our coefficients, or in machine learning lingo, our "parameters."

Given input values $x$, our function $h(x)$ will hopefully produce values that are very close to $y$. 

## Defining our "cost function"

In machine learning, we train models by finding model parameters that minimize a cost/loss function. 

Since we want our function $h(x)$ to produce values very close to the "target" values $y$, we can measure that "closeness" using a cost function $J$:

$$J(\theta) = \frac{1}{2}\sum_{i = 1}^{m}(h_{\theta}(x^i) - y^i)^2$$

## Gradient descent

To find the values of $\theta$ that minimize our cost function $J(\theta)$, we can start by picking a *totally random* initial value for $\theta$.

Then, we can iteratively update $\theta$ values by asking: if we increase $\theta$, does $J(\theta)$ go up or down? 

If increasing $\theta$ makes the cost function decrease, we should continue bumping up our values of $\theta$, and vice versa.

More formally, we perform the following:

$$\theta_j := \theta_j - \alpha\frac{\delta}{\delta\theta_j}J(\theta)$$

where $\alpha$ is a term called the "learning rate."

## Gradient descent (less formally)

In plain-ish English:

For the $j$th parameter, take the partial derivative of the cost function $J(\theta)$ with respect to $\theta_j$. That partial derivative will be negative if the cost function *decreases* when $\theta_j$ *increases*, and positive if the cost function *increases* when $\theta_j$ *increases*.

Then, update $\theta_j$ by subtracting the partial derivative multiplied by the learning rate. If the learning rate is large, our "updates" to $\theta_j$ will be large, and if the learning rate is small, our updates will be small.

**Note:** This will always update $\theta_j$ in the direction of decreasing the cost function!

## Gradient descent (without partial derivatives)

$$\theta_j := \theta_j - \alpha\sum_{i = 1}^{m}(y^i - h_\theta(x^i))x_j^i$$
