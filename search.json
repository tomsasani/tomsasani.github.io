[
  {
    "objectID": "ml_reading_group/2024_intro.html",
    "href": "ml_reading_group/2024_intro.html",
    "title": "ML for genomics, an evolving reading list",
    "section": "",
    "text": "Introductions to ML\nThe CS230 course at Stanford has made slides and lectures available online. A fantastic resource.\nThe Amidi brothers have created cheat sheets for CS230 and other Stanford ML courses. Also great.\nAndrej Karpathy’s videos are also excellent introductions to ML concepts. In his own words, the first video in his main series (called The spelled-out intro to neural networks and backpropagation: building micrograd) “only assumes basic knowledge of Python and a vague recollection of calculus from high school.” Highly recommended.\n\n\n\n\n\n\nReview articles\n\n\n\n\n\nDeep learning: new computational modeling techniques for genomics\n\nAn excellent and detailed overview, probably where I’d start.\n\nA primer on deep learning in genomics\nHarnessing deep learning for population genetic inference\nNavigating the pitfalls of applying machine learning in genomics\n\nExcellent overview of pitfalls and possible mistakes that can confound ML analyses, with a particular focus on biological inference.\n\nOpportunities and obstacles for deep learning in medicine\nSupervised machine learning for population genetics: a new paradigm\nTo transformers and beyond: large language models for the genome\n\n\n\n\n\nApplications\nPapers are broadly grouped by ML architecture. Many of these papers involve a mix of architectures, so the groups should be considered “fuzzy.”\n\n\n\n\n\n\nTransformers\n\n\n\n\n\nEffective gene expression prediction from sequence by integrating long-range interactions\n\nDescribes the “Enformer” model, which utilizes a transformer-based architecture to predict gene expression from sequence alone. Also see a suite of papers describing the limitations of “Enformer” for personal transcriptome inference.\n\nPredicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation\n\n\n\n\n\n\n\n\n\nGenerative adversarial networks (GANs)\n\n\n\n\n\nAutomatic inference of demographic parameters using generative adversarial networks\n\nThe authors describe a novel GAN architecture that that features a population genetic simulator (in this case, the backwards-in-time msprime tool) as the “generator” and a convolutional neural network as the “discriminator”. The parameters of the msprime generator are randomly initialized, and the discriminator is trained to differentiate between simulated and real “images” of haplotypes in genomic regions of a predefined size. Over time, the generator gets better at simulating realistic-looking data and the discriminator gets better at telling the two classes of data apart. By the end of training, the generator can be interpreted by examining the population genetic parameters (population size, mutation rate, etc.) that optimally confused the discriminator. A well-written and clear overview of a cool (and interpretable) method.\n\nInterpreting generative adversarial networks to infer natural selection from genetic data\n\nA follow-up the the paper listed above. The authors fine-tune the trained discriminator from their GAN to infer regions of the genome under the effects of natural selection.\n\n\n\n\n\n\n\n\n\n\nLanguage models (LMs)\n\n\n\n\n\nDNA language models are powerful predictors of genome-wide variant effects\nGPN-MSA: an alignment-based DNA language model for genome-wide variant effect prediction\nThe nucleotide transformer: building and evaluating robust foundation models for human genomics\n\nA recent attempt to build a “foundation model” for genomics. The authors essentially adapt BERT for DNA sequence by developing an encoder-only architecture that attempts to reconstruct randomly-masked 6-mer DNA “tokens.” The learned embeddings from an input DNA sequence can then be plugged into simple regression models to make predictions about chromatin accessibility, enhancer status, etc., or the model itself can be efficiently fine-tuned for a particular downstream classification task.\n\n\n\n\n\n\n\n\n\n\nConvolutional neural networks (CNNs)\n\n\n\n\n\nThe unreasonable effectiveness of convolutional neural networks in population genetic inference\n\nCheck this paper out for a nice introduction to CNNs and how they can be applied to “images” of haplotypes in genomic regions. The associated GitHub repository includes a few simple models (written in TensorFlow/Keras), as well.\n\nBasset: Learning the regulatory code of the accessible genome with deep convolutional neural networks\nSequential regulatory activity prediction across chromosomes with convolutional neural networks\nLocalizing post-admixture adaptive variants with object detection on ancestry-painted chromosomes\n\nThe authors train an off-the-shelf object detection model to identify genomic regions with recent (adaptive) admixture events. Nice example of using off-the-shelf models, rather than building architectures from scratch.\n\nDiscovery of ongoing selective sweeps within Anopholes mosquito populations using deep learning\n\nA nice example of training CNNs to detect selection using pre-computed features (e.g., a large collection of population genetic summary statistics) rather than “painted haplotype” images.\n\n\n\n\n\n\n\n\n\n\nAutoencoders\n\n\n\n\n\nVisualizing population structure with variational autoencoders\n\nThe authors use a variational autoencoder (VAE) to embed sample genotype vectors into a 2-dimensional latent space that reflects geographical origin.\n\nHaplotype and population structure inference using neural networks in whole-genome sequencing data\nA deep learning framework for characterization of genotype data"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I develop and apply computational methods to study mutation rate and spectrum evolution in mammalian genomes. Here are a few research questions I’ve pursued in the past:\n\nHow does inherited genetic variation impact the germline mutation rate?\n\nRelated paper: A natural mutator allele shapes mutation spectrum variation in mice\n\n\n\nTo what degree do mutation rates vary among human families?\n\nRelated paper: Large, three-generation human families reveal post-zygotic mosaicism and variability in germline mutation accumulation\n\n\n\nHow do structural and single-nucleotide variants interact to promote rapid adaptation?\n\nRelated paper: Long read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays"
  },
  {
    "objectID": "notes/snakemake.html",
    "href": "notes/snakemake.html",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "",
    "text": "A quick-start guide for Snakemake",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#motivation",
    "href": "notes/snakemake.html#motivation",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Motivation",
    "text": "Motivation\nThere are few (if any) scientific questions that you can answer by running a single program or script. Calling variants involves aligning reads, sorting reads, indexing reads, running a variant caller, filtering those calls, etc. Running a simulation will inevitably require changing parameters or including new combinations of those parameters.\nAnd there are few (if any) programs or scripts that will run/compile correctly on the first try, due to either user error, code bugs, dependency conflicts, improperly configured environments, or any combination therein.\nFor these reasons, it’s often crucial to package the various steps of your analysis into a “pipeline.” Ideally, this pipeline would accept a file(s) as input, do some stuff with that file, and generate an output file(s). For example, your pipeline might take a FASTQ file and reference genome FASTA as input, and output an aligned, sorted, and indexed BAM.\nIn theory, a pipeline could just be a Bash script in which you enumerate each step of the process. But what if you want to run the pipeline on hundreds of samples’ FASTQ files? And what if the final step in your Bash script fails? You’ll have to re-run the entire pipeline all over again.\nThis is where Snakemake comes in. Snakemake is a flexible Python-based pipeline manager, and it’s even tuned for running on the Sage Grid Engine (or pretty much any other compute environment).\n\nAs an example, let’s imagine that we want to take paired-end FASTQ from 3 different mouse samples (A, B, and C) and generate a preliminary set of variant calls for each sample.\n\n\nTo start, let’s imagine we only want to process one sample: A.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#every-step-of-the-pipeline-gets-its-own-rule",
    "href": "notes/snakemake.html#every-step-of-the-pipeline-gets-its-own-rule",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Every step of the pipeline gets its own “rule”",
    "text": "Every step of the pipeline gets its own “rule”\nThe first step of the pipeline will be to download an M. musculus reference genome so that we can align reads.\nrule download_reference:\n  input:\n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nThis rule, which is named download_reference, doesn’t take any input, since it’s just downloading a FASTA directly.\n\n\nWe specify that the expected output of this rule is a single gzipped FASTA.\n\n\nAfter shell:, we simply list the commands we’d normally type at the command line to produce the specified output. These commands can be wrapped up in a docstring for easy formatting.\n\nNext, we want to align the FASTQ data from sample A to the reference.\nrule bwa_align:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"A_1.fastq.gz\",\n    fq2 = \"A_2.fastq.gz\"\n  output:\n    \"A.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nThis rule takes as input a reference genome and two FASTQ files. As you can see, it’s possible to name individual input or output files (using Python variable assignment) so that we can access particular files in our shell command.\n\n\nNote that if a rule has more than one input (or output) files, they should be comma-separated.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#snakemake-will-only-run-a-rule-if-it-has-to",
    "href": "notes/snakemake.html#snakemake-will-only-run-a-rule-if-it-has-to",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Snakemake will only run a rule if it has to",
    "text": "Snakemake will only run a rule if it has to\nLet’s take a look at the full pipeline we’ve defined so far.\nrule all:\n  input:\n    \"A.sorted.bam\"\n\nrule download_reference:\n  input:\n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"A_1.fastq.gz\",\n    fq2 = \"A_2.fastq.gz\"\n  output:\n    \"A.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nA huge advantage of Snakemake is that it will only run a rule if its output is needed by a downstream rule.\n\n\nYou can see that I’ve added a rule (called all) to the top of the pipeline. This is because Snakemake runs in a “bottom-up” fashion.\n\n\nThe all rule tells Snakemake what the final output of the entire pipeline should be. In this case, we want the final output to be an aligned BAM.\n\n\nIn this example, Snakemake finds the rule that outputs A.sorted.bam (which is bwa_align), and checks to see if that rule has access to all of its necessary inputs (a reference and two FASTQ files). If not, Snakemake finds the rules that produce those files and runs them. And so on. Once bwa_align has all of the inputs it needs, Snakemake runs it to produce the final output.\n\n\nAnd let’s say that for whatever reason, bwa_align fails when its run. When we re-run the pipeline, Snakemake will check that its input files are present. Since all of the previous rules will have been run before invoking bwa_align, Snakemake will see that its inputs are present and won’t re-run any of the upstream steps!",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#using-wildcards-to-avoid-re-writing-the-same-command-over-and-over-again",
    "href": "notes/snakemake.html#using-wildcards-to-avoid-re-writing-the-same-command-over-and-over-again",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using wildcards to avoid re-writing the same command over and over again",
    "text": "Using wildcards to avoid re-writing the same command over and over again\nIn the previous example, we were only running the pipeline on a single sample. But to generalize the pipeline to run on any list of samples, we can make use of the expand feature, as well as Snakemake “wildcards.”\nWere this a bash pipeline we’d have to write out a separate set of commands for every sample. But with Snakemake it’s much easier.\nHere’s an example of what our pipeline would look like with wildcard placeholders instead of explicit sample names.\nsamples = [\"A\", \"B\", \"C\"]\n\nrule all:\n  input:\n    expand(\"{sample}.sorted.bam\", sample=samples)\n\nrule download_reference:\n  input: \n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input: \n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"{sample}_1.fastq.gz\",\n    fq2 = \"{sample}_2.fastq.gz\"\n  output:\n    \"{sample}.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nHere, I’ve just replaced every instance of a sample name in an output or input file with a {sample} wildcard.\n\n\nIn the all rule, I’m using the expand function to tell Snakemake that the expected output is a list of sorted BAMs, with the sample names in samples filled in.\n\nIn this case, the result of the expand would just be:\n&gt;&gt;&gt; expand(\"{sample}.sorted.bam\", sample=samples)\n\n[\"A.sorted.bam\", \"B.sorted.bam\", \"C.sorted.bam\"]\nBut we can also use expand for more complicated chaining of multiple sample names and parameters.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#using-expand-to-run-a-pipeline-on-many-samples-or-with-many-parameters",
    "href": "notes/snakemake.html#using-expand-to-run-a-pipeline-on-many-samples-or-with-many-parameters",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using expand to run a pipeline on many samples or with many parameters",
    "text": "Using expand to run a pipeline on many samples or with many parameters\nLet’s say that once we’ve produced aligned and sorted BAMs, we want to then run a simple variant calling program on each sample’s alignments.\nTo speed up execution of the variant calling pipeline, it might help to parallelize our pipeline to run on every chromosome separately.\nTo do this, we’ll again make use of expand.\nsamples = [\"A\", \"B\", \"C\"]\n\nchromosomes = list(range(1, 20))\nchromosomes = list(map(str, chromosomes))\nchromosomes.extend(['X', 'Y'])\n\nrule all:\n  input:\n    expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n\nrule download_reference:\n  input: \n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input: \n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"{sample}_1.fastq.gz\",\n    fq2 = \"{sample}_2.fastq.gz\"\n  output:\n    \"{sample}.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nrule call_variants:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    bam = \"{sample}.sorted.bam\"\n  output:\n    \"{sample}.{chrom}.vcf\"\n  shell:\n    \"\"\"\n    freebayes -f {input.ref} -r {wildcards.chrom} {input.bam} &gt; {output}\n    \"\"\"\n\nWe’ve now added a step to the pipeline which takes a reference genome and a BAM as input, and outputs a variant call file (VCF).\n\n\nNotice that in the call_variants rule, my freebayes command takes a -r argument that specifies the region we want to analyze. Whenever you want to access a wildcard inside of the shell: portion of a rule, its necessary to preface the wildcard name with wildcard.\n\nAnd in this example, expand takes the Cartesian product (i.e., itertools.product()) of the lists of parameters. So, the output of expand would be:\n&gt;&gt;&gt; expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n\n[\"A.chr1.vcf\",\n \"A.chr2.vcf\",\n \"A.chr3.vcf\",\n ...\n \"C.chrY.vcf\"]\nThis way, we can enumerate every possible combination of input parameters instead of typing out 3 * 21 separate commands.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#including-pure-python-in-a-rule",
    "href": "notes/snakemake.html#including-pure-python-in-a-rule",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Including pure python in a rule",
    "text": "Including pure python in a rule\nSo far, our rules have only invoked shell commands like bwa or wget. But Snakemake also allows you to make a rule that is just a block of python code. For example, the following rule would be totally acceptable.\nrule count_snps:\n  input:\n    expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n  output:\n    \"per_sample.snp_counts.tsv\"\n  run:\n    \"\"\"\n    from cyvcf2 import VCF\n\n    vcf_file = VCF({input})\n\n    output_fh = open({output}, \"w\")\n    \n    # loop over input files\n    for vcf_fh in {input}:\n\n      sample_name = vcf_fh.split('.')[0]\n      chrom = vcf_fh.split('.')[1]\n\n      # initialize a VCF object using each\n      # file handle in the input list\n      vcf = VCF(vcf_fh)\n\n      snp_count = 0\n\n      for v in vcf:\n        if v.var_type == \"snp\": snp_count += 1\n\n      print (','.join([sample_name, chrom, str(snp_count)]), file=output_fh)\n\n    \"\"\"\n\nNotice that when a rule includes python code, we use the run: syntax instead of the shell: syntax at the top of the code block.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#running-a-snakemake-pipeline-on-the-sage-grid-engine-sge",
    "href": "notes/snakemake.html#running-a-snakemake-pipeline-on-the-sage-grid-engine-sge",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Running a Snakemake pipeline on the Sage Grid Engine (SGE)",
    "text": "Running a Snakemake pipeline on the Sage Grid Engine (SGE)\nIn our lab, we use the SGE to submit and manage compute jobs on the cluster. Snakemake is actually compatible with SGE (and SLURM, etc.), which makes it super easy to submit jobs to SGE directly from a Snakemake pipeline.\nAs an example, we could execute our pipeline as follows:\nsnakemake -j 10 \\\n      --cluster \\\n      --rerun-incomplete \\\n       \"qsub -l centos=7 -l mfree=16G -l h_rt=12:0:0 -o /path/to/outdir -e /path/to/errdir\"\n\nThe -j flag specifies the maximum number of jobs Snakemake is allowed to submit to SGE at a time.\n\n\nThe --rerun-incomplete flag is awesome. If we run our pipeline and it fails for some reason, using --rerun-incomplete will tell Snakemake to re-run a rule if the output of that rule is incomplete (i.e., if the rule didn’t finish due to a job failure in the last pipeline execution).\n\n\nAfter specifying --cluster, we just put the normal qsub command we’d normally use to submit a .sh or .sge script to the cluster, specifying the memory required by each job, wall time, etc.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#using-a-config-file-to-flexibly-change-grid-requirements-for-particular-rules",
    "href": "notes/snakemake.html#using-a-config-file-to-flexibly-change-grid-requirements-for-particular-rules",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using a config file to flexibly change grid requirements for particular rules",
    "text": "Using a config file to flexibly change grid requirements for particular rules\nOne natural issue with the above command is that some rules might require different cluster specifications than others.\nFor example, I’ve written my bwa_align rule such that bwa mem will use 4 threads during alignment, and it’ll probably use much more memory than downloading a reference genome.\nThankfully, Snakemake lets us create individual cluster specifications for each of our rules using a config file. See the example config file below, which is written in JSON.\n\"__default__\":\n\n{ \"memory\": \"8G\",\n  \"time\": \"1:0:0\",\n  \"threads\": \"1\",\n  \"os\": \"7\" },\n\n\"bwa_align\":\n\n{ \"memory\": \"4G\",\n  \"time\": \"8:0:0\",\n  \"threads\": \"4\",\n  \"os\": \"7\" }\nThen, when we run Snakemake, we could do the following:\nsnakemake -j 10 \\\n      --cluster-config /path/to/config.json \\\n      --cluster \\\n      --rerun-incomplete \\\n      \"qsub -l centos={cluster.os} -l mfree={cluster.memory} -l h_rt={cluster.time} -pe serial {cluster.threads}\"",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#visualizing-the-full-pipeline-with-a-dag",
    "href": "notes/snakemake.html#visualizing-the-full-pipeline-with-a-dag",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Visualizing the full pipeline with a DAG",
    "text": "Visualizing the full pipeline with a DAG\nWe can also visualize the various steps of the pipeline in a directed acyclic graph (DAG).\nAfter putting the full pipeline in a file called Snakefile, we can run the following from the directory in which the Snakefile resides:\nsnakemake --dag | dot -Tsvg &gt; dag.svg\nThis will produce an image showing us exactly what steps Snakemake will during execution. This plot ignores the VCF calling steps, since the DAG gets pretty unweildy with that many steps!",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/tiny_transformers.html",
    "href": "notes/tiny_transformers.html",
    "title": "Tiny transformers for population genetic inference",
    "section": "",
    "text": "Take a look at the two images below. These images are of shape \\((H = 64, W = 36)\\) — each row represents a human haplotype, and each column represents a single-nucleotide polymorphism (SNP). Each “pixel” location \\(H_i, W_j\\) can take a value of \\(0\\) (meaning haplotype \\(i\\) possesses the ancestral allele at site \\(j\\)) or \\(1\\) (meaning it possesses the derived allele).\n\nBoth images were simulated using a backwards-in-time simulation engine called msprime, and each image represents the genetic variation present in collection of \\(H = 64\\) haplotypes sampled at \\(W = 36\\) consecutive SNPs. The genomes in the first image were simulated from a demographic model in the stdpopsim catalog1, assuming a recombination rate \\(\\rho = 10^{-9}\\). The genomes in the second image were simulated from the same demographic model, but assuming \\(\\rho = 10^{-8}\\).\n\n\n\nBy eye, we might be able to tell a difference between the two images, but what if we wanted to classify tens or hundreds of thousands of them? This example is totally contrived, but it represents an important kind of inference challenge in population genetics: distinguishing between the patterns of genetic variation observed in different regions of the genome. For example, we might want to discriminate between regions of the genome undergoing positive or negative selection and those evolving neutrally or near-neutrally. Although numerous statistical methods exist for doing this kind of inference, machine learning methods may be particularly well-suited to the task.\nMachine learning approaches — in particular, convolutional neural networks (CNNs) — have proven “unreasonably effective”2 for population genetic inference. CNNs can be trained to detect population genetic phenomena (e.g., selection3, adaptive admixture4, incomplete lineage sorting5) and predict various summary statistics (e.g., recombination rate) by ingesting lots of labeled training data. These training data often comprise 1- or 2-channel “images” of haplotypes in genomic windows of a defined SNP length (see Figure 1), much like the ones shown above.\n\n\n\n\n\n\nFigure 1: Learning from images of haplotypes. We can represent genomic regions as one- or two-channel “images” in which rows correspond to haplotypes and columns to SNPs. The first channel typically encodes derived alleles as \\(1\\)s and ancestral alleles as \\(0\\)s, while the second channel can encode either absolute genomic positions or inter-SNP distances. In the absence of phased haplotype data, the first channel can also represent diploid genotypes as \\(0\\) (homozygous for the ancestral allele), \\(0.5\\) (heterozygous), and \\(1\\) (homozygous for the derived allele).\n\n\n\n\n\n\n\n\nUnlike images of cats, dogs, or handwritten digits, these images of human genetic variation are “row-invariant.” In other words, the information in the image is exactly the same regardless of how you permute the haplotypes.  If you were to shuffle the rows of an image of a cat, on the other hand, that image would look dramatically different, and the spatial correlations between pixels would be totally broken.The order of haplotypes in an image might matter if the haplotypes are derived from two or more highly structured populations, or if we’ve introduced structure into the image by e.g., sorting haplotypes by genetic similarity. In general, though, we’d like to develop machine learning methods that are agnostic to both haplotype order and number.\nTo enable permutation-invariant population genetic inference on images of human haplotypes, Chan et al. developed the defiNETti architecture (details in Note 1).\n\n\n\n\n\n\nNote 1: The defiNETti architecture: a powerful, yet conceptually simple, machine learning approach designed for images of genetic variation\n\n\n\n\n\n\n\n\nFirst, a series of strided 1D convolutions are applied to an input image of genetic variation (optionally followed by activation functions and pooling) to create high-dimensional feature maps and reduce the width of the input image. These 1D convolutions can presumably capture “interesting” features, like linkage disequilibrium between nearby SNPs, as well as build higher-level combinations of those features. After the convolutional layers we’re left with a batch of tensors of shape \\((B, D, H, W')\\), where \\(B\\) is the batch size, \\(D\\) is the number of feature maps produced by the convolutional operations, \\(H\\) is the original height of the image (i.e., the number of haplotypes) and \\(W'\\) is the width of the image after strided convolutions and pooling operations are taken into account. To make the model agnostic to both the order and number of haplotypes in the input image, a “permutation-invariant” aggregation operation is then applied along the \\(H\\) dimension (typically, max or mean). Then, the resulting tensor of shape \\((B, D, 1, W)\\) can be flattened and passed through a series of fully-connected layers and a final classification head.\n\n\nFrom Chan et al. (2019) Advances in NeurIPS.\n\n\n\nChan et al. demonstrate that their approach is highly effective for a number of population genetics inference tasks, and outperforms established statistical methods for e.g., the detection of recombination hotspots. In addition to being permutation-invariant, the aggregation operation applied before its fully-connected layers means that defiNETti is also (mostly) agnostic to sample size. Even if defiNETti is trained on images of height \\(H = 512\\), it is insensitive to the sizes of images at test time, working quite well even when images contain as few as 64 haplotypes. In the population genetics setting, permutation-invariance and sample size indifference are extremely attractive properties of a machine learning method. For example, we might want to train a machine learning model using data from one cohort (say, a subpopulation of the Thousand Genomes consortium), and test that model on haplotypes sampled from another cohort with fewer or more samples.\n\n\n\nAlthough the defiNETti approach demonstrated that permutation-invariant CNNs are powerful, lightweight tools for population genetic inference, other architectures may be even better. For example, the transformer architecture, and the self-attention mechanism in particular, is a natural choice for embedding images of human genetic variation. To my knowledge, though, transformers are relatively unexplored in the population genetics space.\nIn the parlance of the vision transformer (ViT)6, we can treat input images of genetic variation as collections of “haplotype patches.” Each patch \\(P\\) is shape \\((1, W)\\), where \\(W\\) is the number of SNPs and \\(P \\in \\{0, 1\\}\\). We first embed each patch in a new, \\(d\\)-dimensional feature space using a single fully-connected layer (Figure 2).\n\n\n\n\n\n\nFigure 2: Tokenizing haplotype images. Given an image of genetic variation, we embed every haplotype “patch” into a new \\(d\\)-dimensional embedding space using a single fully-connected layer (essentially, just a linear transformation of the \\(0\\)s and \\(1\\)s each haplotype possesses at the \\(W\\) SNPs.)\n\n\n\nI won’t spend much time on the details here, but the basic conceit of self-attention is that the embedding of each patch is compared to the embedding of every other patch, and the weights associated with these pairwise comparisons can be tuned to capture the important relationships between patches.  The transformer outputs an updated set of haplotype patch embeddings that should reflect these inter-haplotype relationships (Figure 3).For a great overview of the transformer architecture, including the building blocks of self-attention, check out the Illustrated Transformer.\n\n\n\n\n\n\nFigure 3: Applying transformers to haplotype images. For a given image, we pass the haplotype patch embeddings from Figure 2 to a transformer with self-attention. A transformer module (details in Note 2) outputs a new, “updated” set of haplotype patch embeddings. We can create a final image embedding by simply aggregating across haplotype patches at each dimension \\(d_i\\) of the embedding space, then add a fully-connected layer on top of that final image embedding for classification purposes. Alternatively, we could include a special [CLS] classification token when training the transformer and use its output embedding for classification tasks.\n\n\n\n\n\n\n\n\n\nNote 2: The transformer module in more detail\n\n\n\n\n\n\n\n\nEach transformer “module” comprises multiple layers of normalization, residual connections and multi-layer-perceptrons (MLPs). First, the input embeddings are normalized using LayerNorm. Next, the normalized embeddings are fed into a multi-headed self-attention block. A residual skip connection adds the output of that self-attention block to the original input embeddings. Those embeddings are normalized again, and then each haplotype (token) embedding is passed through a fully-connected feed-forward (FCFF) network with one hidden layer (in these experiments, the FCFF network has a hidden layer of size \\(2d\\), where \\(d\\) is the dimensionality of the input embeddings). A final residual connection sums the output of the FCFF network and the output of the first skip connection to produce the final haplotype embeddings.\n\n\n\n\n\nThe self-attention mechanism is inherently invariant to the order of the patch embeddings, though we could introduce learned positional/rotary embeddings into our model if their order does matter. It can also be applied to images with any number of haplotypes (assuming our CPU/GPU hardware can handle the sequence length).\nSo, how well do transformers work for population genetic inference?",
    "crumbs": [
      "Tiny transformers for population genetic inference"
    ]
  },
  {
    "objectID": "notes/tiny_transformers.html#introduction",
    "href": "notes/tiny_transformers.html#introduction",
    "title": "Tiny transformers for population genetic inference",
    "section": "",
    "text": "Take a look at the two images below. These images are of shape \\((H = 64, W = 36)\\) — each row represents a human haplotype, and each column represents a single-nucleotide polymorphism (SNP). Each “pixel” location \\(H_i, W_j\\) can take a value of \\(0\\) (meaning haplotype \\(i\\) possesses the ancestral allele at site \\(j\\)) or \\(1\\) (meaning it possesses the derived allele).\n\nBoth images were simulated using a backwards-in-time simulation engine called msprime, and each image represents the genetic variation present in collection of \\(H = 64\\) haplotypes sampled at \\(W = 36\\) consecutive SNPs. The genomes in the first image were simulated from a demographic model in the stdpopsim catalog1, assuming a recombination rate \\(\\rho = 10^{-9}\\). The genomes in the second image were simulated from the same demographic model, but assuming \\(\\rho = 10^{-8}\\).\n\n\n\nBy eye, we might be able to tell a difference between the two images, but what if we wanted to classify tens or hundreds of thousands of them? This example is totally contrived, but it represents an important kind of inference challenge in population genetics: distinguishing between the patterns of genetic variation observed in different regions of the genome. For example, we might want to discriminate between regions of the genome undergoing positive or negative selection and those evolving neutrally or near-neutrally. Although numerous statistical methods exist for doing this kind of inference, machine learning methods may be particularly well-suited to the task.\nMachine learning approaches — in particular, convolutional neural networks (CNNs) — have proven “unreasonably effective”2 for population genetic inference. CNNs can be trained to detect population genetic phenomena (e.g., selection3, adaptive admixture4, incomplete lineage sorting5) and predict various summary statistics (e.g., recombination rate) by ingesting lots of labeled training data. These training data often comprise 1- or 2-channel “images” of haplotypes in genomic windows of a defined SNP length (see Figure 1), much like the ones shown above.\n\n\n\n\n\n\nFigure 1: Learning from images of haplotypes. We can represent genomic regions as one- or two-channel “images” in which rows correspond to haplotypes and columns to SNPs. The first channel typically encodes derived alleles as \\(1\\)s and ancestral alleles as \\(0\\)s, while the second channel can encode either absolute genomic positions or inter-SNP distances. In the absence of phased haplotype data, the first channel can also represent diploid genotypes as \\(0\\) (homozygous for the ancestral allele), \\(0.5\\) (heterozygous), and \\(1\\) (homozygous for the derived allele).\n\n\n\n\n\n\n\n\nUnlike images of cats, dogs, or handwritten digits, these images of human genetic variation are “row-invariant.” In other words, the information in the image is exactly the same regardless of how you permute the haplotypes.  If you were to shuffle the rows of an image of a cat, on the other hand, that image would look dramatically different, and the spatial correlations between pixels would be totally broken.The order of haplotypes in an image might matter if the haplotypes are derived from two or more highly structured populations, or if we’ve introduced structure into the image by e.g., sorting haplotypes by genetic similarity. In general, though, we’d like to develop machine learning methods that are agnostic to both haplotype order and number.\nTo enable permutation-invariant population genetic inference on images of human haplotypes, Chan et al. developed the defiNETti architecture (details in Note 1).\n\n\n\n\n\n\nNote 1: The defiNETti architecture: a powerful, yet conceptually simple, machine learning approach designed for images of genetic variation\n\n\n\n\n\n\n\n\nFirst, a series of strided 1D convolutions are applied to an input image of genetic variation (optionally followed by activation functions and pooling) to create high-dimensional feature maps and reduce the width of the input image. These 1D convolutions can presumably capture “interesting” features, like linkage disequilibrium between nearby SNPs, as well as build higher-level combinations of those features. After the convolutional layers we’re left with a batch of tensors of shape \\((B, D, H, W')\\), where \\(B\\) is the batch size, \\(D\\) is the number of feature maps produced by the convolutional operations, \\(H\\) is the original height of the image (i.e., the number of haplotypes) and \\(W'\\) is the width of the image after strided convolutions and pooling operations are taken into account. To make the model agnostic to both the order and number of haplotypes in the input image, a “permutation-invariant” aggregation operation is then applied along the \\(H\\) dimension (typically, max or mean). Then, the resulting tensor of shape \\((B, D, 1, W)\\) can be flattened and passed through a series of fully-connected layers and a final classification head.\n\n\nFrom Chan et al. (2019) Advances in NeurIPS.\n\n\n\nChan et al. demonstrate that their approach is highly effective for a number of population genetics inference tasks, and outperforms established statistical methods for e.g., the detection of recombination hotspots. In addition to being permutation-invariant, the aggregation operation applied before its fully-connected layers means that defiNETti is also (mostly) agnostic to sample size. Even if defiNETti is trained on images of height \\(H = 512\\), it is insensitive to the sizes of images at test time, working quite well even when images contain as few as 64 haplotypes. In the population genetics setting, permutation-invariance and sample size indifference are extremely attractive properties of a machine learning method. For example, we might want to train a machine learning model using data from one cohort (say, a subpopulation of the Thousand Genomes consortium), and test that model on haplotypes sampled from another cohort with fewer or more samples.\n\n\n\nAlthough the defiNETti approach demonstrated that permutation-invariant CNNs are powerful, lightweight tools for population genetic inference, other architectures may be even better. For example, the transformer architecture, and the self-attention mechanism in particular, is a natural choice for embedding images of human genetic variation. To my knowledge, though, transformers are relatively unexplored in the population genetics space.\nIn the parlance of the vision transformer (ViT)6, we can treat input images of genetic variation as collections of “haplotype patches.” Each patch \\(P\\) is shape \\((1, W)\\), where \\(W\\) is the number of SNPs and \\(P \\in \\{0, 1\\}\\). We first embed each patch in a new, \\(d\\)-dimensional feature space using a single fully-connected layer (Figure 2).\n\n\n\n\n\n\nFigure 2: Tokenizing haplotype images. Given an image of genetic variation, we embed every haplotype “patch” into a new \\(d\\)-dimensional embedding space using a single fully-connected layer (essentially, just a linear transformation of the \\(0\\)s and \\(1\\)s each haplotype possesses at the \\(W\\) SNPs.)\n\n\n\nI won’t spend much time on the details here, but the basic conceit of self-attention is that the embedding of each patch is compared to the embedding of every other patch, and the weights associated with these pairwise comparisons can be tuned to capture the important relationships between patches.  The transformer outputs an updated set of haplotype patch embeddings that should reflect these inter-haplotype relationships (Figure 3).For a great overview of the transformer architecture, including the building blocks of self-attention, check out the Illustrated Transformer.\n\n\n\n\n\n\nFigure 3: Applying transformers to haplotype images. For a given image, we pass the haplotype patch embeddings from Figure 2 to a transformer with self-attention. A transformer module (details in Note 2) outputs a new, “updated” set of haplotype patch embeddings. We can create a final image embedding by simply aggregating across haplotype patches at each dimension \\(d_i\\) of the embedding space, then add a fully-connected layer on top of that final image embedding for classification purposes. Alternatively, we could include a special [CLS] classification token when training the transformer and use its output embedding for classification tasks.\n\n\n\n\n\n\n\n\n\nNote 2: The transformer module in more detail\n\n\n\n\n\n\n\n\nEach transformer “module” comprises multiple layers of normalization, residual connections and multi-layer-perceptrons (MLPs). First, the input embeddings are normalized using LayerNorm. Next, the normalized embeddings are fed into a multi-headed self-attention block. A residual skip connection adds the output of that self-attention block to the original input embeddings. Those embeddings are normalized again, and then each haplotype (token) embedding is passed through a fully-connected feed-forward (FCFF) network with one hidden layer (in these experiments, the FCFF network has a hidden layer of size \\(2d\\), where \\(d\\) is the dimensionality of the input embeddings). A final residual connection sums the output of the FCFF network and the output of the first skip connection to produce the final haplotype embeddings.\n\n\n\n\n\nThe self-attention mechanism is inherently invariant to the order of the patch embeddings, though we could introduce learned positional/rotary embeddings into our model if their order does matter. It can also be applied to images with any number of haplotypes (assuming our CPU/GPU hardware can handle the sequence length).\nSo, how well do transformers work for population genetic inference?",
    "crumbs": [
      "Tiny transformers for population genetic inference"
    ]
  },
  {
    "objectID": "notes/tiny_transformers.html#materials-and-methods",
    "href": "notes/tiny_transformers.html#materials-and-methods",
    "title": "Tiny transformers for population genetic inference",
    "section": "Materials and Methods",
    "text": "Materials and Methods\nLet’s compare a simple transformer model to a simple implementation of the defiNETti architecture. I’ve included pytorch code for replicating the two models below.\n\nArchitectural details\nCNN model architectures were adapted from Chan et al. (defiNETti) and Wang et al. (2021). Convolutional operations used a kernel of shape (1, 5), stride of (1, 1), and no zero-padding. All convolutional operations were followed by ReLU activations. In the Wang et al. architecture, convolutional operations were further followed by MaxPool with a stride and kernel of (1, 2). The first convolutional layer always outputs a feature map with 32 channels, and feature maps increased in dimension by a factor of 2 with each subsequent convolutional layer. I used max as the permutation-invariant function (to collapse along the height dimension) in all cases; empirically, I found that max outperformed mean as an aggregation function. After applying the permutation-invariant function, feature maps were flattened and passed to two fully-connected layers of 128 dimensions each, with ReLU activations after each.\nOur transformer model architecture follows the same general structure as in the Vision Transformer (ViT) paper — see Equations 1-4 in the ViT preprint (or the code samples below) for details. Briefly, we embedded haplotype patches using a single fully-connected layer with 128 dimensions and passed the resulting tensor of patch embeddings to a single multi-headed self-attention block with 8 heads, LayerNorm, a multi-layer perceptron (MLP) and residual connections.\n\nCNN model architecture\n\n\n\n\n\n\n\n\n\n\n\n\nmodel type\nkernel size\nstride\nmax-pool\nconvolutional layers\nfully-connected dimension\ntrainable params (hotspot task)\ntrainable params (rate task)\n\n\n\n\nCNN (defiNETti)\n5\n1\nFalse\n2\n128\n256,770\n256,899\n\n\nCNN (Wang et al.)\n5\n1\nTrue\n2\n128\n76,546\n76,675\n\n\n\n\nTransformer architecture\n\n\n\n\n\n\n\n\n\n\n\nmodel type\ndepth\nhidden size\nMLP size\nheads\ntrainable params (hotspot task)\ntrainable params (rate task)\n\n\n\n\nTransformer\n1\n128\n256\n8\n137,603\n137,732\n\n\n\nAll models were trained with the Adam optimizer. As in Chan et al., we used the following learning rate schedule: \\(10^{-3} \\times 0.9^{\\frac{m}{I}}\\), where \\(m\\) is the current minibatch and \\(I\\) is the total number of training iterations.\n\n\n\n\n\n\n“Tokenizer” for embedding haplotype patches\n\n\n\n\n\n\nfrom torch import nn\n\nclass HaplotypeTokenizer(nn.Module):\n\n    def __init__(\n        self,\n        input_channels: int = 1,\n        input_width: int = 36,\n        hidden_size: int = 128,\n    ):\n        super().__init__()\n\n        self.input_dim = input_channels * input_width\n        self.hidden_size = hidden_size\n        # simple linear projection\n        self.proj = torch.nn.Linear(self.input_dim, hidden_size)\n\n    def forward(self, x):\n        # shape should be (B, C, H, W) where H is the number\n        # of haplotypes and W is the number of SNPs\n        B, C, H, W = x.shape\n        # permute to (B, H, C, W)\n        x = x.permute(0, 2, 1, 3)\n        # then, flatten each \"patch\" of C * W such that\n        # each patch is 1D and size (C * W).\n        x = x.reshape(B, H, -1)\n        # embed \"patches\" of size (C * W, effectively a 1d\n        # array equivalent to the number of SNPs)\n        tokens = self.proj(x)\n        return tokens\n\n\n\n\n\n\n\n\n\n\nA simple transformer model with self-attention\n\n\n\n\n\n\nclass Transformer(nn.Module):\n\n    def __init__(\n        self,\n        embed_dim: int = 128,\n        num_heads: int = 1,\n        mlp_hidden_dim_ratio: int = 2,\n    ):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim,\n            num_heads,\n            batch_first=True,\n        )\n        self.norm = nn.LayerNorm(embed_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * mlp_hidden_dim_ratio),\n            nn.GELU(),\n            nn.Linear(embed_dim * mlp_hidden_dim_ratio, embed_dim),\n        )\n\n    def forward(self, x):\n        # layernorm initial embeddings\n        x_norm = self.norm(x)\n        # self-attention on normalized embeddings\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)  \n        # residual connection + layernorm\n        x = self.norm(x + attn_out)\n        # mlp on each haplotype token\n        x_mlp = self.mlp(x)\n        # final residual connection + layernorm\n        return self.norm(x + x_mlp)\n\n\nclass TinyTransformer(torch.nn.Module):\n    def __init__(\n        self,\n        width: int = 36,\n        in_channels: int = 1,\n        num_heads: int = 1,\n        hidden_size: int = 128,\n        num_classes: int = 2,\n        depth: int = 1,\n        mlp_ratio: int = 2,\n        agg: str = \"max\",\n    ):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.norm = nn.LayerNorm(hidden_size)\n        self.agg = agg\n\n        # we use a \"custom\" tokenizer that takes\n        # patches of size (C * W), where W is the\n        # number of SNPs\n        \n        self.tokenizer = ColumnTokenizer(\n            input_channels=in_channels,\n            input_width=width,\n            hidden_size=hidden_size,\n        )\n\n        self.attention = nn.Sequential(\n            *[\n                Transformer(\n                    embed_dim=hidden_size,\n                    num_heads=num_heads,\n                    mlp_hidden_dim_ratio=mlp_ratio,\n                )\n                for _ in range(depth)\n            ]\n        )\n\n        # linear classifier head\n        self.classifier = torch.nn.Linear(\n            hidden_size,\n            num_classes,\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.tokenizer(x)  # (B, H, hidden_size)\n        # pass through transformer encoder\n        x = self.attention(x)\n        if self.agg == \"max\":\n            cls_output = torch.amax(x, dim=1)\n        elif self.agg == \"mean\":\n            cls_output = torch.mean(x, dim=1)\n        logits = self.classifier(cls_output)\n\n        return logits\n\n\n\n\n\n\n\n\n\n\nThe defiNETti architecture\n\n\n\n\n\n\nclass DeFinetti(nn.Module):\n\n    def __init__(\n        self,\n        *,\n        in_channels: int,\n        kernel: Union[int, Tuple[int]] = (1, 5),\n        hidden_dims: List[int] = [32, 64],\n        agg: str = \"max\",\n        width: int = 36,\n        num_classes: int = 2,\n        fc_dim: int = 128,\n        padding: int = 0,\n        stride: int = 1,\n        pool: bool = False,\n    ) -&gt; None:\n\n        super(DeFinetti, self).__init__()\n\n        self.agg = agg\n        self.width = width\n\n        _stride = (1, stride)\n        _padding = (0, padding)\n\n        out_W = width\n\n        conv = []\n        for h_dim in hidden_dims:\n            # initialize convolutional block\n            block = [\n                nn.Conv2d(\n                    in_channels,\n                    h_dim,\n                    kernel_size=kernel,\n                    stride=_stride,\n                    padding=_padding,\n                ),\n                nn.ReLU(),\n            ]\n            if pool:\n                block.append(\n                    nn.MaxPool2d(\n                        kernel_size=(1, 2),\n                        stride=(1, 2),\n                    ),\n                )\n            \n            out_W = (\n                math.floor((out_W - kernel[1] + (2 * (padding))) / _stride[1]) + 1\n            )\n\n            # account for max pooling\n            if pool:\n                out_W //= 2\n            in_channels = h_dim\n            conv.extend(block)\n\n        self.conv = nn.Sequential(*conv)\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dims[-1] * out_W, fc_dim),\n            nn.ReLU(),\n            nn.Linear(fc_dim, fc_dim),\n            nn.ReLU(),\n        )\n        # two projection layers with batchnorm\n        self.project = nn.Sequential(\n            nn.Linear(fc_dim, num_classes),\n        )\n        self.flatten = nn.Flatten()\n\n    def forward(self, x):\n        x = self.conv(x)\n        # take the average across haplotypes\n        if self.agg == \"mean\":\n            x = torch.mean(x, dim=2)\n        elif self.agg == \"max\":\n            x = torch.amax(x, dim=2)\n        elif self.agg is None:\n            pass\n        # flatten, but ignore batch\n        x = self.flatten(x)\n        encoded = self.fc(x)\n        projection = self.project(encoded)\n\n        return projection\n\n\n\n\n\n\nTraining datasets and classification tasks\nI compared the performance of CNNs and transformers on three simple classification tasks.\n\nRecombination hotspot detection\nThe first task was adapted from Chan et al. Training data belonged to one of two classes: regions that either contained or did not contain a recombination hotspot. All regions were simulated using a simple msprime demographic model and an msprime recombination RateMap object as follows:\n\n\n\n\n\n\nSimulating recombination hotspots\n\n\n\n\n\n\nimport msprime\nimport numpy as np\n\nrng = np.random.default_rng(42)\n\n# interval length\nL = 25_000\n# hotspot length\nHOTSPOT_L = 2_000\n\n# define background recomb rate\nbackground = rng.uniform(1e-8, 1.5e-8)\n\nclass_label = rng.choice(2)\nheat = 1 if class_label == 0 else rng.uniform(10, 100)\n\n# starts and ends of the hotspot given length of simulated inderval\nhs_s, hs_e = (\n    (L - HOTSPOT_L) / 2,\n    (L + HOTSPOT_L) / 2,\n)\nrho_map = msprime.RateMap(\n    position=[\n        0,\n        hs_s,\n        hs_e,\n        L,\n    ],\n    rate=[background, background * heat, background],\n)\n\nseed = rng.integers(0, 2**32)\nts = msprime.simulate(\n    sample_size=200,\n    Ne=1e4,\n    recombination_map=rho_map,\n    mutation_rate=1.1e-8,\n    random_seed=seed,\n)\n\n\n\n\nIn all rate maps, \\(\\rho_{bg} \\sim \\mathcal{U}\\{1 \\times 10^{-8}, 1.5 \\times 10^{-8}\\}\\).\n\nRegions with recombination hotspots\n\n\n\n\n\n\n\n\n\n\nleft\nright\nmid\nspan\nrate\n\n\n\n\n\n0\n11,500\n5,750\n11,500\n\\(\\rho_{bg}\\)\n\n\n\n11,500\n13,500\n12,500\n2,000\n\\(\\rho_{bg} \\times \\mathcal{U}\\{10, 100\\}\\)\n\n\n\n13,500\n25,000\n19,250\n11,500\n\\(\\rho_{bg}\\)\n\n\n\n\n\nRegions without recombination hotspots\n\n\nleft\nright\nmid\nspan\nrate\n\n\n\n\n\n0\n25,000\n12,500\n25,000\n\\(\\rho_{bg}\\)\n\n\n\n\n\n\nRecombination rate classification\nIn the second task, training data belonged to one of three classes: regions with one of three different recombination rates (\\(10^{-7}\\), \\(10^{-8}\\), or \\(10^{-9}\\)). All regions were simulated using a CEU population from the OutOfAfrica_3G09 model7 in the stdpopsim catalog.\n\n\n\n\n\n\nSimulating variable recombination rates\n\n\n\n\n\n\nimport stdpopsim\n\n# choose species and model\nspecies = stdpopsim.get_species(\"HomSap\")\ndemography = species.get_demographic_model(\"OutOfAfrica_3G09\")\nRHO = [1e-9, 1e-8, 1e-7]\n\ncontigs = [\n    species.get_contig(\n        length=50_000,\n        recombination_rate=r,\n        mutation_rate=2.35e-8,\n    )\n    for r in RHO\n]\n\n# msprime by default\nengine = stdpopsim.get_default_engine()\n\nsamples_per_population = [0, 0, 0]\n# use a CEU population\nsamples_per_population[1] = n_smps\nsamples = dict(\n    zip(\n        [\"YRI\", \"CEU\", \"CHB\"],\n        samples_per_population,\n    )\n)\n\nclass_label = rng.choice(3)\n\nts = engine.simulate(\n    demography,\n    contig=contigs[class_label],\n    samples=samples,\n)\n\n\n\n\n\n\nClassifying different populations with a shared demographic history\nThe final task involved classifying images derived from three populations (CEU, CHB, and YRI) that were simulated from the OutOfAfrica_3G09 model in stdpopsim.\n\n\n\n\n\n\nSimulating three populations from a shared demographic history\n\n\n\n\n\n\nimport stdpopsim\n\n# choose species and model\nspecies = stdpopsim.get_species(\"HomSap\")\ndemography = species.get_demographic_model(\"OutOfAfrica_3G09\")\n\ncontig = species.get_contig(\n    length=50_000,\n    recombination_rate=1e-8,\n    mutation_rate=2.35e-8,\n)\n\n# msprime by default\nengine = stdpopsim.get_default_engine()\n\n# pick a random class label\nclass_label = rng.choice(3)\n\nsamples_per_population = [0, 0, 0]\n# simulate all samples from that class label\nsamples_per_population[class_label] = n_smps\nsamples = dict(\n    zip(\n        [\"YRI\", \"CEU\", \"CHB\"],\n        samples_per_population,\n    )\n)\n\nts = engine.simulate(\n    demography,\n    contig=contig,\n    samples=samples,\n)\n\n\n\n\n\n\nSimulation “on-the-fly”\nRather than train these models with a fixed dataset of \\(N\\) images, I simulated training examples “on the fly” as described in Chan et al. In each training iteration, I simulated a fresh minibatch of 256 haplotype images of shape \\((1, 200, 36)\\) — with approximately equal contribution from each class — passed the minibatch through the model, and updated the model weights using cross-entropy loss. As the models never saw the same image twice during training, there was no need to run them on a “held out” validation or test set. Models were trained for 1,000 iterations (one minibatch per iteration); therefore, each model saw a total of 25,600 unique images during training. I used random seeds to ensure that each model saw the same 25,600 images during training.",
    "crumbs": [
      "Tiny transformers for population genetic inference"
    ]
  },
  {
    "objectID": "notes/tiny_transformers.html#results",
    "href": "notes/tiny_transformers.html#results",
    "title": "Tiny transformers for population genetic inference",
    "section": "Results",
    "text": "Results\n\nTransformer models outperform CNNs for recombination rate classification\nOverall, the transformer exhibits higher accuracy than the CNNs on the recombination rate classification task (Figure 4).\n\n\n\n\n\n\n\n        \n        \n        \n\n\n(a)\n\n\n\n\n\n\n                            \n                                            \n\n\n(b)\n\n\n\n\n\n\n                            \n                                            \n\n\n(c)\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\nTransformer models slightly outperform CNNs for recombination hotspot detection\nTransformer and CNN performance is more similar on the recombination hotspot detection task (Figure 5), though the former still exhibits better accuracy.\n\n\n\n\n\n\n\n                            \n                                            \n\n\n(a)\n\n\n\n\n\n\n                            \n                                            \n\n\n(b)\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\nTransformer models also outperform CNNs for population classification\nOverall, the transformer exhibits higher accuracy than the CNNs on the three-population classification task, as well (Figure 6).\n\n\n\n\n\n\n\n                            \n                                            \n\n\n(a)\n\n\n\n\n\n\n                            \n                                            \n\n\n(b)\n\n\n\n\n\nFigure 6",
    "crumbs": [
      "Tiny transformers for population genetic inference"
    ]
  },
  {
    "objectID": "notes/tiny_transformers.html#future-work",
    "href": "notes/tiny_transformers.html#future-work",
    "title": "Tiny transformers for population genetic inference",
    "section": "Future work",
    "text": "Future work\nThis was a very simple experiment, and involved very little hyper-parameter or model tuning. At the very least, it suggests that simple transformers with self-attention are useful architectures for population genetics inference. There are many ways to investigate their utility further.\n\ntest models on more complex and realistic classification tasks\ntweak both CNNs and transformers to find optimal hyper-parameters\n\nperhaps we could engineer CNNs to have comparable performance?\n\ncompare performance on phased vs. unphased data\nfine-tune pre-trained transformer models (e.g., from huggingface) instead of training from scratch\n\nwe’ll need to modify these pre-trained models to ignore positional embeddings to ensure permutation-invariance\n\nexamine the “representations” learned by each model to see if the transformer representation space is generally more or less useful for diverse classification tasks\nensure that both models are robust to the number of haplotypes in images at test time\n\nrandomly downsample each batch of haplotype images at test time by taking a batch of \\((B, C, H, W)\\) and randomly subsampling \\(H' \\sim \\mathcal{U}\\{32, H\\}\\) so that the new batch is \\((B, C, H', W)\\)",
    "crumbs": [
      "Tiny transformers for population genetic inference"
    ]
  },
  {
    "objectID": "notes/tiny_transformers.html#footnotes",
    "href": "notes/tiny_transformers.html#footnotes",
    "title": "Tiny transformers for population genetic inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nprecisely, a CEU population from the OutOfAfrica_3G09 demography↩︎\nfor an overview of the utility of CNNs in popgen, see Flagel et al. 2019↩︎\nsee Xue et al. (2020)↩︎\nsee Hamid et al. (2023)↩︎\nsee Rosenzweig et al. (2022)↩︎\nsee Dosovitskiy et al. (2021) arXiv↩︎\nbased on Gutenkunst et al. (2009), PLoS Genetics↩︎",
    "crumbs": [
      "Tiny transformers for population genetic inference"
    ]
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Click 💻 to access associated code and 📖 for PDFs."
  },
  {
    "objectID": "pubs.html#preprints",
    "href": "pubs.html#preprints",
    "title": "Publications",
    "section": "Preprints",
    "text": "Preprints\nKunisaki J, Goldberg ME, Lulla S, Sasani TA, Hiatt L, Nicholas TJ, Liu L, Torres-Arce E, Guo Y, James E, Horns JJ, Ramsay JM, Chen W, Hotaling JM, Aston KI, Quinlan AR. Sperm from infertile, oligozoospermic men have elevated mutation rates (2024). medRxiv.\nMokveld T, Dolzhenko E, Dashnow H, Nicholas TJ, Sasani TA, van der Sanden B, Jadhav B, Pedersen B, Kronenberg S, Tucci A, Sharp AJ, Quinlan AR, Gilissen C, Hoischen A, Eberle MA. TRGT-denovo: accurate detection of de novo tandem repeat mutations (2024). bioRxiv.\nXu K, Zhang Y, Baldwin-Brown J, Sasani TA, Phadnis N, Miller MP, Rog O. Decoding chromosome organization using CheC-PLS: chromosome conformation by proximity labeling and long-read sequencing (2024). bioRxiv.\nAshbrook DG, Sasani TA, Maksimov M, Gunturkun MH, Ma N, Villani F, Ren Y, Rothschild D, Chen H, Lu L, Colonna V, Dumont B, Harris K, Gymrek M, Pritchard JK, Palmer AA, Williams RW. Private and sub-family specific mutations of founder haplotypes in the BXD family reveal phenotypic consequences relevant to health and disease (2022). bioRxiv."
  },
  {
    "objectID": "pubs.html#peer-reviewed-manuscripts",
    "href": "pubs.html#peer-reviewed-manuscripts",
    "title": "Publications",
    "section": "Peer-reviewed Manuscripts",
    "text": "Peer-reviewed Manuscripts\nPorubsky D, Dashnow H*, Sasani TA*, Logsdon GA*, Hallast P*, Noyes MD*, Kronenberg ZN*, Mokveld T*, et al. Human de novo mutation rates from a four-generation pedigree reference (2025). Nature.\nSasani TA, Quinlan AR, Harris K. Epistasis between mutator alleles contributes to germline mutation rate variability in laboratory mice (2024). eLife.  💻\nFixsen SM, Cone KR, Goldstein SA, Sasani TA, Quinlan AR, Rothenburg S, Elde NC. Poxviruses capture host genes by LINE-1 retrotransposition (2022). eLife.  📖\nSasani TA, Ashbrook DG, Beichman AC, Lu L, Palmer AA, Williams RW, Pritchard JK, Harris K. A natural mutator allele shapes mutation spectrum variation in mice (2022). Nature.  💻  📖\nBelyeu JR*, Sasani TA*, Pedersen BS, Quinlan AR. Unfazed: parent-of-origin detection for large and small de novo variants (2021). Bioinformatics.  💻  📖\nWallace AD, Sasani TA, Swanier J, Gates B, Greenland J, Pedersen BP, Varley KT, Quinlan AR. CaBagE: a Cas9-based Background Elimination strategy for targeted, long-read DNA sequencing (2021). PLoS One.  📖\nCawthon RM, Meeks HD, Sasani TA*, Smith KR, Kerber RA, O’Brien E, Baird L, Dixon MM, Peiffer AP, Leppert MF, Quinlan AR, Jorde LB. Germline mutation rates in young adults predict longevity and reproductive lifespan (2020). Scientific Reports.  📖\nSasani TA, Pedersen BS, Gao Z, Baird L, Przeworski M, Jorde LB, Quinlan AR. Large, three-generation human families reveal post-zygotic mosaicism and variability in germline mutation accumulation (2019). eLife.  💻  📖\nGao Z, Moorjani P, Sasani TA, Pedersen BP, Quinlan AR, Jorde LB, Amster G, Przeworski MP. Overlooked roles of DNA damage and maternal age in generating human germline mutations (2019). PNAS.   📖\nSasani TA*, Cone KR*, Quinlan AR, Elde NC. Long read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays (2018). eLife.  💻  📖\nBelyeu JR, Nicholas TJ, Pedersen BS, Sasani TA, Havrilla JM, Kravitz SN, Conway ME, Lohman BK, Quinlan AR, Layer RM. SV-plaudit: a cloud-based framework for manually curating thousands of structural variants (2018). GigaScience.  💻  📖\nJain M*, Koren S*, Miga K*, Quick J*, Rand AC*, Sasani TA*, Tyson JR*, Beggs AD, Dilthey AT, Fiddes IT, Malla S, Marriott H, Nieto T, O’Grady J, Olsen HE, Pedersen BS, Rhie A, Richardson H, Quinlan AR, Snutch TP, Tee L, Paten B, Phillippy AM, Simpson JT, Loman NJ, Loose M. Nanopore sequencing and assembly of a human genome with ultra-long reads (2018). Nature Biotechnology.  📖\nFeusier J, Witherspoon DJ, Watkins SW, Goubert C, Sasani TA, Jorde LB. Discovery of rare, diagnostic AluYb8/9 elements in diverse human populations (2017). Mobile DNA.  📖\nPiasecki BP, Sasani TA, Lessenger AT, Huth N, Farrell S. MAPK-15 is a ciliary protein required for PKD-2 localization and male mating behavior in Caenorhabditis elegans (2017).  Cytoskeleton.  📖\n* indicates equal contribution"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tom Sasani, PhD",
    "section": "",
    "text": "I’m a computational biologist using genetic data to better understand health and evolution.\n\nExperience\nI’m currently a (remote) Staff Research Scientist with Dr. Aaron Quinlan at the University of Utah, where I develop new methods to study the evolution of germline mutation rates and spectra.\nAs a Postdoctoral Fellow with Dr. Kelley Harris at the University of Washington, I discovered genetic modifiers of the germline mutation rate in mice.\nIn between my academic roles I was also a Senior Data Scientist at Recursion Pharmaceuticals, where I analyzed data from massive cellular imaging experiments to identify new treatments for human disease."
  },
  {
    "objectID": "notes/gatk.html",
    "href": "notes/gatk.html",
    "title": "A simple guide to running GATK",
    "section": "",
    "text": "There are probably hundreds of “how to use GATK” guides published online, not including the detailed documentation on the GATK website. But in my (admittedly limited) experience, running GATK can be a bit finicky, and the workflow for producing a genotyped VCF using GATK “best practices” can take a little bit of troubleshooting. Below I’ve outlined my own simple workflow for calling SNVs in human data using GATK, including the various preprocessing steps and file/software downloads. This is mostly for my own reference in the future, when the ramblings in my lab notebook eventually lose all meaning to me.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#motivation",
    "href": "notes/gatk.html#motivation",
    "title": "A simple guide to running GATK",
    "section": "",
    "text": "There are probably hundreds of “how to use GATK” guides published online, not including the detailed documentation on the GATK website. But in my (admittedly limited) experience, running GATK can be a bit finicky, and the workflow for producing a genotyped VCF using GATK “best practices” can take a little bit of troubleshooting. Below I’ve outlined my own simple workflow for calling SNVs in human data using GATK, including the various preprocessing steps and file/software downloads. This is mostly for my own reference in the future, when the ramblings in my lab notebook eventually lose all meaning to me.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#installing-gatk",
    "href": "notes/gatk.html#installing-gatk",
    "title": "A simple guide to running GATK",
    "section": "Installing GATK",
    "text": "Installing GATK\nGo to the releases on the GATK GitHub, and wget the most recent version (in my case, v.4.1.8.1). Then, just unzip the package.\nwget https://github.com/broadinstitute/gatk/releases/download/4.1.8.1/gatk-4.1.8.1.zip\n\nunzip gatk-4.1.8.1.zip\nThe gatk binary should be sitting in that directory. Just run as follows:\n/path/to/gatk/binary/gatk",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#preparing-bam-files-for-initial-variant-calling",
    "href": "notes/gatk.html#preparing-bam-files-for-initial-variant-calling",
    "title": "A simple guide to running GATK",
    "section": "Preparing BAM files for initial variant calling",
    "text": "Preparing BAM files for initial variant calling\nLet’s assume you’re running GATK separately on individual sample BAMs. Even though each BAM contains a single sample’s reads, it’s still necessary to add read groups to the BAM header and read entries.\nsamtools addreplacerg -r \"@RG\\tID:${sample_name}\\tSM:${sample_name}\" -o {output} {input}\nAlthough GATK will run just fine on a BAM with no read groups, exactly zero variants will be output to VCF. In fact, my version of GATK HaplotypeCaller stated that all reads were filtered out by MappingQualityFilter.\nOf course, if you’re planning on merging multiple sample BAMs into a single BAM before running HaplotypeCaller, you should add read groups in order to know exactly which sample a given read is derived from.\nThen, as usual, it’s good to sort and index the BAM, and mark duplicates.\nsamtools sort -O BAM -o {output} {input}\n\nsamtools index {input}\nFor duplicate marking, I tend to use picard (available here).\njava -jar /path/to/picard.jar MarkDuplicates INPUT={input} OUTPUT={output} METRICS_FILE={metrics} TMP_DIR={tmpdir}\nAs it marches along the BAM and marks duplicates, picard will output lots of temporary files, so it’s good to specify a $tmpdir with enough space for potentially hundreds (or more) temporary files.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#running-gatk-haplotypecaller",
    "href": "notes/gatk.html#running-gatk-haplotypecaller",
    "title": "A simple guide to running GATK",
    "section": "Running GATK HaplotypeCaller",
    "text": "Running GATK HaplotypeCaller\nThe first step of variant calling involves running HaplotypeCaller, which will produce an initial list of variants. By design, HaplotypeCaller is very sensitive, so it’s possible that most of the variants in the output VCF will be junk. We’ll do some filtering and “calibration” of variants in later steps.\ngatk HaplotypeCaller \\\n    --input ${input_bam} \\\n    --output ${output_VCF} \\\n    --reference ${reference_genome} \\\n    --java-options \"-Xmx8G\"\n    -ERC GVCF\n\nSince GATK is written in Java, we can also pass in some standard Java options (such as max memory to be used) as an argument!\n\n\nAlso, by passing in the -ERC GVCF argument, we tell HaplotypeCaller to produce GVCF (generic VCF) output, which looks a bit different from normal VCF. The benefit of outputting GVCFs is that we can then run joint genotyping on many samples’ GVCFs together quite quickly.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#joint-genotyping-gvcfs",
    "href": "notes/gatk.html#joint-genotyping-gvcfs",
    "title": "A simple guide to running GATK",
    "section": "Joint genotyping GVCFs",
    "text": "Joint genotyping GVCFs\ngatk GenotypeGVCFs \\\n    --variant ${input_gvcfs} \\\n    --output {output} \\\n    --reference {input.ref} \\\n    --java-options \"-Xmx8G\"\nHere, we can run GenotypeGVCFs on one or many GVCFs together. By passing in multiple GVCFs, we can take advantage of the joint genotyping process to consider evidence from multiple samples at a given variant site. In any case, the output here will be “true” VCF.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#variant-quality-score-recalibration-vqsr",
    "href": "notes/gatk.html#variant-quality-score-recalibration-vqsr",
    "title": "A simple guide to running GATK",
    "section": "Variant quality score recalibration (VQSR)",
    "text": "Variant quality score recalibration (VQSR)\nNow, we arrive at the most arcane (in my opinion) step in running GATK. Essentially, VQSR takes in a few “truth sets,” which are just VCF files containing variants considered to be either “very good” or “very bad,” so that GATK can get a feeling for the qualities that make a “good variant.” But first, we need to get a hold of these truth sets.\n\nDownloading GATK resources for VQSR\nGATK hosts its “resource bundle,” containing various files that are important for running GATK, on Google. The four truth sets we can use are hosted here.\nTo download, we can use a tool called gsutil, developed by Google specifically for fetching files from Google Cloud.\n# download \"omni\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz\n# download \"hapmap\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz\n# download \"1000G\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz\n# download \"dbsnp\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\nWe’ll also need the tabix indices for these VCFs, which can also be download from the Google Cloud repository, or done by hand. The “dbsnp” truth set also needs to be bgzip-ed!\n\n\nRunning VQSR\nNow that we’ve collected the various files we need, we can run VQSR. Importantly, VQSR does not output VCF files. Instead it outputs a .recal file and a .tranches file, which are “applied” to the previous VCF file in the final step of calibration.\nA couple of notes here:\n\nwe can specify exactly which annotations we want VQSR to add by specifying -an multiple times\nwe can specify the “tranches” we want VQSR to divide our variants into by specifying tranche multiple times\nin GATK v4.1.8.1, we must include a space between the --resource truth set info and the path to the truth set\nit looks as though GATK now has a convolutional neural net approach to VQSR in GATK v4+ for single samples (see here)\n\nMore detailed documentation on running VQSR (for human data) is here.\ngatk VariantRecalibrator \\\n    --variant {input} \\\n    --output {recal_file_output} \\\n    --tranches-file {tranch_file_output} \\\n    -an QD -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\\n    --mode SNP \\\n    --trust-all-polymorphic true \\\n    --java-options \"-Xmx8G\" \\\n    -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \\\n    --resource:omni,known=false,training=true,truth=true,prior=12 /path/to/1000G_omni2.5.hg38.vcf.gz \\\n    --resource:1000G,known=false,training=true,truth=false,prior=10 /path/to/1000G_phase1.snps.high_confidence.hg38.vcf.gz \\\n    --resource:hapmap,known=false,training=true,truth=true,prior=15 /path/to/hapmap_3.3.hg38.vcf.gz \\\n    --resource:dbsnp,known=true,training=false,truth=false,prior=7 /path/to/Homo_sapiens_assembly38.dbsnp138.vcf.gz\n\n\nApplying VQSR\nNow, we arrive at the final step of VQSR. Here, we use the .tranches and .recal files produced in the previous step, and “apply” them to the VCF produced in the first step of this section. The output is a recalibrated VCF, which we can then pass into any downstream filtering or analysis pipelines we want!\ngatk ApplyVQSR \\\n    --variant {input_vcf} \\\n    --recal-file {recal_file} \\\n    --tranches-file {tranches_file} \\\n    --truth-sensitivity-filter-level 99.7 \\\n    --create-output-variant-index true \\\n    -mode SNP \\\n    --output {output} \\\n    --java-options \"-Xmx8G\"",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Basic data visualization using R.\nSlides Video\n\n\nIntro to regression (Part I): linear models and interpretation.\nSlides Video\n\n\nIntro to regression (Part II): multiple variables, interaction, and diagnostics.\nSlides\n\n\nIntro to regression (Part III): statistical tests as linear models.\nSlides"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Mapping mutator alleles in mice. Lawrence University, Recent Advances in Biology Lecture Series (2021). Virtual.\nMapping mutator alleles in mice. Pritchard Lab Mini-conference (2021). Virtual.\nA wild-derived antimutator drives germline mutation spectrum differences in a genetically diverse murine family. Columbia University, Przeworski Lab Meeting (2021). Virtual.\nLong read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays. Society for Molecular Biology and Evolution annual meeting (2019). Manchester, UK.\nTracking adaptive structural variation during host-pathogen conflict. London Calling, Oxford Nanopore Technologies annual meeting (2017). London, UK. Viewable here."
  },
  {
    "objectID": "talks.html#invited-presentations",
    "href": "talks.html#invited-presentations",
    "title": "Talks",
    "section": "",
    "text": "Mapping mutator alleles in mice. Lawrence University, Recent Advances in Biology Lecture Series (2021). Virtual.\nMapping mutator alleles in mice. Pritchard Lab Mini-conference (2021). Virtual.\nA wild-derived antimutator drives germline mutation spectrum differences in a genetically diverse murine family. Columbia University, Przeworski Lab Meeting (2021). Virtual.\nLong read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays. Society for Molecular Biology and Evolution annual meeting (2019). Manchester, UK.\nTracking adaptive structural variation during host-pathogen conflict. London Calling, Oxford Nanopore Technologies annual meeting (2017). London, UK. Viewable here."
  },
  {
    "objectID": "talks.html#contributed-presentations",
    "href": "talks.html#contributed-presentations",
    "title": "Talks",
    "section": "Contributed Presentations",
    "text": "Contributed Presentations\nFast and furious mutation at tandem repeats in a large, four-generation human family. The Biology of Genomes (2025). Cold Spring Harbor, NY.\nDiscovering epistasis between germline mutator alleles in mice. The Biology of Genomes (2023). Cold Spring Harbor, NY.\nDirectly measuring the dynamics of the human mutation rate by sequencing large, multi-generational pedigrees. American Society of Human Genetics annual meeting (2018). Plenary presentation. San Diego, CA.\nHuman immune defense mechanisms drive rapid genome evolution in vaccinia virus. American Society of Human Genetics annual meeting (2017). Platform presentation. Orlando, FL."
  }
]