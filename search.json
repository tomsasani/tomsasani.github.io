[
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Click 💻 to access associated code and 📖 for PDFs."
  },
  {
    "objectID": "pubs.html#preprints",
    "href": "pubs.html#preprints",
    "title": "Publications",
    "section": "Preprints",
    "text": "Preprints\nKunisaki J, Goldberg ME, Lulla S, Sasani TA, Hiatt L, Nicholas TJ, Liu L, Torres-Arce E, Guo Y, James E, Horns JJ, Ramsay JM, Chen W, Hotaling JM, Aston KI, Quinlan AR. Sperm from infertile, oligozoospermic men have elevated mutation rates (2024). medRxiv.\nPorubsky D, Dashnow H, Sasani TA, Logsdon GA, Hallast P, Noyes MD, Kronenberg ZN, Mokveld T, et al. A familial, telomere-to-telomere reference for human de novo mutation and recombination from a four-generation pedigree (2024). bioRxiv\nMokveld T, Dolzhenko E, Dashnow H, Nicholas TJ, Sasani TA, van der Sanden B, Jadhav B, Pedersen B, Kronenberg S, Tucci A, Sharp AJ, Quinlan AR, Gilissen C, Hoischen A, Eberle MA. TRGT-denovo: accurate detection of de novo tandem repeat mutations (2024). bioRxiv\nXu K, Zhang Y, Baldwin-Brown J, Sasani TA, Phadnis N, Miller MP, Rog O. Decoding chromosome organization using CheC-PLS: chromosome conformation by proximity labeling and long-read sequencing (2024). bioRxiv\nAshbrook DG, Sasani TA, Maksimov M, Gunturkun MH, Ma N, Villani F, Ren Y, Rothschild D, Chen H, Lu L, Colonna V, Dumont B, Harris K, Gymrek M, Pritchard JK, Palmer AA, Williams RW. Private and sub-family specific mutations of founder haplotypes in the BXD family reveal phenotypic consequences relevant to health and disease (2022). bioRxiv."
  },
  {
    "objectID": "pubs.html#peer-reviewed-manuscripts",
    "href": "pubs.html#peer-reviewed-manuscripts",
    "title": "Publications",
    "section": "Peer-reviewed Manuscripts",
    "text": "Peer-reviewed Manuscripts\nSasani TA, Quinlan AR, Harris K. Epistasis between mutator alleles contributes to germline mutation rate variability in laboratory mice (2024). eLife.  💻\nFixsen SM, Cone KR, Goldstein SA, Sasani TA, Quinlan AR, Rothenburg S, Elde NC. Poxviruses capture host genes by LINE-1 retrotransposition (2022). eLife.  📖\nSasani TA, Ashbrook DG, Beichman AC, Lu L, Palmer AA, Williams RW, Pritchard JK, Harris K. A natural mutator allele shapes mutation spectrum variation in mice (2022). Nature.  💻  📖\nBelyeu JR*, Sasani TA*, Pedersen BS, Quinlan AR. Unfazed: parent-of-origin detection for large and small de novo variants (2021). Bioinformatics.  💻  📖\nWallace AD, Sasani TA, Swanier J, Gates B, Greenland J, Pedersen BP, Varley KT, Quinlan AR. CaBagE: a Cas9-based Background Elimination strategy for targeted, long-read DNA sequencing (2021). PLoS One.  📖\nCawthon RM, Meeks HD, Sasani TA*, Smith KR, Kerber RA, O’Brien E, Baird L, Dixon MM, Peiffer AP, Leppert MF, Quinlan AR, Jorde LB. Germline mutation rates in young adults predict longevity and reproductive lifespan (2020). Scientific Reports.  📖\nSasani TA, Pedersen BS, Gao Z, Baird L, Przeworski M, Jorde LB, Quinlan AR. Large, three-generation human families reveal post-zygotic mosaicism and variability in germline mutation accumulation (2019). eLife.  💻  📖\nGao Z, Moorjani P, Sasani TA, Pedersen BP, Quinlan AR, Jorde LB, Amster G, Przeworski MP. Overlooked roles of DNA damage and maternal age in generating human germline mutations (2019). PNAS.   📖\nSasani TA*, Cone KR*, Quinlan AR, Elde NC. Long read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays (2018). eLife.  💻  📖\nBelyeu JR, Nicholas TJ, Pedersen BS, Sasani TA, Havrilla JM, Kravitz SN, Conway ME, Lohman BK, Quinlan AR, Layer RM. SV-plaudit: a cloud-based framework for manually curating thousands of structural variants (2018). GigaScience.  💻  📖\nJain M*, Koren S*, Miga K*, Quick J*, Rand AC*, Sasani TA*, Tyson JR*, Beggs AD, Dilthey AT, Fiddes IT, Malla S, Marriott H, Nieto T, O’Grady J, Olsen HE, Pedersen BS, Rhie A, Richardson H, Quinlan AR, Snutch TP, Tee L, Paten B, Phillippy AM, Simpson JT, Loman NJ, Loose M. Nanopore sequencing and assembly of a human genome with ultra-long reads (2018). Nature Biotechnology.  📖\nFeusier J, Witherspoon DJ, Watkins SW, Goubert C, Sasani TA, Jorde LB. Discovery of rare, diagnostic AluYb8/9 elements in diverse human populations (2017). Mobile DNA.  📖\nPiasecki BP, Sasani TA, Lessenger AT, Huth N, Farrell S. MAPK-15 is a ciliary protein required for PKD-2 localization and male mating behavior in Caenorhabditis elegans (2017).  Cytoskeleton.  📖\n* indicates equal contribution"
  },
  {
    "objectID": "ml_reading_group/2024_intro.html",
    "href": "ml_reading_group/2024_intro.html",
    "title": "ML for genomics, an evolving reading list",
    "section": "",
    "text": "Introductions to ML\nThe CS230 course at Stanford has made slides and lectures available online. A fantastic resource.\nThe Amidi brothers have created cheat sheets for CS230 and other Stanford ML courses. Also great.\nAndrej Karpathy’s videos are also excellent introductions to ML concepts. In his own words, the first video in his main series (called The spelled-out intro to neural networks and backpropagation: building micrograd) “only assumes basic knowledge of Python and a vague recollection of calculus from high school.” Highly recommended.\n\n\n\n\n\n\nReview articles\n\n\n\n\n\nDeep learning: new computational modeling techniques for genomics\n\nAn excellent and detailed overview, probably where I’d start.\n\nA primer on deep learning in genomics\nHarnessing deep learning for population genetic inference\nNavigating the pitfalls of applying machine learning in genomics\n\nExcellent overview of pitfalls and possible mistakes that can confound ML analyses, with a particular focus on biological inference.\n\nOpportunities and obstacles for deep learning in medicine\nSupervised machine learning for population genetics: a new paradigm\nTo transformers and beyond: large language models for the genome\n\n\n\n\n\nApplications\nPapers are broadly grouped by ML architecture. Many of these papers involve a mix of architectures, so the groups should be considered “fuzzy.”\n\n\n\n\n\n\nTransformers\n\n\n\n\n\nEffective gene expression prediction from sequence by integrating long-range interactions\n\nDescribes the “Enformer” model, which utilizes a transformer-based architecture to predict gene expression from sequence alone. Also see a suite of papers describing the limitations of “Enformer” for personal transcriptome inference.\n\nPredicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation\n\n\n\n\n\n\n\n\n\nGenerative adversarial networks (GANs)\n\n\n\n\n\nAutomatic inference of demographic parameters using generative adversarial networks\n\nThe authors describe a novel GAN architecture that that features a population genetic simulator (in this case, the backwards-in-time msprime tool) as the “generator” and a convolutional neural network as the “discriminator”. The parameters of the msprime generator are randomly initialized, and the discriminator is trained to differentiate between simulated and real “images” of haplotypes in genomic regions of a predefined size. Over time, the generator gets better at simulating realistic-looking data and the discriminator gets better at telling the two classes of data apart. By the end of training, the generator can be interpreted by examining the population genetic parameters (population size, mutation rate, etc.) that optimally confused the discriminator. A well-written and clear overview of a cool (and interpretable) method.\n\nInterpreting generative adversarial networks to infer natural selection from genetic data\n\nA follow-up the the paper listed above. The authors fine-tune the trained discriminator from their GAN to infer regions of the genome under the effects of natural selection.\n\n\n\n\n\n\n\n\n\n\nLanguage models (LMs)\n\n\n\n\n\nDNA language models are powerful predictors of genome-wide variant effects\nGPN-MSA: an alignment-based DNA language model for genome-wide variant effect prediction\nThe nucleotide transformer: building and evaluating robust foundation models for human genomics\n\nA recent attempt to build a “foundation model” for genomics. The authors essentially adapt BERT for DNA sequence by developing an encoder-only architecture that attempts to reconstruct randomly-masked 6-mer DNA “tokens.” The learned embeddings from an input DNA sequence can then be plugged into simple regression models to make predictions about chromatin accessibility, enhancer status, etc., or the model itself can be efficiently fine-tuned for a particular downstream classification task.\n\n\n\n\n\n\n\n\n\n\nConvolutional neural networks (CNNs)\n\n\n\n\n\nThe unreasonable effectiveness of convolutional neural networks in population genetic inference\n\nCheck this paper out for a nice introduction to CNNs and how they can be applied to “images” of haplotypes in genomic regions. The associated GitHub repository includes a few simple models (written in TensorFlow/Keras), as well.\n\nBasset: Learning the regulatory code of the accessible genome with deep convolutional neural networks\nSequential regulatory activity prediction across chromosomes with convolutional neural networks\nLocalizing post-admixture adaptive variants with object detection on ancestry-painted chromosomes\n\nThe authors train an off-the-shelf object detection model to identify genomic regions with recent (adaptive) admixture events. Nice example of using off-the-shelf models, rather than building architectures from scratch.\n\nDiscovery of ongoing selective sweeps within Anopholes mosquito populations using deep learning\n\nA nice example of training CNNs to detect selection using pre-computed features (e.g., a large collection of population genetic summary statistics) rather than “painted haplotype” images.\n\n\n\n\n\n\n\n\n\n\nAutoencoders\n\n\n\n\n\nVisualizing population structure with variational autoencoders\n\nThe authors use a variational autoencoder (VAE) to embed sample genotype vectors into a 2-dimensional latent space that reflects geographical origin.\n\nHaplotype and population structure inference using neural networks in whole-genome sequencing data\nA deep learning framework for characterization of genotype data"
  },
  {
    "objectID": "notes/gatk.html",
    "href": "notes/gatk.html",
    "title": "A simple guide to running GATK",
    "section": "",
    "text": "There are probably hundreds of “how to use GATK” guides published online, not including the detailed documentation on the GATK website. But in my (admittedly limited) experience, running GATK can be a bit finicky, and the workflow for producing a genotyped VCF using GATK “best practices” can take a little bit of troubleshooting. Below I’ve outlined my own simple workflow for calling SNVs in human data using GATK, including the various preprocessing steps and file/software downloads. This is mostly for my own reference in the future, when the ramblings in my lab notebook eventually lose all meaning to me.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#motivation",
    "href": "notes/gatk.html#motivation",
    "title": "A simple guide to running GATK",
    "section": "",
    "text": "There are probably hundreds of “how to use GATK” guides published online, not including the detailed documentation on the GATK website. But in my (admittedly limited) experience, running GATK can be a bit finicky, and the workflow for producing a genotyped VCF using GATK “best practices” can take a little bit of troubleshooting. Below I’ve outlined my own simple workflow for calling SNVs in human data using GATK, including the various preprocessing steps and file/software downloads. This is mostly for my own reference in the future, when the ramblings in my lab notebook eventually lose all meaning to me.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#installing-gatk",
    "href": "notes/gatk.html#installing-gatk",
    "title": "A simple guide to running GATK",
    "section": "Installing GATK",
    "text": "Installing GATK\nGo to the releases on the GATK GitHub, and wget the most recent version (in my case, v.4.1.8.1). Then, just unzip the package.\nwget https://github.com/broadinstitute/gatk/releases/download/4.1.8.1/gatk-4.1.8.1.zip\n\nunzip gatk-4.1.8.1.zip\nThe gatk binary should be sitting in that directory. Just run as follows:\n/path/to/gatk/binary/gatk",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#preparing-bam-files-for-initial-variant-calling",
    "href": "notes/gatk.html#preparing-bam-files-for-initial-variant-calling",
    "title": "A simple guide to running GATK",
    "section": "Preparing BAM files for initial variant calling",
    "text": "Preparing BAM files for initial variant calling\nLet’s assume you’re running GATK separately on individual sample BAMs. Even though each BAM contains a single sample’s reads, it’s still necessary to add read groups to the BAM header and read entries.\nsamtools addreplacerg -r \"@RG\\tID:${sample_name}\\tSM:${sample_name}\" -o {output} {input}\nAlthough GATK will run just fine on a BAM with no read groups, exactly zero variants will be output to VCF. In fact, my version of GATK HaplotypeCaller stated that all reads were filtered out by MappingQualityFilter.\nOf course, if you’re planning on merging multiple sample BAMs into a single BAM before running HaplotypeCaller, you should add read groups in order to know exactly which sample a given read is derived from.\nThen, as usual, it’s good to sort and index the BAM, and mark duplicates.\nsamtools sort -O BAM -o {output} {input}\n\nsamtools index {input}\nFor duplicate marking, I tend to use picard (available here).\njava -jar /path/to/picard.jar MarkDuplicates INPUT={input} OUTPUT={output} METRICS_FILE={metrics} TMP_DIR={tmpdir}\nAs it marches along the BAM and marks duplicates, picard will output lots of temporary files, so it’s good to specify a $tmpdir with enough space for potentially hundreds (or more) temporary files.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#running-gatk-haplotypecaller",
    "href": "notes/gatk.html#running-gatk-haplotypecaller",
    "title": "A simple guide to running GATK",
    "section": "Running GATK HaplotypeCaller",
    "text": "Running GATK HaplotypeCaller\nThe first step of variant calling involves running HaplotypeCaller, which will produce an initial list of variants. By design, HaplotypeCaller is very sensitive, so it’s possible that most of the variants in the output VCF will be junk. We’ll do some filtering and “calibration” of variants in later steps.\ngatk HaplotypeCaller \\\n    --input ${input_bam} \\\n    --output ${output_VCF} \\\n    --reference ${reference_genome} \\\n    --java-options \"-Xmx8G\"\n    -ERC GVCF\n\nSince GATK is written in Java, we can also pass in some standard Java options (such as max memory to be used) as an argument!\n\n\nAlso, by passing in the -ERC GVCF argument, we tell HaplotypeCaller to produce GVCF (generic VCF) output, which looks a bit different from normal VCF. The benefit of outputting GVCFs is that we can then run joint genotyping on many samples’ GVCFs together quite quickly.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#joint-genotyping-gvcfs",
    "href": "notes/gatk.html#joint-genotyping-gvcfs",
    "title": "A simple guide to running GATK",
    "section": "Joint genotyping GVCFs",
    "text": "Joint genotyping GVCFs\ngatk GenotypeGVCFs \\\n    --variant ${input_gvcfs} \\\n    --output {output} \\\n    --reference {input.ref} \\\n    --java-options \"-Xmx8G\"\nHere, we can run GenotypeGVCFs on one or many GVCFs together. By passing in multiple GVCFs, we can take advantage of the joint genotyping process to consider evidence from multiple samples at a given variant site. In any case, the output here will be “true” VCF.",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "notes/gatk.html#variant-quality-score-recalibration-vqsr",
    "href": "notes/gatk.html#variant-quality-score-recalibration-vqsr",
    "title": "A simple guide to running GATK",
    "section": "Variant quality score recalibration (VQSR)",
    "text": "Variant quality score recalibration (VQSR)\nNow, we arrive at the most arcane (in my opinion) step in running GATK. Essentially, VQSR takes in a few “truth sets,” which are just VCF files containing variants considered to be either “very good” or “very bad,” so that GATK can get a feeling for the qualities that make a “good variant.” But first, we need to get a hold of these truth sets.\n\nDownloading GATK resources for VQSR\nGATK hosts its “resource bundle,” containing various files that are important for running GATK, on Google. The four truth sets we can use are hosted here.\nTo download, we can use a tool called gsutil, developed by Google specifically for fetching files from Google Cloud.\n# download \"omni\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz\n# download \"hapmap\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz\n# download \"1000G\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz\n# download \"dbsnp\" truth set\n/path/to/gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\nWe’ll also need the tabix indices for these VCFs, which can also be download from the Google Cloud repository, or done by hand. The “dbsnp” truth set also needs to be bgzip-ed!\n\n\nRunning VQSR\nNow that we’ve collected the various files we need, we can run VQSR. Importantly, VQSR does not output VCF files. Instead it outputs a .recal file and a .tranches file, which are “applied” to the previous VCF file in the final step of calibration.\nA couple of notes here:\n\nwe can specify exactly which annotations we want VQSR to add by specifying -an multiple times\nwe can specify the “tranches” we want VQSR to divide our variants into by specifying tranche multiple times\nin GATK v4.1.8.1, we must include a space between the --resource truth set info and the path to the truth set\nit looks as though GATK now has a convolutional neural net approach to VQSR in GATK v4+ for single samples (see here)\n\nMore detailed documentation on running VQSR (for human data) is here.\ngatk VariantRecalibrator \\\n    --variant {input} \\\n    --output {recal_file_output} \\\n    --tranches-file {tranch_file_output} \\\n    -an QD -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\\n    --mode SNP \\\n    --trust-all-polymorphic true \\\n    --java-options \"-Xmx8G\" \\\n    -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \\\n    --resource:omni,known=false,training=true,truth=true,prior=12 /path/to/1000G_omni2.5.hg38.vcf.gz \\\n    --resource:1000G,known=false,training=true,truth=false,prior=10 /path/to/1000G_phase1.snps.high_confidence.hg38.vcf.gz \\\n    --resource:hapmap,known=false,training=true,truth=true,prior=15 /path/to/hapmap_3.3.hg38.vcf.gz \\\n    --resource:dbsnp,known=true,training=false,truth=false,prior=7 /path/to/Homo_sapiens_assembly38.dbsnp138.vcf.gz\n\n\nApplying VQSR\nNow, we arrive at the final step of VQSR. Here, we use the .tranches and .recal files produced in the previous step, and “apply” them to the VCF produced in the first step of this section. The output is a recalibrated VCF, which we can then pass into any downstream filtering or analysis pipelines we want!\ngatk ApplyVQSR \\\n    --variant {input_vcf} \\\n    --recal-file {recal_file} \\\n    --tranches-file {tranches_file} \\\n    --truth-sensitivity-filter-level 99.7 \\\n    --create-output-variant-index true \\\n    -mode SNP \\\n    --output {output} \\\n    --java-options \"-Xmx8G\"",
    "crumbs": [
      "A simple guide to running GATK"
    ]
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Basic data visualization using R.\nSlides Video\n\n\nIntro to regression (Part I): linear models and interpretation.\nSlides Video\n\n\nIntro to regression (Part II): multiple variables, interaction, and diagnostics.\nSlides\n\n\nIntro to regression (Part III): statistical tests as linear models.\nSlides"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I develop and apply computational methods to study mutation rate and spectrum evolution in mammalian genomes. Here are a few research questions I’ve pursued in the past:\n\nHow does inherited genetic variation impact the germline mutation rate?\n\nRelated paper: A natural mutator allele shapes mutation spectrum variation in mice\n\n\n\nTo what degree do mutation rates vary among human families?\n\nRelated paper: Large, three-generation human families reveal post-zygotic mosaicism and variability in germline mutation accumulation\n\n\n\nHow do structural and single-nucleotide variants interact to promote rapid adaptation?\n\nRelated paper: Long read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays"
  },
  {
    "objectID": "notes/snakemake.html",
    "href": "notes/snakemake.html",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "",
    "text": "A quick-start guide for Snakemake",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#motivation",
    "href": "notes/snakemake.html#motivation",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Motivation",
    "text": "Motivation\nThere are few (if any) scientific questions that you can answer by running a single program or script. Calling variants involves aligning reads, sorting reads, indexing reads, running a variant caller, filtering those calls, etc. Running a simulation will inevitably require changing parameters or including new combinations of those parameters.\nAnd there are few (if any) programs or scripts that will run/compile correctly on the first try, due to either user error, code bugs, dependency conflicts, improperly configured environments, or any combination therein.\nFor these reasons, it’s often crucial to package the various steps of your analysis into a “pipeline.” Ideally, this pipeline would accept a file(s) as input, do some stuff with that file, and generate an output file(s). For example, your pipeline might take a FASTQ file and reference genome FASTA as input, and output an aligned, sorted, and indexed BAM.\nIn theory, a pipeline could just be a Bash script in which you enumerate each step of the process. But what if you want to run the pipeline on hundreds of samples’ FASTQ files? And what if the final step in your Bash script fails? You’ll have to re-run the entire pipeline all over again.\nThis is where Snakemake comes in. Snakemake is a flexible Python-based pipeline manager, and it’s even tuned for running on the Sage Grid Engine (or pretty much any other compute environment).\n\nAs an example, let’s imagine that we want to take paired-end FASTQ from 3 different mouse samples (A, B, and C) and generate a preliminary set of variant calls for each sample.\n\n\nTo start, let’s imagine we only want to process one sample: A.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#every-step-of-the-pipeline-gets-its-own-rule",
    "href": "notes/snakemake.html#every-step-of-the-pipeline-gets-its-own-rule",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Every step of the pipeline gets its own “rule”",
    "text": "Every step of the pipeline gets its own “rule”\nThe first step of the pipeline will be to download an M. musculus reference genome so that we can align reads.\nrule download_reference:\n  input:\n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nThis rule, which is named download_reference, doesn’t take any input, since it’s just downloading a FASTA directly.\n\n\nWe specify that the expected output of this rule is a single gzipped FASTA.\n\n\nAfter shell:, we simply list the commands we’d normally type at the command line to produce the specified output. These commands can be wrapped up in a docstring for easy formatting.\n\nNext, we want to align the FASTQ data from sample A to the reference.\nrule bwa_align:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"A_1.fastq.gz\",\n    fq2 = \"A_2.fastq.gz\"\n  output:\n    \"A.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nThis rule takes as input a reference genome and two FASTQ files. As you can see, it’s possible to name individual input or output files (using Python variable assignment) so that we can access particular files in our shell command.\n\n\nNote that if a rule has more than one input (or output) files, they should be comma-separated.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#snakemake-will-only-run-a-rule-if-it-has-to",
    "href": "notes/snakemake.html#snakemake-will-only-run-a-rule-if-it-has-to",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Snakemake will only run a rule if it has to",
    "text": "Snakemake will only run a rule if it has to\nLet’s take a look at the full pipeline we’ve defined so far.\nrule all:\n  input:\n    \"A.sorted.bam\"\n\nrule download_reference:\n  input:\n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"A_1.fastq.gz\",\n    fq2 = \"A_2.fastq.gz\"\n  output:\n    \"A.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nA huge advantage of Snakemake is that it will only run a rule if its output is needed by a downstream rule.\n\n\nYou can see that I’ve added a rule (called all) to the top of the pipeline. This is because Snakemake runs in a “bottom-up” fashion.\n\n\nThe all rule tells Snakemake what the final output of the entire pipeline should be. In this case, we want the final output to be an aligned BAM.\n\n\nIn this example, Snakemake finds the rule that outputs A.sorted.bam (which is bwa_align), and checks to see if that rule has access to all of its necessary inputs (a reference and two FASTQ files). If not, Snakemake finds the rules that produce those files and runs them. And so on. Once bwa_align has all of the inputs it needs, Snakemake runs it to produce the final output.\n\n\nAnd let’s say that for whatever reason, bwa_align fails when its run. When we re-run the pipeline, Snakemake will check that its input files are present. Since all of the previous rules will have been run before invoking bwa_align, Snakemake will see that its inputs are present and won’t re-run any of the upstream steps!",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#using-wildcards-to-avoid-re-writing-the-same-command-over-and-over-again",
    "href": "notes/snakemake.html#using-wildcards-to-avoid-re-writing-the-same-command-over-and-over-again",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using wildcards to avoid re-writing the same command over and over again",
    "text": "Using wildcards to avoid re-writing the same command over and over again\nIn the previous example, we were only running the pipeline on a single sample. But to generalize the pipeline to run on any list of samples, we can make use of the expand feature, as well as Snakemake “wildcards.”\nWere this a bash pipeline we’d have to write out a separate set of commands for every sample. But with Snakemake it’s much easier.\nHere’s an example of what our pipeline would look like with wildcard placeholders instead of explicit sample names.\nsamples = [\"A\", \"B\", \"C\"]\n\nrule all:\n  input:\n    expand(\"{sample}.sorted.bam\", sample=samples)\n\nrule download_reference:\n  input: \n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input: \n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"{sample}_1.fastq.gz\",\n    fq2 = \"{sample}_2.fastq.gz\"\n  output:\n    \"{sample}.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nHere, I’ve just replaced every instance of a sample name in an output or input file with a {sample} wildcard.\n\n\nIn the all rule, I’m using the expand function to tell Snakemake that the expected output is a list of sorted BAMs, with the sample names in samples filled in.\n\nIn this case, the result of the expand would just be:\n&gt;&gt;&gt; expand(\"{sample}.sorted.bam\", sample=samples)\n\n[\"A.sorted.bam\", \"B.sorted.bam\", \"C.sorted.bam\"]\nBut we can also use expand for more complicated chaining of multiple sample names and parameters.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#using-expand-to-run-a-pipeline-on-many-samples-or-with-many-parameters",
    "href": "notes/snakemake.html#using-expand-to-run-a-pipeline-on-many-samples-or-with-many-parameters",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using expand to run a pipeline on many samples or with many parameters",
    "text": "Using expand to run a pipeline on many samples or with many parameters\nLet’s say that once we’ve produced aligned and sorted BAMs, we want to then run a simple variant calling program on each sample’s alignments.\nTo speed up execution of the variant calling pipeline, it might help to parallelize our pipeline to run on every chromosome separately.\nTo do this, we’ll again make use of expand.\nsamples = [\"A\", \"B\", \"C\"]\n\nchromosomes = list(range(1, 20))\nchromosomes = list(map(str, chromosomes))\nchromosomes.extend(['X', 'Y'])\n\nrule all:\n  input:\n    expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n\nrule download_reference:\n  input: \n  output:\n    \"GRCm38.primary_assembly.genome.fa.gz\"\n  shell:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M10/GRCm38.primary_assembly.genome.fa.gz\n    \"\"\"\n\nrule bwa_align:\n  input: \n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    fq1 = \"{sample}_1.fastq.gz\",\n    fq2 = \"{sample}_2.fastq.gz\"\n  output:\n    \"{sample}.sorted.bam\"\n  shell:\n    \"\"\"\n    bwa mem -t 4 {input.ref} {input.fq1} {input.fq2} | sambamba view -S -f bam /dev/stdin | sambamba sort -o {output} /dev/stdin\n    \"\"\"\n\nrule call_variants:\n  input:\n    ref = \"GRCm38.primary_assembly.genome.fa.gz\",\n    bam = \"{sample}.sorted.bam\"\n  output:\n    \"{sample}.{chrom}.vcf\"\n  shell:\n    \"\"\"\n    freebayes -f {input.ref} -r {wildcards.chrom} {input.bam} &gt; {output}\n    \"\"\"\n\nWe’ve now added a step to the pipeline which takes a reference genome and a BAM as input, and outputs a variant call file (VCF).\n\n\nNotice that in the call_variants rule, my freebayes command takes a -r argument that specifies the region we want to analyze. Whenever you want to access a wildcard inside of the shell: portion of a rule, its necessary to preface the wildcard name with wildcard.\n\nAnd in this example, expand takes the Cartesian product (i.e., itertools.product()) of the lists of parameters. So, the output of expand would be:\n&gt;&gt;&gt; expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n\n[\"A.chr1.vcf\",\n \"A.chr2.vcf\",\n \"A.chr3.vcf\",\n ...\n \"C.chrY.vcf\"]\nThis way, we can enumerate every possible combination of input parameters instead of typing out 3 * 21 separate commands.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#including-pure-python-in-a-rule",
    "href": "notes/snakemake.html#including-pure-python-in-a-rule",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Including pure python in a rule",
    "text": "Including pure python in a rule\nSo far, our rules have only invoked shell commands like bwa or wget. But Snakemake also allows you to make a rule that is just a block of python code. For example, the following rule would be totally acceptable.\nrule count_snps:\n  input:\n    expand(\"{sample}.{chrom}.vcf\", sample=samples, chrom=chromosomes)\n  output:\n    \"per_sample.snp_counts.tsv\"\n  run:\n    \"\"\"\n    from cyvcf2 import VCF\n\n    vcf_file = VCF({input})\n\n    output_fh = open({output}, \"w\")\n    \n    # loop over input files\n    for vcf_fh in {input}:\n\n      sample_name = vcf_fh.split('.')[0]\n      chrom = vcf_fh.split('.')[1]\n\n      # initialize a VCF object using each\n      # file handle in the input list\n      vcf = VCF(vcf_fh)\n\n      snp_count = 0\n\n      for v in vcf:\n        if v.var_type == \"snp\": snp_count += 1\n\n      print (','.join([sample_name, chrom, str(snp_count)]), file=output_fh)\n\n    \"\"\"\n\nNotice that when a rule includes python code, we use the run: syntax instead of the shell: syntax at the top of the code block.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#running-a-snakemake-pipeline-on-the-sage-grid-engine-sge",
    "href": "notes/snakemake.html#running-a-snakemake-pipeline-on-the-sage-grid-engine-sge",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Running a Snakemake pipeline on the Sage Grid Engine (SGE)",
    "text": "Running a Snakemake pipeline on the Sage Grid Engine (SGE)\nIn our lab, we use the SGE to submit and manage compute jobs on the cluster. Snakemake is actually compatible with SGE (and SLURM, etc.), which makes it super easy to submit jobs to SGE directly from a Snakemake pipeline.\nAs an example, we could execute our pipeline as follows:\nsnakemake -j 10 \\\n      --cluster \\\n      --rerun-incomplete \\\n       \"qsub -l centos=7 -l mfree=16G -l h_rt=12:0:0 -o /path/to/outdir -e /path/to/errdir\"\n\nThe -j flag specifies the maximum number of jobs Snakemake is allowed to submit to SGE at a time.\n\n\nThe --rerun-incomplete flag is awesome. If we run our pipeline and it fails for some reason, using --rerun-incomplete will tell Snakemake to re-run a rule if the output of that rule is incomplete (i.e., if the rule didn’t finish due to a job failure in the last pipeline execution).\n\n\nAfter specifying --cluster, we just put the normal qsub command we’d normally use to submit a .sh or .sge script to the cluster, specifying the memory required by each job, wall time, etc.",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#using-a-config-file-to-flexibly-change-grid-requirements-for-particular-rules",
    "href": "notes/snakemake.html#using-a-config-file-to-flexibly-change-grid-requirements-for-particular-rules",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Using a config file to flexibly change grid requirements for particular rules",
    "text": "Using a config file to flexibly change grid requirements for particular rules\nOne natural issue with the above command is that some rules might require different cluster specifications than others.\nFor example, I’ve written my bwa_align rule such that bwa mem will use 4 threads during alignment, and it’ll probably use much more memory than downloading a reference genome.\nThankfully, Snakemake lets us create individual cluster specifications for each of our rules using a config file. See the example config file below, which is written in JSON.\n\"__default__\":\n\n{ \"memory\": \"8G\",\n  \"time\": \"1:0:0\",\n  \"threads\": \"1\",\n  \"os\": \"7\" },\n\n\"bwa_align\":\n\n{ \"memory\": \"4G\",\n  \"time\": \"8:0:0\",\n  \"threads\": \"4\",\n  \"os\": \"7\" }\nThen, when we run Snakemake, we could do the following:\nsnakemake -j 10 \\\n      --cluster-config /path/to/config.json \\\n      --cluster \\\n      --rerun-incomplete \\\n      \"qsub -l centos={cluster.os} -l mfree={cluster.memory} -l h_rt={cluster.time} -pe serial {cluster.threads}\"",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "notes/snakemake.html#visualizing-the-full-pipeline-with-a-dag",
    "href": "notes/snakemake.html#visualizing-the-full-pipeline-with-a-dag",
    "title": "An introduction to Snakemake for pipeline management",
    "section": "Visualizing the full pipeline with a DAG",
    "text": "Visualizing the full pipeline with a DAG\nWe can also visualize the various steps of the pipeline in a directed acyclic graph (DAG).\nAfter putting the full pipeline in a file called Snakefile, we can run the following from the directory in which the Snakefile resides:\nsnakemake --dag | dot -Tsvg &gt; dag.svg\nThis will produce an image showing us exactly what steps Snakemake will during execution. This plot ignores the VCF calling steps, since the DAG gets pretty unweildy with that many steps!",
    "crumbs": [
      "An introduction to Snakemake for pipeline management"
    ]
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Mapping mutator alleles in mice. Lawrence University, Recent Advances in Biology Lecture Series (2021). Virtual.\nMapping mutator alleles in mice. Pritchard Lab Mini-conference (2021). Virtual.\nA wild-derived antimutator drives germline mutation spectrum differences in a genetically diverse murine family. Columbia University, Przeworski Lab Meeting (2021). Virtual.\nLong read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays. Society for Molecular Biology and Evolution annual meeting (2019). Manchester, UK.\nTracking adaptive structural variation during host-pathogen conflict. London Calling, Oxford Nanopore Technologies annual meeting (2017). London, UK. Viewable here."
  },
  {
    "objectID": "talks.html#invited-presentations",
    "href": "talks.html#invited-presentations",
    "title": "Talks",
    "section": "",
    "text": "Mapping mutator alleles in mice. Lawrence University, Recent Advances in Biology Lecture Series (2021). Virtual.\nMapping mutator alleles in mice. Pritchard Lab Mini-conference (2021). Virtual.\nA wild-derived antimutator drives germline mutation spectrum differences in a genetically diverse murine family. Columbia University, Przeworski Lab Meeting (2021). Virtual.\nLong read sequencing reveals poxvirus evolution through rapid homogenization of gene arrays. Society for Molecular Biology and Evolution annual meeting (2019). Manchester, UK.\nTracking adaptive structural variation during host-pathogen conflict. London Calling, Oxford Nanopore Technologies annual meeting (2017). London, UK. Viewable here."
  },
  {
    "objectID": "talks.html#contributed-presentations",
    "href": "talks.html#contributed-presentations",
    "title": "Talks",
    "section": "Contributed Presentations",
    "text": "Contributed Presentations\nDiscovering epistasis between germline mutator alleles in mice. The Biology of Genomes (2023). Cold Spring Harbor, NY.\nDirectly measuring the dynamics of the human mutation rate by sequencing large, multi-generational pedigrees. American Society of Human Genetics annual meeting (2018). Plenary presentation. San Diego, CA.\nHuman immune defense mechanisms drive rapid genome evolution in vaccinia virus. American Society of Human Genetics annual meeting (2017). Platform presentation. Orlando, FL."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tom Sasani, PhD",
    "section": "",
    "text": "I’m a computational biologist using genetic data to better understand health and evolution.\n\nExperience\nI’m currently a (remote) Staff Research Scientist with Dr. Aaron Quinlan at the University of Utah, where I develop new methods to study the evolution of germline mutation rates and spectra.\nAs a Postdoctoral Fellow with Dr. Kelley Harris at the University of Washington, I discovered genetic modifiers of the germline mutation rate in mice.\nIn between my academic roles I was also a Senior Data Scientist at Recursion Pharmaceuticals, where I analyzed data from massive cellular imaging experiments to identify new treatments for human disease."
  }
]